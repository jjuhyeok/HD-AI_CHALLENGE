{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kvY2HmDexFj3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets, ensemble\n",
        "from catboost import CatBoostRegressor\n",
        "from tqdm import tqdm\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "import itertools\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>LENGTH</th>\n",
              "      <th>SHIPMANAGER</th>\n",
              "      <th>FLAG</th>\n",
              "      <th>U_WIND</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>30.881018</td>\n",
              "      <td>2018-12-17 21:29</td>\n",
              "      <td>Z618338</td>\n",
              "      <td>30.0</td>\n",
              "      <td>24</td>\n",
              "      <td>24300</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>CQSB78</td>\n",
              "      <td>Panama</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>3.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>IN</td>\n",
              "      <td>UJM2</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2014-09-23 6:59</td>\n",
              "      <td>X886125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>13</td>\n",
              "      <td>35900</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>SPNO34</td>\n",
              "      <td>Marshall Islands</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-02-03 22:00</td>\n",
              "      <td>T674582</td>\n",
              "      <td>50.0</td>\n",
              "      <td>12</td>\n",
              "      <td>146000</td>\n",
              "      <td>...</td>\n",
              "      <td>370.0</td>\n",
              "      <td>FNPK22</td>\n",
              "      <td>Malta</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>ZAG4</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-01-17 4:02</td>\n",
              "      <td>Y847238</td>\n",
              "      <td>20.0</td>\n",
              "      <td>18</td>\n",
              "      <td>6910</td>\n",
              "      <td>...</td>\n",
              "      <td>120.0</td>\n",
              "      <td>PBZV77</td>\n",
              "      <td>Bahamas</td>\n",
              "      <td>-3.18</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.629350</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>27.037650</td>\n",
              "      <td>2020-01-26 7:51</td>\n",
              "      <td>A872328</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10</td>\n",
              "      <td>116000</td>\n",
              "      <td>...</td>\n",
              "      <td>300.0</td>\n",
              "      <td>GUCE76</td>\n",
              "      <td>Liberia</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>25.6</td>\n",
              "      <td>2.495953</td>\n",
              "      <td>15</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>253.554444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391934</th>\n",
              "      <td>TRAIN_391934</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2017-06-06 5:02</td>\n",
              "      <td>Y375615</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>6820</td>\n",
              "      <td>...</td>\n",
              "      <td>110.0</td>\n",
              "      <td>KEJZ24</td>\n",
              "      <td>China, People's Republic Of</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391935</th>\n",
              "      <td>TRAIN_391935</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>5.884603</td>\n",
              "      <td>2019-10-16 0:36</td>\n",
              "      <td>K635567</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12</td>\n",
              "      <td>3160</td>\n",
              "      <td>...</td>\n",
              "      <td>80.0</td>\n",
              "      <td>JLTM64</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>27.3</td>\n",
              "      <td>1.253491</td>\n",
              "      <td>8</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>144.061389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391936</th>\n",
              "      <td>TRAIN_391936</td>\n",
              "      <td>US</td>\n",
              "      <td>QGN3</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>70.660241</td>\n",
              "      <td>2021-03-23 22:35</td>\n",
              "      <td>J284147</td>\n",
              "      <td>30.0</td>\n",
              "      <td>8</td>\n",
              "      <td>60300</td>\n",
              "      <td>...</td>\n",
              "      <td>200.0</td>\n",
              "      <td>YERJ68</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>-3.44</td>\n",
              "      <td>7.99</td>\n",
              "      <td>21.1</td>\n",
              "      <td>4.766257</td>\n",
              "      <td>18</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>41.482222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391937</th>\n",
              "      <td>TRAIN_391937</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>9.448179</td>\n",
              "      <td>2015-01-08 7:15</td>\n",
              "      <td>J644215</td>\n",
              "      <td>30.0</td>\n",
              "      <td>29</td>\n",
              "      <td>23800</td>\n",
              "      <td>...</td>\n",
              "      <td>170.0</td>\n",
              "      <td>HCZK58</td>\n",
              "      <td>Comoros</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>7.485278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391938</th>\n",
              "      <td>TRAIN_391938</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-06-08 23:30</td>\n",
              "      <td>D123358</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>50600</td>\n",
              "      <td>...</td>\n",
              "      <td>260.0</td>\n",
              "      <td>GRJG55</td>\n",
              "      <td>Hong Kong, China</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>391939 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     SG   GIW5          Container  30.881018   \n",
              "1       TRAIN_000001     IN   UJM2               Bulk   0.000000   \n",
              "2       TRAIN_000002     CN   EUC8          Container   0.000000   \n",
              "3       TRAIN_000003     JP   ZAG4          Container   0.000000   \n",
              "4       TRAIN_000004     SG   GIW5          Container  27.037650   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "391934  TRAIN_391934     JP   QYY1          Container   0.000000   \n",
              "391935  TRAIN_391935     SG   GIW5               Bulk   5.884603   \n",
              "391936  TRAIN_391936     US   QGN3               Bulk  70.660241   \n",
              "391937  TRAIN_391937     TW   JWI3          Container   9.448179   \n",
              "391938  TRAIN_391938     TW   JWI3          Container   0.000000   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  LENGTH  \\\n",
              "0       2018-12-17 21:29  Z618338     30.0     24       24300  ...   180.0   \n",
              "1        2014-09-23 6:59  X886125     30.0     13       35900  ...   180.0   \n",
              "2       2015-02-03 22:00  T674582     50.0     12      146000  ...   370.0   \n",
              "3        2020-01-17 4:02  Y847238     20.0     18        6910  ...   120.0   \n",
              "4        2020-01-26 7:51  A872328     50.0     10      116000  ...   300.0   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "391934   2017-06-06 5:02  Y375615     20.0     27        6820  ...   110.0   \n",
              "391935   2019-10-16 0:36  K635567     10.0     12        3160  ...    80.0   \n",
              "391936  2021-03-23 22:35  J284147     30.0      8       60300  ...   200.0   \n",
              "391937   2015-01-08 7:15  J644215     30.0     29       23800  ...   170.0   \n",
              "391938  2015-06-08 23:30  D123358     30.0     15       50600  ...   260.0   \n",
              "\n",
              "        SHIPMANAGER                         FLAG  U_WIND V_WIND  \\\n",
              "0            CQSB78                       Panama     NaN    NaN   \n",
              "1            SPNO34             Marshall Islands     NaN    NaN   \n",
              "2            FNPK22                        Malta     NaN    NaN   \n",
              "3            PBZV77                      Bahamas   -3.18  -1.61   \n",
              "4            GUCE76                      Liberia   -0.33  -3.28   \n",
              "...             ...                          ...     ...    ...   \n",
              "391934       KEJZ24  China, People's Republic Of     NaN    NaN   \n",
              "391935       JLTM64                      Vietnam   -0.66   0.97   \n",
              "391936       YERJ68                    Singapore   -3.44   7.99   \n",
              "391937       HCZK58                      Comoros     NaN    NaN   \n",
              "391938       GRJG55             Hong Kong, China     NaN    NaN   \n",
              "\n",
              "       AIR_TEMPERATURE        BN  ATA_LT  PORT_SIZE     CI_HOUR  \n",
              "0                  NaN       NaN       5   0.002615    3.450000  \n",
              "1                  NaN       NaN      12   0.000217    0.000000  \n",
              "2                  NaN       NaN       6   0.001614    0.000000  \n",
              "3                  6.7  2.629350      13   0.000356    0.000000  \n",
              "4                 25.6  2.495953      15   0.002615  253.554444  \n",
              "...                ...       ...     ...        ...         ...  \n",
              "391934             NaN       NaN      14   0.000552    0.000000  \n",
              "391935            27.3  1.253491       8   0.002615  144.061389  \n",
              "391936            21.1  4.766257      18   0.000155   41.482222  \n",
              "391937             NaN       NaN      15   0.000990    7.485278  \n",
              "391938             NaN       NaN       7   0.000990    0.000000  \n",
              "\n",
              "[391939 rows x 23 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>DUBAI</th>\n",
              "      <th>BRENT</th>\n",
              "      <th>WTI</th>\n",
              "      <th>BDI_ADJ</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>30.736578</td>\n",
              "      <td>2020-10-15 4:03</td>\n",
              "      <td>Z517571</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>73100</td>\n",
              "      <td>...</td>\n",
              "      <td>3.77</td>\n",
              "      <td>15.9</td>\n",
              "      <td>2.730798</td>\n",
              "      <td>12</td>\n",
              "      <td>42.01</td>\n",
              "      <td>43.16</td>\n",
              "      <td>40.96</td>\n",
              "      <td>1407.668330</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>3.048333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>63.220425</td>\n",
              "      <td>2019-09-17 2:55</td>\n",
              "      <td>U467618</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37900</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.72</td>\n",
              "      <td>24.5</td>\n",
              "      <td>4.289058</td>\n",
              "      <td>10</td>\n",
              "      <td>67.53</td>\n",
              "      <td>64.55</td>\n",
              "      <td>59.34</td>\n",
              "      <td>2089.046774</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>17.138611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>90.427421</td>\n",
              "      <td>2019-02-23 6:43</td>\n",
              "      <td>V378315</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>115000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>65.30</td>\n",
              "      <td>66.39</td>\n",
              "      <td>56.94</td>\n",
              "      <td>603.193047</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>98.827500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-09-18 22:06</td>\n",
              "      <td>B726632</td>\n",
              "      <td>10.0</td>\n",
              "      <td>33</td>\n",
              "      <td>1490</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.31</td>\n",
              "      <td>22.1</td>\n",
              "      <td>4.693735</td>\n",
              "      <td>7</td>\n",
              "      <td>43.02</td>\n",
              "      <td>43.15</td>\n",
              "      <td>41.11</td>\n",
              "      <td>1169.853455</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>RU</td>\n",
              "      <td>NNC2</td>\n",
              "      <td>Container</td>\n",
              "      <td>8.813725</td>\n",
              "      <td>2022-08-13 12:57</td>\n",
              "      <td>D215135</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10</td>\n",
              "      <td>27600</td>\n",
              "      <td>...</td>\n",
              "      <td>2.31</td>\n",
              "      <td>22.8</td>\n",
              "      <td>2.345875</td>\n",
              "      <td>14</td>\n",
              "      <td>90.45</td>\n",
              "      <td>93.65</td>\n",
              "      <td>88.11</td>\n",
              "      <td>1107.944894</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>96.030556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367436</th>\n",
              "      <td>TRAIN_367436</td>\n",
              "      <td>CN</td>\n",
              "      <td>YRT6</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>59.018184</td>\n",
              "      <td>2017-11-11 22:23</td>\n",
              "      <td>J661243</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>93200</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>61.25</td>\n",
              "      <td>62.21</td>\n",
              "      <td>55.70</td>\n",
              "      <td>1333.609109</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>65.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367437</th>\n",
              "      <td>TRAIN_367437</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Tanker</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2022-04-29 2:58</td>\n",
              "      <td>D847216</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9</td>\n",
              "      <td>1280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.028558</td>\n",
              "      <td>11</td>\n",
              "      <td>105.37</td>\n",
              "      <td>109.34</td>\n",
              "      <td>104.69</td>\n",
              "      <td>1955.103846</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367438</th>\n",
              "      <td>TRAIN_367438</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>1.768630</td>\n",
              "      <td>2022-07-14 7:58</td>\n",
              "      <td>Q635545</td>\n",
              "      <td>30.0</td>\n",
              "      <td>6</td>\n",
              "      <td>25000</td>\n",
              "      <td>...</td>\n",
              "      <td>3.36</td>\n",
              "      <td>31.7</td>\n",
              "      <td>2.557156</td>\n",
              "      <td>15</td>\n",
              "      <td>97.73</td>\n",
              "      <td>99.10</td>\n",
              "      <td>95.78</td>\n",
              "      <td>1601.291086</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.997500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367439</th>\n",
              "      <td>TRAIN_367439</td>\n",
              "      <td>JP</td>\n",
              "      <td>TMR7</td>\n",
              "      <td>Cargo</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-12-22 10:07</td>\n",
              "      <td>N211282</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8</td>\n",
              "      <td>2400</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.44</td>\n",
              "      <td>10.8</td>\n",
              "      <td>3.055715</td>\n",
              "      <td>19</td>\n",
              "      <td>49.75</td>\n",
              "      <td>50.08</td>\n",
              "      <td>47.02</td>\n",
              "      <td>1191.353331</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367440</th>\n",
              "      <td>TRAIN_367440</td>\n",
              "      <td>CN</td>\n",
              "      <td>EKP8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>32.152412</td>\n",
              "      <td>2021-06-04 14:54</td>\n",
              "      <td>V628821</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10</td>\n",
              "      <td>87200</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>19.8</td>\n",
              "      <td>3.177475</td>\n",
              "      <td>22</td>\n",
              "      <td>70.10</td>\n",
              "      <td>71.89</td>\n",
              "      <td>69.62</td>\n",
              "      <td>2115.046707</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>8.464167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>367441 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     CN   EKP8               Bulk  30.736578   \n",
              "1       TRAIN_000001     CN   EUC8          Container  63.220425   \n",
              "2       TRAIN_000002     CN   NGG6          Container  90.427421   \n",
              "3       TRAIN_000003     JP   TMR7              Cargo   0.000000   \n",
              "4       TRAIN_000004     RU   NNC2          Container   8.813725   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "367436  TRAIN_367436     CN   YRT6               Bulk  59.018184   \n",
              "367437  TRAIN_367437     JP   QYY1             Tanker   0.000000   \n",
              "367438  TRAIN_367438     SG   GIW5          Container   1.768630   \n",
              "367439  TRAIN_367439     JP   TMR7              Cargo   0.000000   \n",
              "367440  TRAIN_367440     CN   EKP8               Bulk  32.152412   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  V_WIND  \\\n",
              "0        2020-10-15 4:03  Z517571     30.0     28       73100  ...    3.77   \n",
              "1        2019-09-17 2:55  U467618     30.0     15       37900  ...   -6.72   \n",
              "2        2019-02-23 6:43  V378315     50.0      7      115000  ...    0.00   \n",
              "3       2020-09-18 22:06  B726632     10.0     33        1490  ...   -7.31   \n",
              "4       2022-08-13 12:57  D215135     30.0     10       27600  ...    2.31   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "367436  2017-11-11 22:23  J661243     40.0     13       93200  ...     NaN   \n",
              "367437   2022-04-29 2:58  D847216     10.0      9        1280  ...    0.87   \n",
              "367438   2022-07-14 7:58  Q635545     30.0      6       25000  ...    3.36   \n",
              "367439  2020-12-22 10:07  N211282     10.0      8        2400  ...   -2.44   \n",
              "367440  2021-06-04 14:54  V628821     40.0     10       87200  ...   -0.84   \n",
              "\n",
              "        AIR_TEMPERATURE        BN  ATA_LT   DUBAI   BRENT     WTI  \\\n",
              "0                  15.9  2.730798      12   42.01   43.16   40.96   \n",
              "1                  24.5  4.289058      10   67.53   64.55   59.34   \n",
              "2                   9.4  0.000000      14   65.30   66.39   56.94   \n",
              "3                  22.1  4.693735       7   43.02   43.15   41.11   \n",
              "4                  22.8  2.345875      14   90.45   93.65   88.11   \n",
              "...                 ...       ...     ...     ...     ...     ...   \n",
              "367436              NaN       NaN       6   61.25   62.21   55.70   \n",
              "367437             17.1  1.028558      11  105.37  109.34  104.69   \n",
              "367438             31.7  2.557156      15   97.73   99.10   95.78   \n",
              "367439             10.8  3.055715      19   49.75   50.08   47.02   \n",
              "367440             19.8  3.177475      22   70.10   71.89   69.62   \n",
              "\n",
              "            BDI_ADJ  PORT_SIZE    CI_HOUR  \n",
              "0       1407.668330   0.001660   3.048333  \n",
              "1       2089.046774   0.001614  17.138611  \n",
              "2        603.193047   0.001743  98.827500  \n",
              "3       1169.853455   0.000069   0.000000  \n",
              "4       1107.944894   0.000197  96.030556  \n",
              "...             ...        ...        ...  \n",
              "367436  1333.609109   0.000360  65.850000  \n",
              "367437  1955.103846   0.000552   0.000000  \n",
              "367438  1601.291086   0.002615   0.997500  \n",
              "367439  1191.353331   0.000069   0.000000  \n",
              "367440  2115.046707   0.001660   8.464167  \n",
              "\n",
              "[367441 rows x 27 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#train = train[train['DIST'] != 0]\n",
        "#train = train.reset_index(drop = True)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['SAMPLE_ID'],axis=1,inplace=True)\n",
        "test.drop(['SAMPLE_ID'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "      <th>rounded_hour</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018</td>\n",
              "      <td>10</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  month  day  weekday  rounded_hour\n",
              "0  2020      6   18        3            12\n",
              "1  2021      5   26        2            22\n",
              "2  2019     12   16        0             0\n",
              "3  2015     11   16        0             6\n",
              "4  2018     10   24        2             1"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['ATA'] = pd.to_datetime(train['ATA'])\n",
        "test['ATA'] = pd.to_datetime(test['ATA'])\n",
        "train['year'] = train['ATA'].dt.year\n",
        "train['month'] = train['ATA'].dt.month\n",
        "train['day'] = train['ATA'].dt.day\n",
        "train['weekday'] = train['ATA'].dt.dayofweek\n",
        "train['rounded_hour'] = (train['ATA'].dt.hour + (train['ATA'].dt.minute // 30)).apply(lambda x: 0 if x == 24 else x)\n",
        "test['year'] = test['ATA'].dt.year\n",
        "test['month'] = test['ATA'].dt.month\n",
        "test['day'] = test['ATA'].dt.day\n",
        "test['weekday'] = test['ATA'].dt.dayofweek\n",
        "test['rounded_hour'] = (test['ATA'].dt.hour + (test['ATA'].dt.minute // 30)).apply(lambda x: 0 if x == 24 else x)\n",
        "\n",
        "# sin, cos 변환 함수 정의\n",
        "def encode_cyclic_feature(data, column, max_val):\n",
        "    data[column + '_sin'] = np.sin(2 * np.pi * data[column] / max_val)\n",
        "    data[column + '_cos'] = np.cos(2 * np.pi * data[column] / max_val)\n",
        "    return data\n",
        "\n",
        "# 각 피처에 대해 sin, cos 변환 수행\n",
        "train = encode_cyclic_feature(train, 'month', 12)\n",
        "train = encode_cyclic_feature(train, 'day', 31)\n",
        "train = encode_cyclic_feature(train, 'weekday', 7)\n",
        "train = encode_cyclic_feature(train, 'rounded_hour', 24)\n",
        "test = encode_cyclic_feature(test, 'month', 12)\n",
        "test = encode_cyclic_feature(test, 'day', 31)\n",
        "test = encode_cyclic_feature(test, 'weekday', 7)\n",
        "test = encode_cyclic_feature(test, 'rounded_hour', 24)\n",
        "\n",
        "train.drop(['ATA'],axis=1,inplace=True)\n",
        "test.drop(['ATA'],axis=1,inplace=True)\n",
        "\n",
        "test[['year', 'month', 'day', 'weekday', 'rounded_hour']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 244989 entries, 0 to 244988\n",
            "Data columns (total 37 columns):\n",
            " #   Column              Non-Null Count   Dtype  \n",
            "---  ------              --------------   -----  \n",
            " 0   ARI_CO              244989 non-null  object \n",
            " 1   ARI_PO              244989 non-null  object \n",
            " 2   SHIP_TYPE_CATEGORY  244989 non-null  object \n",
            " 3   DIST                244989 non-null  float64\n",
            " 4   ID                  244989 non-null  object \n",
            " 5   BREADTH             244989 non-null  float64\n",
            " 6   BUILT               244989 non-null  int64  \n",
            " 7   DEADWEIGHT          244989 non-null  int64  \n",
            " 8   DEPTH               244989 non-null  float64\n",
            " 9   DRAUGHT             244989 non-null  float64\n",
            " 10  GT                  244989 non-null  int64  \n",
            " 11  LENGTH              244989 non-null  float64\n",
            " 12  SHIPMANAGER         244989 non-null  object \n",
            " 13  FLAG                244989 non-null  object \n",
            " 14  U_WIND              143062 non-null  float64\n",
            " 15  V_WIND              143062 non-null  float64\n",
            " 16  AIR_TEMPERATURE     142478 non-null  float64\n",
            " 17  BN                  143062 non-null  float64\n",
            " 18  ATA_LT              244989 non-null  int64  \n",
            " 19  DUBAI               244989 non-null  float64\n",
            " 20  BRENT               244989 non-null  float64\n",
            " 21  WTI                 244989 non-null  float64\n",
            " 22  BDI_ADJ             244989 non-null  float64\n",
            " 23  PORT_SIZE           244989 non-null  float64\n",
            " 24  year                244989 non-null  int64  \n",
            " 25  month               244989 non-null  int64  \n",
            " 26  day                 244989 non-null  int64  \n",
            " 27  weekday             244989 non-null  int64  \n",
            " 28  rounded_hour        244989 non-null  int64  \n",
            " 29  month_sin           244989 non-null  float64\n",
            " 30  month_cos           244989 non-null  float64\n",
            " 31  day_sin             244989 non-null  float64\n",
            " 32  day_cos             244989 non-null  float64\n",
            " 33  weekday_sin         244989 non-null  float64\n",
            " 34  weekday_cos         244989 non-null  float64\n",
            " 35  rounded_hour_sin    244989 non-null  float64\n",
            " 36  rounded_hour_cos    244989 non-null  float64\n",
            "dtypes: float64(22), int64(9), object(6)\n",
            "memory usage: 69.2+ MB\n"
          ]
        }
      ],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = train[['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin', 'BDI_ADJ', 'CI_HOUR', 'ARI_CO', 'ARI_PO', 'ATA_LT', 'DIST']].copy()\n",
        "test = test[['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin', 'BDI_ADJ' , 'ARI_CO', 'ARI_PO', 'ATA_LT', 'DIST']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = train[['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin', 'BDI_ADJ', 'CI_HOUR']].copy()\n",
        "test = test[['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin', 'BDI_ADJ']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x = train.drop(['CI_HOUR'],axis=1)\n",
        "train_y = train[['CI_HOUR']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# ARI_CO + weekday 결합 열 생성\\ntrain[\\'fold_column\\'] = train[\\'ARI_CO\\'].astype(str) + \"_\" + train[\\'weekday\\'].astype(str)\\n\\n# Stratified 5-fold 생성\\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=313)\\n\\n# 각 폴드마다의 예측값과 MAE를 저장할 리스트\\npredictions = []\\nmae_scores = []\\n\\n# StratifiedKFold는 레이블 분포를 기반으로 하기 때문에, 너무 적은 빈도의 조합은 오류를 발생시킬 수 있습니다.\\nmin_samples = train[\\'fold_column\\'].value_counts().min()\\nfiltered_train = train[train[\\'fold_column\\'].map(train[\\'fold_column\\'].value_counts()) > min_samples]\\n\\nfor train_idx, val_idx in skf.split(filtered_train, filtered_train[\\'fold_column\\']):\\n    train_fold = filtered_train.iloc[train_idx].drop(columns=[\\'fold_column\\'])\\n    val_fold = filtered_train.iloc[val_idx].drop(columns=[\\'fold_column\\'])\\n    train_data = TabularDataset(train_fold)\\n    val_data = TabularDataset(val_fold)\\n    test_data = TabularDataset(test)\\n    \\n    # 각 폴드마다 모델 학습\\n    predictor = TabularPredictor(label=\\'CI_HOUR\\', eval_metric=\\'mean_absolute_error\\').fit(\\n        train_data, \\n        presets=\\'medium_quality\\',\\n        ag_args_fit={\\'num_gpus\\': 0},\\n        included_model_types=[\\'CAT\\']\\n    )\\n    \\n    # validation set에 대한 예측값 계산\\n    val_predictions = predictor.predict(val_data.drop(columns=\\'CI_HOUR\\'))\\n    \\n    # MAE 계산 후 저장\\n    mae = mean_absolute_error(val_data[\\'CI_HOUR\\'], val_predictions)\\n    mae_scores.append(mae)\\n    \\n    # test set에 대한 예측값 저장\\n    predictions.append(predictor.predict(test_data))\\n\\n# 예측값의 평균 계산\\nfinal_predictions = np.mean(predictions, axis=0)\\ny_pred = pd.DataFrame(final_predictions, columns=[\\'CI_HOUR\\'])\\nsubmission[\\'CI_HOUR\\'] = y_pred[\\'CI_HOUR\\']\\nmae_scores, np.mean(mae_scores)\\n'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "'''\n",
        "# ARI_CO + weekday 결합 열 생성\n",
        "train['fold_column'] = train['ARI_CO'].astype(str) + \"_\" + train['weekday'].astype(str)\n",
        "\n",
        "# Stratified 5-fold 생성\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=313)\n",
        "\n",
        "# 각 폴드마다의 예측값과 MAE를 저장할 리스트\n",
        "predictions = []\n",
        "mae_scores = []\n",
        "\n",
        "# StratifiedKFold는 레이블 분포를 기반으로 하기 때문에, 너무 적은 빈도의 조합은 오류를 발생시킬 수 있습니다.\n",
        "min_samples = train['fold_column'].value_counts().min()\n",
        "filtered_train = train[train['fold_column'].map(train['fold_column'].value_counts()) > min_samples]\n",
        "\n",
        "for train_idx, val_idx in skf.split(filtered_train, filtered_train['fold_column']):\n",
        "    train_fold = filtered_train.iloc[train_idx].drop(columns=['fold_column'])\n",
        "    val_fold = filtered_train.iloc[val_idx].drop(columns=['fold_column'])\n",
        "    train_data = TabularDataset(train_fold)\n",
        "    val_data = TabularDataset(val_fold)\n",
        "    test_data = TabularDataset(test)\n",
        "    \n",
        "    # 각 폴드마다 모델 학습\n",
        "    predictor = TabularPredictor(label='CI_HOUR', eval_metric='mean_absolute_error').fit(\n",
        "        train_data, \n",
        "        presets='medium_quality',\n",
        "        ag_args_fit={'num_gpus': 0},\n",
        "        included_model_types=['CAT']\n",
        "    )\n",
        "    \n",
        "    # validation set에 대한 예측값 계산\n",
        "    val_predictions = predictor.predict(val_data.drop(columns='CI_HOUR'))\n",
        "    \n",
        "    # MAE 계산 후 저장\n",
        "    mae = mean_absolute_error(val_data['CI_HOUR'], val_predictions)\n",
        "    mae_scores.append(mae)\n",
        "    \n",
        "    # test set에 대한 예측값 저장\n",
        "    predictions.append(predictor.predict(test_data))\n",
        "\n",
        "# 예측값의 평균 계산\n",
        "final_predictions = np.mean(predictions, axis=0)\n",
        "y_pred = pd.DataFrame(final_predictions, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred['CI_HOUR']\n",
        "mae_scores, np.mean(mae_scores)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_031040\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_031040\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.96 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    41214\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2158.901944, 0.0, 108.52434, 288.64109)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21710.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 36.84 MB (0.2% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GIW5\n",
            "GIW5\n",
            "GIW5\n",
            "GIW5\n",
            "GIW5\n",
            "41214\n",
            "41214\n",
            "41214\n",
            "41214\n",
            "41214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.4s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 11.87 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.45s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.060658999369146406, Train Rows: 38714, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-60.2822\t = Validation score   (-mean_absolute_error)\n",
            "\t670.36s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-60.2822\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 671.08s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_031040\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_032152\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_032152\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.89 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    27243\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2116.295556, 0.0, 42.78751, 123.31688)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21823.32 MB\n",
            "\tTrain Data (Original)  Memory Usage: 24.44 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NGG6\n",
            "NGG6\n",
            "NGG6\n",
            "NGG6\n",
            "NGG6\n",
            "27243\n",
            "27243\n",
            "27243\n",
            "27243\n",
            "27243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.4s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 7.85 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.49s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.09176669236134052, Train Rows: 24743, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-31.9993\t = Validation score   (-mean_absolute_error)\n",
            "\t163.88s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-31.9993\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 164.5s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_032152\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_032437\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_032437\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.87 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    26070\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2148.446389, 0.0, 34.94739, 101.04002)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21814.07 MB\n",
            "\tTrain Data (Original)  Memory Usage: 23.36 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EKP8\n",
            "EKP8\n",
            "EKP8\n",
            "EKP8\n",
            "EKP8\n",
            "26070\n",
            "26070\n",
            "26070\n",
            "26070\n",
            "26070\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 4): ['ARI_PO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 3): ['mean_enc_ARI_CO', 'std_enc_ARI_CO', 'PORT_SIZE_Zone']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', [])  : 2 | ['mean_enc_ARI_CO', 'std_enc_ARI_CO']\n",
            "\t\t('object', []) : 1 | ['PORT_SIZE_Zone']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 29 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  8 | ['ARI_CO', 'SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['ARI_CO', 'PORT_SIZE']\n",
            "\t0.6s = Fit runtime\n",
            "\t46 features in original data used to generate 46 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 7.56 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.69s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.09589566551591867, Train Rows: 23570, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-26.9454\t = Validation score   (-mean_absolute_error)\n",
            "\t573.55s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-26.9454\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 574.5s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_032437\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033411\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033411\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.84 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    22995\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2126.3375, 0.0, 23.12988, 104.99103)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22032.46 MB\n",
            "\tTrain Data (Original)  Memory Usage: 20.58 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JWI3\n",
            "JWI3\n",
            "JWI3\n",
            "JWI3\n",
            "JWI3\n",
            "22995\n",
            "22995\n",
            "22995\n",
            "22995\n",
            "22995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.4s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 6.63 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.43s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 20695, Val Rows: 2300\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-23.4259\t = Validation score   (-mean_absolute_error)\n",
            "\t20.1s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-23.4259\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 20.64s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033411\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033432\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033432\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.84 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    21263\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2157.861389, 0.0, 55.46718, 172.33473)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22027.17 MB\n",
            "\tTrain Data (Original)  Memory Usage: 19.07 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EUC8\n",
            "EUC8\n",
            "EUC8\n",
            "EUC8\n",
            "EUC8\n",
            "21263\n",
            "21263\n",
            "21263\n",
            "21263\n",
            "21263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.3s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 6.13 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.4s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 19136, Val Rows: 2127\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-45.4334\t = Validation score   (-mean_absolute_error)\n",
            "\t13.61s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-45.4334\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033432\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033447\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033447\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.83 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    15803\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2105.045, 0.0, 19.3953, 95.21709)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22042.27 MB\n",
            "\tTrain Data (Original)  Memory Usage: 14.12 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QYY1\n",
            "QYY1\n",
            "QYY1\n",
            "QYY1\n",
            "QYY1\n",
            "15803\n",
            "15803\n",
            "15803\n",
            "15803\n",
            "15803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.3s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 4.55 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.35s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 14222, Val Rows: 1581\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-16.1147\t = Validation score   (-mean_absolute_error)\n",
            "\t12.08s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-16.1147\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.52s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033447\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033459\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033459\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.82 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    10253\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
            "\tLabel info (max, min, mean, stddev): (1362.646111, 0.0, 1.38527, 25.97867)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22006.53 MB\n",
            "\tTrain Data (Original)  Memory Usage: 9.19 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NCU8\n",
            "NCU8\n",
            "NCU8\n",
            "NCU8\n",
            "NCU8\n",
            "10253\n",
            "10253\n",
            "10253\n",
            "10253\n",
            "10253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.3s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.96 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.3s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9227, Val Rows: 1026\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-1.7321\t = Validation score   (-mean_absolute_error)\n",
            "\t12.78s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-1.7321\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.19s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033459\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033513\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033513\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.82 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    9408\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1159.246111, 0.0, 90.14209, 91.9508)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21990.39 MB\n",
            "\tTrain Data (Original)  Memory Usage: 8.37 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WHH4\n",
            "WHH4\n",
            "WHH4\n",
            "WHH4\n",
            "WHH4\n",
            "9408\n",
            "9408\n",
            "9408\n",
            "9408\n",
            "9408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.71 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.28s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8467, Val Rows: 941\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-53.2302\t = Validation score   (-mean_absolute_error)\n",
            "\t90.25s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-53.2302\t = Validation score   (-mean_absolute_error)\n",
            "\t0.0s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 90.63s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033513\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033643\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033643\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.81 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    7992\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1956.061111, 0.0, 34.19422, 110.19482)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22036.85 MB\n",
            "\tTrain Data (Original)  Memory Usage: 7.17 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WEY7\n",
            "WEY7\n",
            "WEY7\n",
            "WEY7\n",
            "WEY7\n",
            "7992\n",
            "7992\n",
            "7992\n",
            "7992\n",
            "7992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.31 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.27s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7192, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-27.0674\t = Validation score   (-mean_absolute_error)\n",
            "\t22.29s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-27.0674\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 22.67s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033643\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033706\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033706\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.80 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    7959\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2082.405, 0.0, 59.47002, 119.03786)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22020.12 MB\n",
            "\tTrain Data (Original)  Memory Usage: 7.15 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JEN5\n",
            "JEN5\n",
            "JEN5\n",
            "JEN5\n",
            "JEN5\n",
            "7959\n",
            "7959\n",
            "7959\n",
            "7959\n",
            "7959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.3 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.27s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7163, Val Rows: 796\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-49.1888\t = Validation score   (-mean_absolute_error)\n",
            "\t26.53s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-49.1888\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 26.89s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033706\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033733\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033733\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.80 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    7568\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2102.433333, 0.0, 46.34137, 153.57297)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21997.66 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.73 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ZAG4\n",
            "ZAG4\n",
            "ZAG4\n",
            "ZAG4\n",
            "ZAG4\n",
            "7568\n",
            "7568\n",
            "7568\n",
            "7568\n",
            "7568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.18 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.27s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6811, Val Rows: 757\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-42.8422\t = Validation score   (-mean_absolute_error)\n",
            "\t12.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-42.8422\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.99s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033733\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033746\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033746\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.80 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    7056\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2129.140833, 0.0, 66.71269, 121.37896)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21985.39 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.34 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JTD1\n",
            "JTD1\n",
            "JTD1\n",
            "JTD1\n",
            "JTD1\n",
            "7056\n",
            "7056\n",
            "7056\n",
            "7056\n",
            "7056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.04 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.26s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6350, Val Rows: 706\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-46.6242\t = Validation score   (-mean_absolute_error)\n",
            "\t19.61s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-46.6242\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.96s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033746\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033807\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033807\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.79 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    6791\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1982.005278, 0.0, 42.67987, 119.85316)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21965.36 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QQW1\n",
            "QQW1\n",
            "QQW1\n",
            "QQW1\n",
            "QQW1\n",
            "6791\n",
            "6791\n",
            "6791\n",
            "6791\n",
            "6791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.96 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.26s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6111, Val Rows: 680\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-34.4332\t = Validation score   (-mean_absolute_error)\n",
            "\t12.24s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-34.4332\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.6s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033807\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033819\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033819\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.79 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    6695\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2132.006111, 0.0, 95.34582, 139.73533)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21959.66 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.05 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YRT6\n",
            "YRT6\n",
            "YRT6\n",
            "YRT6\n",
            "YRT6\n",
            "6695\n",
            "6695\n",
            "6695\n",
            "6695\n",
            "6695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.93 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.25s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6025, Val Rows: 670\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-54.1797\t = Validation score   (-mean_absolute_error)\n",
            "\t67.87s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-54.1797\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 68.23s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033819\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033928\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033928\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.78 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    6146\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2136.308889, 0.0, 63.59897, 124.29261)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21968.78 MB\n",
            "\tTrain Data (Original)  Memory Usage: 5.5 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TDA5\n",
            "TDA5\n",
            "TDA5\n",
            "TDA5\n",
            "TDA5\n",
            "6146\n",
            "6146\n",
            "6146\n",
            "6146\n",
            "6146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.77 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.26s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 5531, Val Rows: 615\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-45.9086\t = Validation score   (-mean_absolute_error)\n",
            "\t49.68s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-45.9086\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 50.05s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033928\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034018\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034018\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.78 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    5311\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2113.031111, 0.0, 47.52771, 144.75331)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21966.95 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MIA8\n",
            "MIA8\n",
            "MIA8\n",
            "MIA8\n",
            "MIA8\n",
            "5311\n",
            "5311\n",
            "5311\n",
            "5311\n",
            "5311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.53 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 4779, Val Rows: 532\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-42.2842\t = Validation score   (-mean_absolute_error)\n",
            "\t17.34s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-42.2842\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.67s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034018\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034036\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034036\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.78 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4913\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1360.705833, 0.0, 42.80297, 84.08974)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21940.64 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.37 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIB7\n",
            "AIB7\n",
            "AIB7\n",
            "AIB7\n",
            "AIB7\n",
            "4913\n",
            "4913\n",
            "4913\n",
            "4913\n",
            "4913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.42 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.24s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1017708121310808, Train Rows: 4413, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-28.9496\t = Validation score   (-mean_absolute_error)\n",
            "\t12.67s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-28.9496\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.01s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034036\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034049\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034049\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.77 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4787\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2109.445, 0.0, 81.79908, 143.94335)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21898.8 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.27 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFM2\n",
            "FFM2\n",
            "FFM2\n",
            "FFM2\n",
            "FFM2\n",
            "4787\n",
            "4787\n",
            "4787\n",
            "4787\n",
            "4787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.38 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.10444955086693128, Train Rows: 4286, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-60.114\t = Validation score   (-mean_absolute_error)\n",
            "\t11.29s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-60.114\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.6s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034049\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034101\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034101\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.77 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4694\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2083.849722, 0.0, 36.2394, 112.59804)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21925.23 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.17 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HYG5\n",
            "HYG5\n",
            "HYG5\n",
            "HYG5\n",
            "HYG5\n",
            "4694\n",
            "4694\n",
            "4694\n",
            "4694\n",
            "4694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.36 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.10651896037494674, Train Rows: 4194, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-35.1917\t = Validation score   (-mean_absolute_error)\n",
            "\t14.12s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-35.1917\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.42s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034101\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034115\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034115\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.77 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4138\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2073.600556, 0.0, 35.07902, 136.91397)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21926.55 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.69 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BGD2\n",
            "BGD2\n",
            "BGD2\n",
            "BGD2\n",
            "BGD2\n",
            "4138\n",
            "4138\n",
            "4138\n",
            "4138\n",
            "4138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.2 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1208313194780087, Train Rows: 3638, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-43.2944\t = Validation score   (-mean_absolute_error)\n",
            "\t22.88s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-43.2944\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 23.36s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034115\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034139\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034139\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.77 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4010\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2150.8, 0.0, 122.56764, 168.50125)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21949.75 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.57 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NQO4\n",
            "NQO4\n",
            "NQO4\n",
            "NQO4\n",
            "NQO4\n",
            "4010\n",
            "4010\n",
            "4010\n",
            "4010\n",
            "4010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.16 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.12468827930174564, Train Rows: 3510, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-92.9599\t = Validation score   (-mean_absolute_error)\n",
            "\t16.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-92.9599\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 16.82s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034139\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034156\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034156\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.76 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3885\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2147.478056, 0.0, 129.74604, 299.71952)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21907.91 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.46 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FCD5\n",
            "FCD5\n",
            "FCD5\n",
            "FCD5\n",
            "FCD5\n",
            "3885\n",
            "3885\n",
            "3885\n",
            "3885\n",
            "3885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.12 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1287001287001287, Train Rows: 3385, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-119.0399\t = Validation score   (-mean_absolute_error)\n",
            "\t224.55s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-119.0399\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 224.9s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034156\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034541\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034541\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.75 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3805\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2102.570833, 0.0, 39.37323, 170.90898)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21985.82 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.4 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JER7\n",
            "JER7\n",
            "JER7\n",
            "JER7\n",
            "JER7\n",
            "3805\n",
            "3805\n",
            "3805\n",
            "3805\n",
            "3805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.1 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1314060446780552, Train Rows: 3305, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-43.0196\t = Validation score   (-mean_absolute_error)\n",
            "\t11.08s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-43.0196\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.39s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034541\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034552\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034552\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.75 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3554\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2134.278333, 0.0, 84.48681, 208.12173)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21949.59 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LXJ7\n",
            "LXJ7\n",
            "LXJ7\n",
            "LXJ7\n",
            "LXJ7\n",
            "3554\n",
            "3554\n",
            "3554\n",
            "3554\n",
            "3554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.14068655036578503, Train Rows: 3054, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-77.0219\t = Validation score   (-mean_absolute_error)\n",
            "\t12.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-77.0219\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.72s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034552\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034605\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034605\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.75 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3520\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1871.841667, 0.0, 221.06208, 218.10843)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21985.74 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.13 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KSF1\n",
            "KSF1\n",
            "KSF1\n",
            "KSF1\n",
            "KSF1\n",
            "3520\n",
            "3520\n",
            "3520\n",
            "3520\n",
            "3520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.02 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.14204545454545456, Train Rows: 3019, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-136.9769\t = Validation score   (-mean_absolute_error)\n",
            "\t17.83s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-136.9769\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 18.12s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034605\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034623\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034623\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.75 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3515\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2078.504722, 0.0, 178.71886, 223.27654)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21972.23 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.13 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TMW2\n",
            "TMW2\n",
            "TMW2\n",
            "TMW2\n",
            "TMW2\n",
            "3515\n",
            "3515\n",
            "3515\n",
            "3515\n",
            "3515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.02 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1422475106685633, Train Rows: 3014, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-108.8703\t = Validation score   (-mean_absolute_error)\n",
            "\t114.91s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-108.8703\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 115.25s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034623\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034819\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034819\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.74 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3349\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1568.539167, 0.0, 44.52857, 96.45209)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21979.17 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.0 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XVZ3\n",
            "XVZ3\n",
            "XVZ3\n",
            "XVZ3\n",
            "XVZ3\n",
            "3349\n",
            "3349\n",
            "3349\n",
            "3349\n",
            "3349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.97 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1492982979994028, Train Rows: 2849, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-39.1893\t = Validation score   (-mean_absolute_error)\n",
            "\t17.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-39.1893\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.95s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034819\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034837\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034837\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.74 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3342\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1121.085, 0.0, 90.40821, 95.40416)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21942.77 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.97 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YDP4\n",
            "YDP4\n",
            "YDP4\n",
            "YDP4\n",
            "YDP4\n",
            "3342\n",
            "3342\n",
            "3342\n",
            "3342\n",
            "3342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.97 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.14961101137043686, Train Rows: 2842, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-52.9934\t = Validation score   (-mean_absolute_error)\n",
            "\t25.56s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-52.9934\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 25.87s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034837\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034903\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034903\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.74 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3137\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2039.986944, 0.0, 190.8984, 242.32361)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21919.81 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.79 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VFD8\n",
            "VFD8\n",
            "VFD8\n",
            "VFD8\n",
            "VFD8\n",
            "3137\n",
            "3137\n",
            "3137\n",
            "3137\n",
            "3137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.91 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1593879502709595, Train Rows: 2637, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-136.9176\t = Validation score   (-mean_absolute_error)\n",
            "\t54.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-136.9176\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 54.94s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034903\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034958\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034958\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.73 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3064\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1862.48, 0.0, 69.99151, 140.95876)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21835.64 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.73 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QTU5\n",
            "QTU5\n",
            "QTU5\n",
            "QTU5\n",
            "QTU5\n",
            "3064\n",
            "3064\n",
            "3064\n",
            "3064\n",
            "3064\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.89 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.16318537859007834, Train Rows: 2563, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-56.8586\t = Validation score   (-mean_absolute_error)\n",
            "\t13.66s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-56.8586\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.96s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034958\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035012\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035012\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.73 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3032\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1943.883611, 0.0, 63.90981, 153.28187)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21839.75 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.7 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UJM2\n",
            "UJM2\n",
            "UJM2\n",
            "UJM2\n",
            "UJM2\n",
            "3032\n",
            "3032\n",
            "3032\n",
            "3032\n",
            "3032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.88 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.16490765171503957, Train Rows: 2532, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-54.0115\t = Validation score   (-mean_absolute_error)\n",
            "\t19.5s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-54.0115\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.8s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035012\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035032\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035032\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.73 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2950\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2074.1025, 0.0, 66.00011, 84.48183)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21839.47 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.62 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WXQ2\n",
            "WXQ2\n",
            "WXQ2\n",
            "WXQ2\n",
            "WXQ2\n",
            "2950\n",
            "2950\n",
            "2950\n",
            "2950\n",
            "2950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.85 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1694915254237288, Train Rows: 2450, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-39.6865\t = Validation score   (-mean_absolute_error)\n",
            "\t14.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-39.6865\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.01s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035032\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035047\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035047\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2887\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2151.602778, 0.0, 48.69182, 153.9367)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21810.06 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TMR7\n",
            "TMR7\n",
            "TMR7\n",
            "TMR7\n",
            "TMR7\n",
            "2887\n",
            "2887\n",
            "2887\n",
            "2887\n",
            "2887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.83 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.17319016279875304, Train Rows: 2386, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-49.7143\t = Validation score   (-mean_absolute_error)\n",
            "\t10.46s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-49.7143\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.72s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035047\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035058\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035058\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2853\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.0, 68.77393, 196.47248)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21866.64 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.54 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VYJ1\n",
            "VYJ1\n",
            "VYJ1\n",
            "VYJ1\n",
            "VYJ1\n",
            "2853\n",
            "2853\n",
            "2853\n",
            "2853\n",
            "2853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.83 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1752541184717841, Train Rows: 2352, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-73.4274\t = Validation score   (-mean_absolute_error)\n",
            "\t11.55s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-73.4274\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.84s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035058\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035110\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035110\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2746\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2069.403889, 0.0, 47.43914, 129.15143)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21865.95 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.44 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SLZ5\n",
            "SLZ5\n",
            "SLZ5\n",
            "SLZ5\n",
            "SLZ5\n",
            "2746\n",
            "2746\n",
            "2746\n",
            "2746\n",
            "2746\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.79 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1820830298616169, Train Rows: 2246, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-42.0307\t = Validation score   (-mean_absolute_error)\n",
            "\t13.79s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-42.0307\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.08s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035110\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035124\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035124\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2489\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2104.006944, 0.0, 79.38305, 165.08043)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21887.52 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.22 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NNC2\n",
            "NNC2\n",
            "NNC2\n",
            "NNC2\n",
            "NNC2\n",
            "2489\n",
            "2489\n",
            "2489\n",
            "2489\n",
            "2489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.72 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1991, Val Rows: 498\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-66.3843\t = Validation score   (-mean_absolute_error)\n",
            "\t16.17s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-66.3843\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 16.46s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035124\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035140\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035140\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2470\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1965.065556, 0.0, 17.86022, 69.16307)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21871.49 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.21 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UVK6\n",
            "UVK6\n",
            "UVK6\n",
            "UVK6\n",
            "UVK6\n",
            "2470\n",
            "2470\n",
            "2470\n",
            "2470\n",
            "2470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.71 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1976, Val Rows: 494\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-16.4269\t = Validation score   (-mean_absolute_error)\n",
            "\t15.35s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-16.4269\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.64s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035140\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035156\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035156\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2430\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1882.755833, 0.0, 32.62602, 108.89686)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21881.56 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DMD4\n",
            "DMD4\n",
            "DMD4\n",
            "DMD4\n",
            "DMD4\n",
            "2430\n",
            "2430\n",
            "2430\n",
            "2430\n",
            "2430\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.7 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1944, Val Rows: 486\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-29.2154\t = Validation score   (-mean_absolute_error)\n",
            "\t12.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-29.2154\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.81s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035156\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035209\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035209\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2422\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2141.815, 0.0, 30.43492, 118.45549)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21891.28 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TXO3\n",
            "TXO3\n",
            "TXO3\n",
            "TXO3\n",
            "TXO3\n",
            "2422\n",
            "2422\n",
            "2422\n",
            "2422\n",
            "2422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.7 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1937, Val Rows: 485\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-34.0619\t = Validation score   (-mean_absolute_error)\n",
            "\t12.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-34.0619\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.16s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035209\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035222\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035222\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2354\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2125.051667, 0.0, 75.3961, 217.78511)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21861.94 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.1 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VXM8\n",
            "VXM8\n",
            "VXM8\n",
            "VXM8\n",
            "VXM8\n",
            "2354\n",
            "2354\n",
            "2354\n",
            "2354\n",
            "2354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.68 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1883, Val Rows: 471\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-77.796\t = Validation score   (-mean_absolute_error)\n",
            "\t19.65s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-77.796\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.98s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035222\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035242\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035242\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2303\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2116.804167, 0.0, 77.67734, 197.6834)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21836.21 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.05 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AZU6\n",
            "AZU6\n",
            "AZU6\n",
            "AZU6\n",
            "AZU6\n",
            "2303\n",
            "2303\n",
            "2303\n",
            "2303\n",
            "2303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1842, Val Rows: 461\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-79.0597\t = Validation score   (-mean_absolute_error)\n",
            "\t17.55s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-79.0597\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.87s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035242\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035300\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035300\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2300\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
            "\tLabel info (max, min, mean, stddev): (1397.805556, 0.0, 1.10592, 32.89574)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21853.58 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.06 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RKA2\n",
            "RKA2\n",
            "RKA2\n",
            "RKA2\n",
            "RKA2\n",
            "2300\n",
            "2300\n",
            "2300\n",
            "2300\n",
            "2300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1840, Val Rows: 460\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-3.1716\t = Validation score   (-mean_absolute_error)\n",
            "\t11.68s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-3.1716\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.97s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035300\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035313\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035313\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2281\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1905.061944, 0.0, 69.69806, 158.17039)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21844.8 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.02 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IEW6\n",
            "IEW6\n",
            "IEW6\n",
            "IEW6\n",
            "IEW6\n",
            "2281\n",
            "2281\n",
            "2281\n",
            "2281\n",
            "2281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.66 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1824, Val Rows: 457\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-61.2653\t = Validation score   (-mean_absolute_error)\n",
            "\t11.97s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-61.2653\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.25s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035313\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035325\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035325\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2083\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1499.908056, 0.0, 30.56187, 81.47758)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21852.45 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.87 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WAF5\n",
            "WAF5\n",
            "WAF5\n",
            "WAF5\n",
            "WAF5\n",
            "2083\n",
            "2083\n",
            "2083\n",
            "2083\n",
            "2083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.6 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1666, Val Rows: 417\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-31.7412\t = Validation score   (-mean_absolute_error)\n",
            "\t12.96s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-31.7412\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.25s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035325\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035338\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035338\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2073\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1831.358889, 0.0, 21.18246, 93.75764)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21893.9 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.85 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URE2\n",
            "URE2\n",
            "URE2\n",
            "URE2\n",
            "URE2\n",
            "2073\n",
            "2073\n",
            "2073\n",
            "2073\n",
            "2073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.6 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1658, Val Rows: 415\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-24.4263\t = Validation score   (-mean_absolute_error)\n",
            "\t12.31s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-24.4263\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.61s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035338\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035351\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035351\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1959\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2144.961944, 0.0, 94.37881, 177.51358)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21884.58 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.75 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MMW5\n",
            "MMW5\n",
            "MMW5\n",
            "MMW5\n",
            "MMW5\n",
            "1959\n",
            "1959\n",
            "1959\n",
            "1959\n",
            "1959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.57 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1567, Val Rows: 392\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-74.0738\t = Validation score   (-mean_absolute_error)\n",
            "\t14.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-74.0738\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.42s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035351\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035406\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035406\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1876\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1913.126389, 0.0, 50.18176, 134.07432)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21888.97 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.67 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UIR7\n",
            "UIR7\n",
            "UIR7\n",
            "UIR7\n",
            "UIR7\n",
            "1876\n",
            "1876\n",
            "1876\n",
            "1876\n",
            "1876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.54 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1500, Val Rows: 376\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-38.6465\t = Validation score   (-mean_absolute_error)\n",
            "\t10.82s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-38.6465\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035406\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035417\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035417\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1704\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1814.791389, 0.0, 64.76176, 154.89062)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21889.54 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.52 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TYY2\n",
            "TYY2\n",
            "TYY2\n",
            "TYY2\n",
            "TYY2\n",
            "1704\n",
            "1704\n",
            "1704\n",
            "1704\n",
            "1704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.49 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1363, Val Rows: 341\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-51.7079\t = Validation score   (-mean_absolute_error)\n",
            "\t9.83s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-51.7079\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.13s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035417\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035427\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035427\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1694\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2019.872778, 0.0, 51.59201, 179.39584)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21887.19 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.5 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KIU2\n",
            "KIU2\n",
            "KIU2\n",
            "KIU2\n",
            "KIU2\n",
            "1694\n",
            "1694\n",
            "1694\n",
            "1694\n",
            "1694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.49 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1355, Val Rows: 339\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-61.0615\t = Validation score   (-mean_absolute_error)\n",
            "\t12.21s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-61.0615\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.47s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035427\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035440\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035440\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1660\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2080.562222, 0.0, 44.19496, 164.82891)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21892.2 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.47 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MCG4\n",
            "MCG4\n",
            "MCG4\n",
            "MCG4\n",
            "MCG4\n",
            "1660\n",
            "1660\n",
            "1660\n",
            "1660\n",
            "1660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.48 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1328, Val Rows: 332\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-40.8351\t = Validation score   (-mean_absolute_error)\n",
            "\t11.48s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-40.8351\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.74s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035440\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035452\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035452\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1608\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2091.578889, 0.0, 106.35088, 270.13582)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21872.5 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.42 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CEI5\n",
            "CEI5\n",
            "CEI5\n",
            "CEI5\n",
            "CEI5\n",
            "1608\n",
            "1608\n",
            "1608\n",
            "1608\n",
            "1608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.47 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1286, Val Rows: 322\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-104.5481\t = Validation score   (-mean_absolute_error)\n",
            "\t32.15s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-104.5481\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 32.42s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035452\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035524\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035524\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1560\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (972.4280556, 0.0, 80.23576, 104.68262)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21884.05 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CXL1\n",
            "CXL1\n",
            "CXL1\n",
            "CXL1\n",
            "CXL1\n",
            "1560\n",
            "1560\n",
            "1560\n",
            "1560\n",
            "1560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1248, Val Rows: 312\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-45.7441\t = Validation score   (-mean_absolute_error)\n",
            "\t14.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-45.7441\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.89s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035524\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035539\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035539\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1553\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2156.765833, 0.0, 111.84682, 339.81966)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21842.26 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.38 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PEL6\n",
            "PEL6\n",
            "PEL6\n",
            "PEL6\n",
            "PEL6\n",
            "1553\n",
            "1553\n",
            "1553\n",
            "1553\n",
            "1553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1242, Val Rows: 311\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-123.612\t = Validation score   (-mean_absolute_error)\n",
            "\t24.86s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-123.612\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 25.14s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035539\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035604\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035604\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1552\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2152.193889, 0.0, 81.88805, 215.22815)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21877.57 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.38 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SPG1\n",
            "SPG1\n",
            "SPG1\n",
            "SPG1\n",
            "SPG1\n",
            "1552\n",
            "1552\n",
            "1552\n",
            "1552\n",
            "1552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1241, Val Rows: 311\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-74.7191\t = Validation score   (-mean_absolute_error)\n",
            "\t18.84s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-74.7191\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.12s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035604\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035624\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035624\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1432\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1960.149722, 0.0, 55.98423, 137.18119)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21871.27 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.28 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QGN3\n",
            "QGN3\n",
            "QGN3\n",
            "QGN3\n",
            "QGN3\n",
            "1432\n",
            "1432\n",
            "1432\n",
            "1432\n",
            "1432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.42 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1145, Val Rows: 287\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-52.3726\t = Validation score   (-mean_absolute_error)\n",
            "\t14.57s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-52.3726\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.84s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035624\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035639\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035639\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1410\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1452.077222, 0.0, 62.95691, 144.19564)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21865.06 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.26 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HGH2\n",
            "HGH2\n",
            "HGH2\n",
            "HGH2\n",
            "HGH2\n",
            "1410\n",
            "1410\n",
            "1410\n",
            "1410\n",
            "1410\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.41 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1128, Val Rows: 282\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-57.6197\t = Validation score   (-mean_absolute_error)\n",
            "\t15.34s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-57.6197\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.62s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035639\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035654\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035654\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1350\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1250.076389, 0.0, 90.39754, 160.23247)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21847.62 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.21 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BAZ5\n",
            "BAZ5\n",
            "BAZ5\n",
            "BAZ5\n",
            "BAZ5\n",
            "1350\n",
            "1350\n",
            "1350\n",
            "1350\n",
            "1350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1080, Val Rows: 270\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-72.6868\t = Validation score   (-mean_absolute_error)\n",
            "\t10.59s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-72.6868\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.87s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035654\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035705\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035705\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1250\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2068.820556, 0.0, 24.29908, 96.44792)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21862.92 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.12 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OBZ3\n",
            "OBZ3\n",
            "OBZ3\n",
            "OBZ3\n",
            "OBZ3\n",
            "1250\n",
            "1250\n",
            "1250\n",
            "1250\n",
            "1250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1000, Val Rows: 250\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-24.3678\t = Validation score   (-mean_absolute_error)\n",
            "\t15.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-24.3678\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.78s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035705\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035721\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035721\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1243\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1554.027778, 0.0, 33.10661, 103.38623)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21851.66 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.11 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EIA2\n",
            "EIA2\n",
            "EIA2\n",
            "EIA2\n",
            "EIA2\n",
            "1243\n",
            "1243\n",
            "1243\n",
            "1243\n",
            "1243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 994, Val Rows: 249\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-33.4218\t = Validation score   (-mean_absolute_error)\n",
            "\t12.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-33.4218\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.92s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035721\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035734\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035734\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1213\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1679.275556, 0.0, 89.90488, 163.09488)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21834.01 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PAF4\n",
            "PAF4\n",
            "PAF4\n",
            "PAF4\n",
            "PAF4\n",
            "1213\n",
            "1213\n",
            "1213\n",
            "1213\n",
            "1213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.35 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 970, Val Rows: 243\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-71.1421\t = Validation score   (-mean_absolute_error)\n",
            "\t12.67s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-71.1421\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.93s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035734\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035747\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035747\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1178\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1537.126389, 0.0, 24.53383, 87.89059)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21855.81 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.05 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVL6\n",
            "EVL6\n",
            "EVL6\n",
            "EVL6\n",
            "EVL6\n",
            "1178\n",
            "1178\n",
            "1178\n",
            "1178\n",
            "1178\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.34 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 942, Val Rows: 236\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-32.0007\t = Validation score   (-mean_absolute_error)\n",
            "\t9.21s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-32.0007\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 9.48s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035747\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035757\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035757\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1115\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1664.400556, 0.0, 38.42316, 122.65795)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21871.47 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.99 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OKM4\n",
            "OKM4\n",
            "OKM4\n",
            "OKM4\n",
            "OKM4\n",
            "1115\n",
            "1115\n",
            "1115\n",
            "1115\n",
            "1115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  4 | ['ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  3 | ['SHIP_TYPE_CATEGORY', 'DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 892, Val Rows: 223\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-35.7681\t = Validation score   (-mean_absolute_error)\n",
            "\t15.05s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-35.7681\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.33s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035757\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035812\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035812\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1110\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1824.479444, 0.0, 65.53662, 169.49702)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21868.77 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.99 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IVU2\n",
            "IVU2\n",
            "IVU2\n",
            "IVU2\n",
            "IVU2\n",
            "1110\n",
            "1110\n",
            "1110\n",
            "1110\n",
            "1110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 888, Val Rows: 222\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-72.9076\t = Validation score   (-mean_absolute_error)\n",
            "\t16.03s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-72.9076\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 16.28s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035812\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035829\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035829\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1106\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1660.383333, 0.0, 89.72936, 151.27811)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21853.38 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.98 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XIU1\n",
            "XIU1\n",
            "XIU1\n",
            "XIU1\n",
            "XIU1\n",
            "1106\n",
            "1106\n",
            "1106\n",
            "1106\n",
            "1106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 884, Val Rows: 222\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-72.1894\t = Validation score   (-mean_absolute_error)\n",
            "\t13.23s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-72.1894\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.51s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035829\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035842\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035842\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1100\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1086.762778, 0.0, 79.24395, 133.35781)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21845.51 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.98 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TBL3\n",
            "TBL3\n",
            "TBL3\n",
            "TBL3\n",
            "TBL3\n",
            "1100\n",
            "1100\n",
            "1100\n",
            "1100\n",
            "1100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 880, Val Rows: 220\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-64.2456\t = Validation score   (-mean_absolute_error)\n",
            "\t14.46s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-64.2456\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.72s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035842\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035857\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035857\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1075\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1783.734444, 0.0, 15.48255, 85.40852)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21734.21 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.95 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QEA4\n",
            "QEA4\n",
            "QEA4\n",
            "QEA4\n",
            "QEA4\n",
            "1075\n",
            "1075\n",
            "1075\n",
            "1075\n",
            "1075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.31 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 860, Val Rows: 215\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-14.8479\t = Validation score   (-mean_absolute_error)\n",
            "\t11.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-14.8479\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.69s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035857\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035909\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035909\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1027\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1501.324167, 0.0, 87.40313, 146.14811)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21791.82 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.91 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QGL7\n",
            "QGL7\n",
            "QGL7\n",
            "QGL7\n",
            "QGL7\n",
            "1027\n",
            "1027\n",
            "1027\n",
            "1027\n",
            "1027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.3 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 821, Val Rows: 206\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-66.8908\t = Validation score   (-mean_absolute_error)\n",
            "\t11.1s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-66.8908\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.38s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035909\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035920\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035920\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1012\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1999.665278, 0.0, 24.37512, 115.5882)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21854.77 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.9 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SXD2\n",
            "SXD2\n",
            "SXD2\n",
            "SXD2\n",
            "SXD2\n",
            "1012\n",
            "1012\n",
            "1012\n",
            "1012\n",
            "1012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.29 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 809, Val Rows: 203\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-19.2367\t = Validation score   (-mean_absolute_error)\n",
            "\t11.95s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-19.2367\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.21s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035920\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035933\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035933\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1002\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2140.12, 0.0, 165.75148, 352.85272)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21872.38 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.89 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPM2\n",
            "PPM2\n",
            "PPM2\n",
            "PPM2\n",
            "PPM2\n",
            "1002\n",
            "1002\n",
            "1002\n",
            "1002\n",
            "1002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['ID', 'BREADTH', 'DEPTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['SHIP_TYPE_CATEGORY']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.29 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 801, Val Rows: 201\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-134.6236\t = Validation score   (-mean_absolute_error)\n",
            "\t11.45s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-134.6236\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.75s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035933\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035945\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035945\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    882\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2098.553611, 0.0, 97.44456, 225.74055)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21833.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.79 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BGX4\n",
            "BGX4\n",
            "BGX4\n",
            "BGX4\n",
            "BGX4\n",
            "882\n",
            "882\n",
            "882\n",
            "882\n",
            "882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.26 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 705, Val Rows: 177\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-95.8753\t = Validation score   (-mean_absolute_error)\n",
            "\t13.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-95.8753\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.34s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035945\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035958\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035958\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    857\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1979.001389, 0.0, 81.53111, 234.05573)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21865.43 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.76 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REJ1\n",
            "REJ1\n",
            "REJ1\n",
            "REJ1\n",
            "REJ1\n",
            "857\n",
            "857\n",
            "857\n",
            "857\n",
            "857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 685, Val Rows: 172\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-103.0737\t = Validation score   (-mean_absolute_error)\n",
            "\t10.73s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-103.0737\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.99s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035958\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040009\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040009\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    822\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2116.382778, 0.0, 118.91655, 258.5082)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21872.94 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.73 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVX2\n",
            "EVX2\n",
            "EVX2\n",
            "EVX2\n",
            "EVX2\n",
            "822\n",
            "822\n",
            "822\n",
            "822\n",
            "822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.24 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 657, Val Rows: 165\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-146.5937\t = Validation score   (-mean_absolute_error)\n",
            "\t10.59s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-146.5937\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.84s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040009\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040020\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040020\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    792\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1710.938056, 0.0, 61.71432, 181.83172)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21867.36 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.7 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QRN3\n",
            "QRN3\n",
            "QRN3\n",
            "QRN3\n",
            "QRN3\n",
            "792\n",
            "792\n",
            "792\n",
            "792\n",
            "792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.23 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 633, Val Rows: 159\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-60.4714\t = Validation score   (-mean_absolute_error)\n",
            "\t11.86s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-60.4714\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040020\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040032\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040032\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    781\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1479.897778, 0.0, 76.27168, 136.60538)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21848.26 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.7 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OZD2\n",
            "OZD2\n",
            "OZD2\n",
            "OZD2\n",
            "OZD2\n",
            "781\n",
            "781\n",
            "781\n",
            "781\n",
            "781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.23 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 624, Val Rows: 157\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-57.2167\t = Validation score   (-mean_absolute_error)\n",
            "\t14.82s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-57.2167\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.09s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040032\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040047\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040047\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    720\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1743.36, 0.0, 85.17842, 138.91108)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21676.2 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.64 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRQ5\n",
            "GRQ5\n",
            "GRQ5\n",
            "GRQ5\n",
            "GRQ5\n",
            "720\n",
            "720\n",
            "720\n",
            "720\n",
            "720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.21 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 576, Val Rows: 144\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-59.1572\t = Validation score   (-mean_absolute_error)\n",
            "\t12.12s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-59.1572\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.37s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040047\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040100\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040100\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    677\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2120.121111, 0.0, 72.14794, 240.13837)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21731.01 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.6 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JGL5\n",
            "JGL5\n",
            "JGL5\n",
            "JGL5\n",
            "JGL5\n",
            "677\n",
            "677\n",
            "677\n",
            "677\n",
            "677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.2 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 541, Val Rows: 136\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-102.5958\t = Validation score   (-mean_absolute_error)\n",
            "\t12.41s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-102.5958\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.66s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040100\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040113\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040113\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    578\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1604.490278, 0.0, 28.71235, 114.58595)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21746.68 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.51 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 10): ['ARI_CO', 'ARI_PO', 'DEPTH', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  3 | ['ID', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  3 | ['SHIP_TYPE_CATEGORY', 'BREADTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t43 features in original data used to generate 43 features in processed data.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSP6\n",
            "CSP6\n",
            "CSP6\n",
            "CSP6\n",
            "CSP6\n",
            "578\n",
            "578\n",
            "578\n",
            "578\n",
            "578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tTrain Data (Processed) Memory Usage: 0.17 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.16s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 462, Val Rows: 116\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-36.5879\t = Validation score   (-mean_absolute_error)\n",
            "\t10.75s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-36.5879\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.97s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040113\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040124\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040124\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    577\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1966.637778, 0.0, 100.56406, 259.61028)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21767.69 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.51 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UPI6\n",
            "UPI6\n",
            "UPI6\n",
            "UPI6\n",
            "UPI6\n",
            "577\n",
            "577\n",
            "577\n",
            "577\n",
            "577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DEPTH']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.17 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 461, Val Rows: 116\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-96.9723\t = Validation score   (-mean_absolute_error)\n",
            "\t15.85s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-96.9723\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 16.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040124\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040140\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040140\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    568\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2136.011111, 0.0, 100.67017, 274.03948)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21789.48 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.5 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.17 MB (0.0% of available memory)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MOC5\n",
            "MOC5\n",
            "MOC5\n",
            "MOC5\n",
            "MOC5\n",
            "568\n",
            "568\n",
            "568\n",
            "568\n",
            "568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Data preprocessing and feature engineering runtime = 0.16s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 454, Val Rows: 114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-81.0674\t = Validation score   (-mean_absolute_error)\n",
            "\t10.44s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-81.0674\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.68s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040140\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040151\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040151\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    533\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (473.1002778, 0.0, 39.18026, 65.69818)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21812.05 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.47 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 10): ['ARI_CO', 'ARI_PO', 'DRAUGHT', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  3 | ['ID', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HYV6\n",
            "HYV6\n",
            "HYV6\n",
            "HYV6\n",
            "HYV6\n",
            "533\n",
            "533\n",
            "533\n",
            "533\n",
            "533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t('int', ['bool']) :  3 | ['SHIP_TYPE_CATEGORY', 'BREADTH', 'DEPTH']\n",
            "\t0.1s = Fit runtime\n",
            "\t43 features in original data used to generate 43 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.15 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 426, Val Rows: 107\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-28.7005\t = Validation score   (-mean_absolute_error)\n",
            "\t13.36s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-28.7005\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.6s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040151\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040205\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040205\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    512\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (605.1586111, 0.0, 53.38431, 83.78321)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21808.59 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.46 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PBQ1\n",
            "PBQ1\n",
            "PBQ1\n",
            "PBQ1\n",
            "PBQ1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.15 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 409, Val Rows: 103\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-50.1445\t = Validation score   (-mean_absolute_error)\n",
            "\t10.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-50.1445\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.15s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040205\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040216\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040216\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    480\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (129.9619444, 0.0, 2.56958, 11.44516)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21822.81 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.43 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PUF3\n",
            "PUF3\n",
            "PUF3\n",
            "PUF3\n",
            "PUF3\n",
            "480\n",
            "480\n",
            "480\n",
            "480\n",
            "480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DEPTH']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 384, Val Rows: 96\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-1.056\t = Validation score   (-mean_absolute_error)\n",
            "\t90.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-1.056\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 90.98s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040216\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040347\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040347\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    433\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1322.722222, 0.0, 90.32039, 144.56287)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21740.91 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LHD1\n",
            "LHD1\n",
            "LHD1\n",
            "LHD1\n",
            "LHD1\n",
            "433\n",
            "433\n",
            "433\n",
            "433\n",
            "433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 10): ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  6 | ['ID', 'BREADTH', 'DEPTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  4 | ['ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t43 features in original data used to generate 43 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.13 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 346, Val Rows: 87\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-62.3691\t = Validation score   (-mean_absolute_error)\n",
            "\t12.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-62.3691\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.89s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040347\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040400\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040400\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    416\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (600.8713889, 0.0, 8.11309, 38.64977)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21787.76 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.37 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOV8\n",
            "OOV8\n",
            "OOV8\n",
            "OOV8\n",
            "OOV8\n",
            "416\n",
            "416\n",
            "416\n",
            "416\n",
            "416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  4 | ['SHIP_TYPE_CATEGORY', 'ID', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  3 | ['BREADTH', 'DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.12 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 332, Val Rows: 84\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-6.5497\t = Validation score   (-mean_absolute_error)\n",
            "\t12.12s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-6.5497\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.37s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040400\\\")\n"
          ]
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "# Load train, test, and sample_submission data\n",
        "train_data = train.copy()\n",
        "test_data = test.copy()\n",
        "sample_submission = submission.copy()\n",
        "\n",
        "# Create an empty DataFrame for the final predictions\n",
        "final_predictions = pd.DataFrame(index=test_data.index)\n",
        "final_predictions['CI_HOUR'] = 0  # Initialize with 0\n",
        "\n",
        "# Filter the ARI_PO values in train_data that have at least 400 occurrences\n",
        "ari_po_values_to_model = train_data['ARI_PO'].value_counts()\n",
        "ari_po_values_to_model = ari_po_values_to_model[ari_po_values_to_model >= 400].index.tolist()\n",
        "\n",
        "# Loop through each ARI_PO value and train a model for it\n",
        "for ari_po in ari_po_values_to_model:\n",
        "    train_subset = train_data[train_data['ARI_PO'] == ari_po]\n",
        "    test_subset = test_data[test_data['ARI_PO'] == ari_po]\n",
        "    print(ari_po)\n",
        "    print(ari_po)\n",
        "    print(ari_po)\n",
        "    print(ari_po)\n",
        "    print(ari_po)\n",
        "\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    \n",
        "    # Convert to AutoGluon Dataset\n",
        "    train_subset_ag = TabularDataset(train_subset)\n",
        "    test_subset_ag = TabularDataset(test_subset)\n",
        "\n",
        "    # Train the model\n",
        "    predictor = TabularPredictor(label='CI_HOUR', eval_metric='mean_absolute_error').fit(\n",
        "        train_subset_ag, presets='medium_quality', ag_args_fit={'num_gpus': 0}, included_model_types=['CAT']\n",
        "    )\n",
        "\n",
        "    # Predict on the test subset\n",
        "    y_pred = predictor.predict(test_subset_ag)\n",
        "    \n",
        "    # Assign the predictions to the final_predictions DataFrame\n",
        "    final_predictions.loc[test_subset.index, 'CI_HOUR'] = y_pred.values\n",
        "\n",
        "# Merge the final predictions with sample_submission on the index\n",
        "sample_submission['CI_HOUR'] = final_predictions['CI_HOUR']\n",
        "sample_submission.to_csv(\"testing.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_073236\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (367441 samples, 20.58 MB).\n",
            "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_073236\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   14.72 GB / 511.09 GB (2.9%)\n",
            "Train Data Rows:    367441\n",
            "Train Data Columns: 6\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.0, 61.87712, 170.57522)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    19320.5 MB\n",
            "\tTrain Data (Original)  Memory Usage: 17.64 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 6 | ['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 6 | ['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin', ...]\n",
            "\t0.3s = Fit runtime\n",
            "\t6 features in original data used to generate 6 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 17.64 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.4s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 363766, Val Rows: 3675\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['RF', 'LGBM'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "\tThe models types ['LGBM'] are not present in the model list specified by the user and will be ignored:\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-17.8258\t = Validation score   (-mean_absolute_error)\n",
            "\t46.57s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-17.8258\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 47.92s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_073236\\\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_test</th>\n",
              "      <th>score_val</th>\n",
              "      <th>pred_time_test</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_test_marginal</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForestMSE</td>\n",
              "      <td>-10.569477</td>\n",
              "      <td>-17.825751</td>\n",
              "      <td>3.979716</td>\n",
              "      <td>0.060516</td>\n",
              "      <td>46.566810</td>\n",
              "      <td>3.979716</td>\n",
              "      <td>0.060516</td>\n",
              "      <td>46.566810</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>-10.569477</td>\n",
              "      <td>-17.825751</td>\n",
              "      <td>4.009856</td>\n",
              "      <td>0.061610</td>\n",
              "      <td>46.573476</td>\n",
              "      <td>0.030141</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.006665</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 model  score_test  score_val  pred_time_test  pred_time_val  \\\n",
              "0      RandomForestMSE  -10.569477 -17.825751        3.979716       0.060516   \n",
              "1  WeightedEnsemble_L2  -10.569477 -17.825751        4.009856       0.061610   \n",
              "\n",
              "    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
              "0  46.566810                 3.979716                0.060516   \n",
              "1  46.573476                 0.030141                0.001094   \n",
              "\n",
              "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
              "0          46.566810            1       True          1  \n",
              "1           0.006665            2       True          2  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_data = TabularDataset(train)\n",
        "test_data = TabularDataset(test)\n",
        "\n",
        "\n",
        "#predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, included_model_types = ['CAT'])\n",
        "#predictor = TabularPredictor(label='CI_HsOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0})\n",
        "predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, included_model_types = ['RF', 'LGBM'])\n",
        "\n",
        "predictor.leaderboard(train, silent=True)\n",
        "\n",
        "\n",
        "y_pred = predictor.predict(test_data)\n",
        "y_pred = pd.DataFrame(y_pred, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred['CI_HOUR']\n",
        "predictor.leaderboard(train, silent=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# feature_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_073722\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (367441 samples, 17.64 MB).\n",
            "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_073722\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   13.94 GB / 511.09 GB (2.7%)\n",
            "Train Data Rows:    367441\n",
            "Train Data Columns: 5\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.0, 61.87712, 170.57522)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    18761.72 MB\n",
            "\tTrain Data (Original)  Memory Usage: 14.7 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 5 | ['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 5 | ['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin']\n",
            "\t0.4s = Fit runtime\n",
            "\t5 features in original data used to generate 5 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 14.7 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.5s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 363766, Val Rows: 3675\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['RF', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 5 L1 models ...\n",
            "Fitting model: LightGBMXT ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1000]\tvalid_set's l1: 77.3625\n",
            "[2000]\tvalid_set's l1: 73.131\n",
            "[3000]\tvalid_set's l1: 70.167\n",
            "[4000]\tvalid_set's l1: 67.8511\n",
            "[5000]\tvalid_set's l1: 65.8352\n",
            "[6000]\tvalid_set's l1: 64.1543\n",
            "[7000]\tvalid_set's l1: 62.7938\n",
            "[8000]\tvalid_set's l1: 61.5584\n",
            "[9000]\tvalid_set's l1: 60.4681\n",
            "[10000]\tvalid_set's l1: 59.4697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t-59.4697\t = Validation score   (-mean_absolute_error)\n",
            "\t53.31s\t = Training   runtime\n",
            "\t0.97s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1000]\tvalid_set's l1: 57.0162\n",
            "[2000]\tvalid_set's l1: 47.248\n",
            "[3000]\tvalid_set's l1: 41.5582\n",
            "[4000]\tvalid_set's l1: 37.6642\n",
            "[5000]\tvalid_set's l1: 35.1605\n",
            "[6000]\tvalid_set's l1: 33.3518\n",
            "[7000]\tvalid_set's l1: 31.6402\n",
            "[8000]\tvalid_set's l1: 30.4382\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m subset_data \u001b[39m=\u001b[39m train_data[[target] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(feature_combination)]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m predictor \u001b[39m=\u001b[39m TabularPredictor(label\u001b[39m=\u001b[39;49mtarget, eval_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean_absolute_error\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(subset_data, presets\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmedium_quality\u001b[39;49m\u001b[39m'\u001b[39;49m, ag_args_fit\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mnum_gpus\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0\u001b[39;49m}, included_model_types\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mRF\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mGBM\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mXGB\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m lb \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mleaderboard(subset_data, silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m score \u001b[39m=\u001b[39m lb[\u001b[39m'\u001b[39m\u001b[39mscore_val\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmax()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mgargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:986\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    987\u001b[0m     X\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m    988\u001b[0m     X_val\u001b[39m=\u001b[39;49mtuning_data,\n\u001b[0;32m    989\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[0;32m    990\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[0;32m    991\u001b[0m     num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds,\n\u001b[0;32m    992\u001b[0m     num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[0;32m    993\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    994\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    995\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    996\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    997\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    998\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    999\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m   1000\u001b[0m     verbosity\u001b[39m=\u001b[39;49mverbosity,\n\u001b[0;32m   1001\u001b[0m     use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout,\n\u001b[0;32m   1002\u001b[0m )\n\u001b[0;32m   1003\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[0;32m   1006\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1007\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[0;32m   1013\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[1;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, X_val\u001b[39m=\u001b[39;49mX_val, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:157\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m--> 157\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    158\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    159\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    160\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    161\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    162\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    163\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[0;32m    164\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit_trainer,\n\u001b[0;32m    165\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    166\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    167\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    168\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrainer_fit_kwargs,\n\u001b[0;32m    169\u001b[0m )\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[0;32m    171\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:114\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m log_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m20\u001b[39m, log_str)\n\u001b[1;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(\n\u001b[0;32m    115\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    116\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    117\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    118\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    119\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    120\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    121\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    122\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    123\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    124\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    125\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    126\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    127\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    128\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2371\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[0;32m   2369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_rows_val \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_val)\n\u001b[0;32m   2370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_cols_train \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m-> 2371\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_multi_levels(\n\u001b[0;32m   2372\u001b[0m     X,\n\u001b[0;32m   2373\u001b[0m     y,\n\u001b[0;32m   2374\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m   2375\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m   2376\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m   2377\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m   2378\u001b[0m     level_start\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2379\u001b[0m     level_end\u001b[39m=\u001b[39;49mnum_stack_levels \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2380\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2381\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2382\u001b[0m )\n\u001b[0;32m   2383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:395\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    393\u001b[0m         core_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_core)\n\u001b[0;32m    394\u001b[0m         aux_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_aux)\n\u001b[1;32m--> 395\u001b[0m     base_model_names, aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level(\n\u001b[0;32m    396\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    397\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    398\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    399\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    400\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    401\u001b[0m         models\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    402\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    403\u001b[0m         base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[0;32m    404\u001b[0m         core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs_level,\n\u001b[0;32m    405\u001b[0m         aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs_level,\n\u001b[0;32m    406\u001b[0m         name_suffix\u001b[39m=\u001b[39;49mname_suffix,\n\u001b[0;32m    407\u001b[0m         infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    408\u001b[0m         infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m     model_names_fit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m base_model_names \u001b[39m+\u001b[39m aux_models\n\u001b[0;32m    411\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_best \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(model_names_fit) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:539\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    537\u001b[0m     core_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[0;32m    538\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[1;32m--> 539\u001b[0m core_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level_core(\n\u001b[0;32m    540\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    541\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    542\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    543\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    544\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    545\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m    546\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    547\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    548\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    549\u001b[0m     base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[0;32m    550\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcore_kwargs,\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m \u001b[39mif\u001b[39;00m X_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_new_level_aux(\n\u001b[0;32m    555\u001b[0m         X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, base_model_names\u001b[39m=\u001b[39mcore_models, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, infer_limit\u001b[39m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maux_kwargs\n\u001b[0;32m    556\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:673\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m fit_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m    672\u001b[0m \u001b[39m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi(\n\u001b[0;32m    674\u001b[0m     X\u001b[39m=\u001b[39;49mX_init,\n\u001b[0;32m    675\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    676\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    677\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    678\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    679\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m    680\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    681\u001b[0m     stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[0;32m    682\u001b[0m     compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[0;32m    683\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m    684\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    685\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2321\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[39mif\u001b[39;00m n_repeat_start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2320\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m-> 2321\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_initial(\n\u001b[0;32m   2322\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   2323\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   2324\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m   2325\u001b[0m         k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[0;32m   2326\u001b[0m         n_repeats\u001b[39m=\u001b[39;49mn_repeats_initial,\n\u001b[0;32m   2327\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[0;32m   2328\u001b[0m         feature_prune_kwargs\u001b[39m=\u001b[39;49mfeature_prune_kwargs,\n\u001b[0;32m   2329\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2330\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2331\u001b[0m     )\n\u001b[0;32m   2332\u001b[0m     n_repeat_start \u001b[39m=\u001b[39m n_repeats_initial\n\u001b[0;32m   2333\u001b[0m     \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2160\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m bagged:\n\u001b[0;32m   2159\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 2160\u001b[0m     models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_fold(\n\u001b[0;32m   2161\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m   2162\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[0;32m   2163\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2164\u001b[0m         time_split\u001b[39m=\u001b[39;49mtime_split,\n\u001b[0;32m   2165\u001b[0m         time_ratio\u001b[39m=\u001b[39;49mtime_ratio,\n\u001b[0;32m   2166\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_args,\n\u001b[0;32m   2167\u001b[0m     )\n\u001b[0;32m   2168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2169\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2278\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2276\u001b[0m         time_start_model \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2277\u001b[0m         time_left \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time_start_model \u001b[39m-\u001b[39m time_start)\n\u001b[1;32m-> 2278\u001b[0m model_name_trained_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single_full(\n\u001b[0;32m   2279\u001b[0m     X, y, model, time_limit\u001b[39m=\u001b[39;49mtime_left, hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m   2280\u001b[0m )\n\u001b[0;32m   2282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m   2283\u001b[0m     \u001b[39mdel\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[0;32m   2047\u001b[0m         bagged_model_fit_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[0;32m   2048\u001b[0m             k_fold\u001b[39m=\u001b[39mk_fold, k_fold_start\u001b[39m=\u001b[39mk_fold_start, k_fold_end\u001b[39m=\u001b[39mk_fold_end, n_repeats\u001b[39m=\u001b[39mn_repeats, n_repeat_start\u001b[39m=\u001b[39mn_repeat_start\n\u001b[0;32m   2049\u001b[0m         )\n\u001b[0;32m   2050\u001b[0m         model_fit_kwargs\u001b[39m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[1;32m-> 2051\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_and_save(\n\u001b[0;32m   2052\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   2053\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   2054\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   2055\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m   2056\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m   2057\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m   2058\u001b[0m         stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[0;32m   2059\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   2060\u001b[0m         compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[0;32m   2061\u001b[0m         total_resources\u001b[39m=\u001b[39;49mtotal_resources,\n\u001b[0;32m   2062\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs,\n\u001b[0;32m   2063\u001b[0m     )\n\u001b[0;32m   2064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m   2065\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_trained\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1733\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[1;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1731\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1732\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1733\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single(X, y, model, X_val, y_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[0;32m   1735\u001b[0m fit_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   1736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_evaluation:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1684\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[1;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train_single\u001b[39m(\u001b[39mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, total_resources\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AbstractModel:\n\u001b[0;32m   1680\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m \u001b[39m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[0;32m   1682\u001b[0m \u001b[39m    Returns trained model object.\u001b[39;00m\n\u001b[0;32m   1683\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1684\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[0;32m   1685\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:829\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_fit_resources(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    828\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_memory_usage(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 829\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    831\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\lgb\\lgb_model.py:194\u001b[0m, in \u001b[0;36mLGBModel._fit\u001b[1;34m(self, X, y, X_val, y_val, time_limit, num_gpus, num_cpus, sample_weight, sample_weight_val, verbosity, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_column in param dict is overridden.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m train_lgb_model(early_stopping_callback_kwargs\u001b[39m=\u001b[39;49mearly_stopping_callback_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_params)\n\u001b[0;32m    195\u001b[0m \u001b[39mexcept\u001b[39;00m LightGBMError:\n\u001b[0;32m    196\u001b[0m     \u001b[39mif\u001b[39;00m train_params[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\lgb\\lgb_utils.py:124\u001b[0m, in \u001b[0;36mtrain_lgb_model\u001b[1;34m(early_stopping_callback_kwargs, **train_params)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m booster\u001b[39m.\u001b[39mfit(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_params)\n\u001b[0;32m    123\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m lgb\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_params)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[0;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "# 데이터 로드\n",
        "train_data = TabularDataset(train)\n",
        "\n",
        "# 타겟 변수와 모든 피처를 리스트로 변환\n",
        "target = 'CI_HOUR'\n",
        "all_features = [col for col in train_data.columns if col != target]\n",
        "\n",
        "# 모든 피처들에서 하나씩 제외한 조합 생성\n",
        "all_combinations = list(combinations(all_features, len(all_features) - 1))\n",
        "\n",
        "best_score = float('inf')  # MAE는 낮을수록 좋으므로 초기값을 무한대로 설정\n",
        "best_combination = None\n",
        "iteration = 0\n",
        "output_path = 'results.txt'\n",
        "\n",
        "for feature_combination in all_combinations:\n",
        "    iteration += 1\n",
        "    subset_data = train_data[[target] + list(feature_combination)]\n",
        "    \n",
        "    predictor = TabularPredictor(label=target, eval_metric='mean_absolute_error').fit(subset_data, presets='medium_quality', ag_args_fit={'num_gpus': 0}, included_model_types=['RF', 'GBM', 'XGB'])\n",
        "    lb = predictor.leaderboard(subset_data, silent=True)\n",
        "    score = lb['score_val'].max()\n",
        "    \n",
        "    # 결과를 txt 파일에 계속 추가\n",
        "    with open(output_path, 'a') as f:\n",
        "        f.write(\"Iteration: \" + str(iteration) + \"\\n\")\n",
        "        f.write(\"Excluded feature: \" + ', '.join(set(all_features) - set(feature_combination)) + \"\\n\")\n",
        "        f.write(\"MAE: \" + str(score) + \"\\n\")\n",
        "        f.write(\"-\" * 50 + \"\\n\")  # 구분선\n",
        "    \n",
        "    if score < best_score:\n",
        "        best_score = score\n",
        "        best_combination = feature_combination\n",
        "\n",
        "print(\"Best feature combination:\", best_combination)\n",
        "print(\"Best MAE:\", best_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pycaret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_bdb6d_row10_col1, #T_bdb6d_row19_col1 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_bdb6d\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_bdb6d_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
              "      <th id=\"T_bdb6d_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_bdb6d_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
              "      <td id=\"T_bdb6d_row0_col1\" class=\"data row0 col1\" >724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_bdb6d_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
              "      <td id=\"T_bdb6d_row1_col1\" class=\"data row1 col1\" >CI_HOUR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_bdb6d_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
              "      <td id=\"T_bdb6d_row2_col1\" class=\"data row2 col1\" >Regression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_bdb6d_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
              "      <td id=\"T_bdb6d_row3_col1\" class=\"data row3 col1\" >(391939, 34)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_bdb6d_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
              "      <td id=\"T_bdb6d_row4_col1\" class=\"data row4 col1\" >(391939, 61)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_bdb6d_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
              "      <td id=\"T_bdb6d_row5_col1\" class=\"data row5 col1\" >(274357, 61)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_bdb6d_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
              "      <td id=\"T_bdb6d_row6_col1\" class=\"data row6 col1\" >(117582, 61)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_bdb6d_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
              "      <td id=\"T_bdb6d_row7_col1\" class=\"data row7 col1\" >27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_bdb6d_row8_col0\" class=\"data row8 col0\" >Categorical features</td>\n",
              "      <td id=\"T_bdb6d_row8_col1\" class=\"data row8 col1\" >6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_bdb6d_row9_col0\" class=\"data row9 col0\" >Rows with missing values</td>\n",
              "      <td id=\"T_bdb6d_row9_col1\" class=\"data row9 col1\" >42.0%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_bdb6d_row10_col0\" class=\"data row10 col0\" >Preprocess</td>\n",
              "      <td id=\"T_bdb6d_row10_col1\" class=\"data row10 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_bdb6d_row11_col0\" class=\"data row11 col0\" >Imputation type</td>\n",
              "      <td id=\"T_bdb6d_row11_col1\" class=\"data row11 col1\" >simple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_bdb6d_row12_col0\" class=\"data row12 col0\" >Numeric imputation</td>\n",
              "      <td id=\"T_bdb6d_row12_col1\" class=\"data row12 col1\" >mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_bdb6d_row13_col0\" class=\"data row13 col0\" >Categorical imputation</td>\n",
              "      <td id=\"T_bdb6d_row13_col1\" class=\"data row13 col1\" >mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_bdb6d_row14_col0\" class=\"data row14 col0\" >Maximum one-hot encoding</td>\n",
              "      <td id=\"T_bdb6d_row14_col1\" class=\"data row14 col1\" >25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_bdb6d_row15_col0\" class=\"data row15 col0\" >Encoding method</td>\n",
              "      <td id=\"T_bdb6d_row15_col1\" class=\"data row15 col1\" >None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_bdb6d_row16_col0\" class=\"data row16 col0\" >Fold Generator</td>\n",
              "      <td id=\"T_bdb6d_row16_col1\" class=\"data row16 col1\" >KFold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_bdb6d_row17_col0\" class=\"data row17 col0\" >Fold Number</td>\n",
              "      <td id=\"T_bdb6d_row17_col1\" class=\"data row17 col1\" >10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_bdb6d_row18_col0\" class=\"data row18 col0\" >CPU Jobs</td>\n",
              "      <td id=\"T_bdb6d_row18_col1\" class=\"data row18 col1\" >-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_bdb6d_row19_col0\" class=\"data row19 col0\" >Use GPU</td>\n",
              "      <td id=\"T_bdb6d_row19_col1\" class=\"data row19 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_bdb6d_row20_col0\" class=\"data row20 col0\" >Log Experiment</td>\n",
              "      <td id=\"T_bdb6d_row20_col1\" class=\"data row20 col1\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_bdb6d_row21_col0\" class=\"data row21 col0\" >Experiment Name</td>\n",
              "      <td id=\"T_bdb6d_row21_col1\" class=\"data row21 col1\" >reg-default-name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "      <td id=\"T_bdb6d_row22_col0\" class=\"data row22 col0\" >USI</td>\n",
              "      <td id=\"T_bdb6d_row22_col1\" class=\"data row22 col1\" >532b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x28d87dd1340>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;BREADTH&#x27;, &#x27;BUILT&#x27;,\n",
              "                                             &#x27;DEADWEIGHT&#x27;, &#x27;DEPTH&#x27;, &#x27;DRAUGHT&#x27;,\n",
              "                                             &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;U_WIND&#x27;, &#x27;V_WIND&#x27;,\n",
              "                                             &#x27;AIR_TEMPERATURE&#x27;, &#x27;BN&#x27;, &#x27;ATA_LT&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;da...\n",
              "                 TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                    transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;,\n",
              "                                             &#x27;FLAG&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;SHIPMANAGER&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;)))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;BREADTH&#x27;, &#x27;BUILT&#x27;,\n",
              "                                             &#x27;DEADWEIGHT&#x27;, &#x27;DEPTH&#x27;, &#x27;DRAUGHT&#x27;,\n",
              "                                             &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;U_WIND&#x27;, &#x27;V_WIND&#x27;,\n",
              "                                             &#x27;AIR_TEMPERATURE&#x27;, &#x27;BN&#x27;, &#x27;ATA_LT&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;da...\n",
              "                 TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                    transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;,\n",
              "                                             &#x27;FLAG&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;SHIPMANAGER&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;BREADTH&#x27;, &#x27;BUILT&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;DEPTH&#x27;,\n",
              "                            &#x27;DRAUGHT&#x27;, &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;U_WIND&#x27;, &#x27;V_WIND&#x27;,\n",
              "                            &#x27;AIR_TEMPERATURE&#x27;, &#x27;BN&#x27;, &#x27;ATA_LT&#x27;, &#x27;PORT_SIZE&#x27;,\n",
              "                            &#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                            &#x27;month_sin&#x27;, &#x27;month_cos&#x27;, &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                            &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;, &#x27;rounded_hour_sin&#x27;,\n",
              "                            &#x27;rounded_hour_cos&#x27;],\n",
              "                   transformer=SimpleImputer())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;ARI_PO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;ID&#x27;,\n",
              "                            &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "                   transformer=SimpleImputer(strategy=&#x27;most_frequent&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehot_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                   transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                   &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                             handle_missing=&#x27;return_nan&#x27;,\n",
              "                                             use_cat_names=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">rest_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "                   transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;,\n",
              "                                                   &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "                                             handle_missing=&#x27;return_nan&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[('numerical_imputer',\n",
              "                 TransformerWrapper(include=['DIST', 'BREADTH', 'BUILT',\n",
              "                                             'DEADWEIGHT', 'DEPTH', 'DRAUGHT',\n",
              "                                             'GT', 'LENGTH', 'U_WIND', 'V_WIND',\n",
              "                                             'AIR_TEMPERATURE', 'BN', 'ATA_LT',\n",
              "                                             'PORT_SIZE', 'year', 'month',\n",
              "                                             'day', 'weekday', 'rounded_hour',\n",
              "                                             'month_sin', 'month_cos',\n",
              "                                             'day_sin', 'da...\n",
              "                 TransformerWrapper(include=['ARI_CO', 'SHIP_TYPE_CATEGORY'],\n",
              "                                    transformer=OneHotEncoder(cols=['ARI_CO',\n",
              "                                                                    'SHIP_TYPE_CATEGORY'],\n",
              "                                                              handle_missing='return_nan',\n",
              "                                                              use_cat_names=True))),\n",
              "                ('rest_encoding',\n",
              "                 TransformerWrapper(include=['ARI_PO', 'ID', 'SHIPMANAGER',\n",
              "                                             'FLAG'],\n",
              "                                    transformer=TargetEncoder(cols=['ARI_PO',\n",
              "                                                                    'ID',\n",
              "                                                                    'SHIPMANAGER',\n",
              "                                                                    'FLAG'],\n",
              "                                                              handle_missing='return_nan')))])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pycaret.regression import *\n",
        "import category_encoders\n",
        "\n",
        "reg = setup(data=train, target='CI_HOUR', use_gpu=True, session_id=724)\n",
        "reg.pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_06b9d th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_06b9d_row0_col0, #T_06b9d_row0_col2, #T_06b9d_row0_col3, #T_06b9d_row0_col4, #T_06b9d_row0_col5, #T_06b9d_row1_col0, #T_06b9d_row1_col1, #T_06b9d_row1_col2, #T_06b9d_row1_col3, #T_06b9d_row1_col4, #T_06b9d_row1_col5, #T_06b9d_row1_col6, #T_06b9d_row2_col0, #T_06b9d_row2_col1, #T_06b9d_row2_col2, #T_06b9d_row2_col3, #T_06b9d_row2_col4, #T_06b9d_row2_col5, #T_06b9d_row2_col6, #T_06b9d_row3_col0, #T_06b9d_row3_col1, #T_06b9d_row3_col2, #T_06b9d_row3_col3, #T_06b9d_row3_col4, #T_06b9d_row3_col6, #T_06b9d_row4_col0, #T_06b9d_row4_col1, #T_06b9d_row4_col5, #T_06b9d_row4_col6, #T_06b9d_row5_col0, #T_06b9d_row5_col1, #T_06b9d_row5_col2, #T_06b9d_row5_col3, #T_06b9d_row5_col4, #T_06b9d_row5_col5, #T_06b9d_row5_col6, #T_06b9d_row6_col0, #T_06b9d_row6_col1, #T_06b9d_row6_col2, #T_06b9d_row6_col3, #T_06b9d_row6_col4, #T_06b9d_row6_col5, #T_06b9d_row6_col6, #T_06b9d_row7_col0, #T_06b9d_row7_col1, #T_06b9d_row7_col2, #T_06b9d_row7_col3, #T_06b9d_row7_col4, #T_06b9d_row7_col5, #T_06b9d_row7_col6, #T_06b9d_row8_col0, #T_06b9d_row8_col1, #T_06b9d_row8_col2, #T_06b9d_row8_col3, #T_06b9d_row8_col4, #T_06b9d_row8_col5, #T_06b9d_row8_col6, #T_06b9d_row9_col0, #T_06b9d_row9_col1, #T_06b9d_row9_col2, #T_06b9d_row9_col3, #T_06b9d_row9_col4, #T_06b9d_row9_col5, #T_06b9d_row9_col6, #T_06b9d_row10_col0, #T_06b9d_row10_col1, #T_06b9d_row10_col2, #T_06b9d_row10_col3, #T_06b9d_row10_col4, #T_06b9d_row10_col5, #T_06b9d_row10_col6, #T_06b9d_row11_col0, #T_06b9d_row11_col1, #T_06b9d_row11_col2, #T_06b9d_row11_col3, #T_06b9d_row11_col4, #T_06b9d_row11_col5, #T_06b9d_row11_col6, #T_06b9d_row12_col0, #T_06b9d_row12_col1, #T_06b9d_row12_col2, #T_06b9d_row12_col3, #T_06b9d_row12_col4, #T_06b9d_row12_col5, #T_06b9d_row12_col6, #T_06b9d_row13_col0, #T_06b9d_row13_col1, #T_06b9d_row13_col2, #T_06b9d_row13_col3, #T_06b9d_row13_col4, #T_06b9d_row13_col5, #T_06b9d_row13_col6, #T_06b9d_row14_col0, #T_06b9d_row14_col1, #T_06b9d_row14_col2, #T_06b9d_row14_col3, #T_06b9d_row14_col4, #T_06b9d_row14_col5, #T_06b9d_row14_col6, #T_06b9d_row15_col0, #T_06b9d_row15_col1, #T_06b9d_row15_col2, #T_06b9d_row15_col3, #T_06b9d_row15_col4, #T_06b9d_row15_col5, #T_06b9d_row15_col6, #T_06b9d_row16_col0, #T_06b9d_row16_col1, #T_06b9d_row16_col2, #T_06b9d_row16_col3, #T_06b9d_row16_col4, #T_06b9d_row16_col5, #T_06b9d_row16_col6, #T_06b9d_row17_col0, #T_06b9d_row17_col1, #T_06b9d_row17_col2, #T_06b9d_row17_col3, #T_06b9d_row17_col4, #T_06b9d_row17_col5, #T_06b9d_row17_col6, #T_06b9d_row18_col0, #T_06b9d_row18_col1, #T_06b9d_row18_col2, #T_06b9d_row18_col3, #T_06b9d_row18_col4, #T_06b9d_row18_col5, #T_06b9d_row18_col6, #T_06b9d_row19_col0, #T_06b9d_row19_col1, #T_06b9d_row19_col2, #T_06b9d_row19_col3, #T_06b9d_row19_col4, #T_06b9d_row19_col5, #T_06b9d_row19_col6 {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_06b9d_row0_col1, #T_06b9d_row0_col6, #T_06b9d_row3_col5, #T_06b9d_row4_col2, #T_06b9d_row4_col3, #T_06b9d_row4_col4 {\n",
              "  text-align: left;\n",
              "  background-color: yellow;\n",
              "}\n",
              "#T_06b9d_row0_col7, #T_06b9d_row1_col7, #T_06b9d_row2_col7, #T_06b9d_row3_col7, #T_06b9d_row4_col7, #T_06b9d_row5_col7, #T_06b9d_row6_col7, #T_06b9d_row7_col7, #T_06b9d_row8_col7, #T_06b9d_row9_col7, #T_06b9d_row10_col7, #T_06b9d_row11_col7, #T_06b9d_row12_col7, #T_06b9d_row13_col7, #T_06b9d_row14_col7, #T_06b9d_row15_col7, #T_06b9d_row17_col7, #T_06b9d_row18_col7, #T_06b9d_row19_col7 {\n",
              "  text-align: left;\n",
              "  background-color: lightgrey;\n",
              "}\n",
              "#T_06b9d_row16_col7 {\n",
              "  text-align: left;\n",
              "  background-color: yellow;\n",
              "  background-color: lightgrey;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_06b9d\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_06b9d_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_06b9d_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
              "      <th id=\"T_06b9d_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
              "      <th id=\"T_06b9d_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
              "      <th id=\"T_06b9d_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
              "      <th id=\"T_06b9d_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
              "      <th id=\"T_06b9d_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
              "      <th id=\"T_06b9d_level0_col7\" class=\"col_heading level0 col7\" >TT (Sec)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row0\" class=\"row_heading level0 row0\" >huber</th>\n",
              "      <td id=\"T_06b9d_row0_col0\" class=\"data row0 col0\" >Huber Regressor</td>\n",
              "      <td id=\"T_06b9d_row0_col1\" class=\"data row0 col1\" >54.9854</td>\n",
              "      <td id=\"T_06b9d_row0_col2\" class=\"data row0 col2\" >29459.0022</td>\n",
              "      <td id=\"T_06b9d_row0_col3\" class=\"data row0 col3\" >171.6006</td>\n",
              "      <td id=\"T_06b9d_row0_col4\" class=\"data row0 col4\" >-0.0167</td>\n",
              "      <td id=\"T_06b9d_row0_col5\" class=\"data row0 col5\" >1.6046</td>\n",
              "      <td id=\"T_06b9d_row0_col6\" class=\"data row0 col6\" >1.5096</td>\n",
              "      <td id=\"T_06b9d_row0_col7\" class=\"data row0 col7\" >8.5900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row1\" class=\"row_heading level0 row1\" >et</th>\n",
              "      <td id=\"T_06b9d_row1_col0\" class=\"data row1 col0\" >Extra Trees Regressor</td>\n",
              "      <td id=\"T_06b9d_row1_col1\" class=\"data row1 col1\" >55.6215</td>\n",
              "      <td id=\"T_06b9d_row1_col2\" class=\"data row1 col2\" >23147.8583</td>\n",
              "      <td id=\"T_06b9d_row1_col3\" class=\"data row1 col3\" >152.1201</td>\n",
              "      <td id=\"T_06b9d_row1_col4\" class=\"data row1 col4\" >0.2009</td>\n",
              "      <td id=\"T_06b9d_row1_col5\" class=\"data row1 col5\" >1.1782</td>\n",
              "      <td id=\"T_06b9d_row1_col6\" class=\"data row1 col6\" >7.9439</td>\n",
              "      <td id=\"T_06b9d_row1_col7\" class=\"data row1 col7\" >49.7030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row2\" class=\"row_heading level0 row2\" >lightgbm</th>\n",
              "      <td id=\"T_06b9d_row2_col0\" class=\"data row2 col0\" >Light Gradient Boosting Machine</td>\n",
              "      <td id=\"T_06b9d_row2_col1\" class=\"data row2 col1\" >56.1851</td>\n",
              "      <td id=\"T_06b9d_row2_col2\" class=\"data row2 col2\" >23288.6768</td>\n",
              "      <td id=\"T_06b9d_row2_col3\" class=\"data row2 col3\" >152.5767</td>\n",
              "      <td id=\"T_06b9d_row2_col4\" class=\"data row2 col4\" >0.1961</td>\n",
              "      <td id=\"T_06b9d_row2_col5\" class=\"data row2 col5\" >1.3281</td>\n",
              "      <td id=\"T_06b9d_row2_col6\" class=\"data row2 col6\" >8.2002</td>\n",
              "      <td id=\"T_06b9d_row2_col7\" class=\"data row2 col7\" >3.1570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row3\" class=\"row_heading level0 row3\" >rf</th>\n",
              "      <td id=\"T_06b9d_row3_col0\" class=\"data row3 col0\" >Random Forest Regressor</td>\n",
              "      <td id=\"T_06b9d_row3_col1\" class=\"data row3 col1\" >56.2211</td>\n",
              "      <td id=\"T_06b9d_row3_col2\" class=\"data row3 col2\" >23626.3472</td>\n",
              "      <td id=\"T_06b9d_row3_col3\" class=\"data row3 col3\" >153.6832</td>\n",
              "      <td id=\"T_06b9d_row3_col4\" class=\"data row3 col4\" >0.1843</td>\n",
              "      <td id=\"T_06b9d_row3_col5\" class=\"data row3 col5\" >1.1180</td>\n",
              "      <td id=\"T_06b9d_row3_col6\" class=\"data row3 col6\" >9.1540</td>\n",
              "      <td id=\"T_06b9d_row3_col7\" class=\"data row3 col7\" >74.7390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row4\" class=\"row_heading level0 row4\" >catboost</th>\n",
              "      <td id=\"T_06b9d_row4_col0\" class=\"data row4 col0\" >CatBoost Regressor</td>\n",
              "      <td id=\"T_06b9d_row4_col1\" class=\"data row4 col1\" >56.5754</td>\n",
              "      <td id=\"T_06b9d_row4_col2\" class=\"data row4 col2\" >22502.2461</td>\n",
              "      <td id=\"T_06b9d_row4_col3\" class=\"data row4 col3\" >149.9807</td>\n",
              "      <td id=\"T_06b9d_row4_col4\" class=\"data row4 col4\" >0.2232</td>\n",
              "      <td id=\"T_06b9d_row4_col5\" class=\"data row4 col5\" >1.5418</td>\n",
              "      <td id=\"T_06b9d_row4_col6\" class=\"data row4 col6\" >7.7854</td>\n",
              "      <td id=\"T_06b9d_row4_col7\" class=\"data row4 col7\" >7.3630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row5\" class=\"row_heading level0 row5\" >xgboost</th>\n",
              "      <td id=\"T_06b9d_row5_col0\" class=\"data row5 col0\" >Extreme Gradient Boosting</td>\n",
              "      <td id=\"T_06b9d_row5_col1\" class=\"data row5 col1\" >56.9273</td>\n",
              "      <td id=\"T_06b9d_row5_col2\" class=\"data row5 col2\" >22891.6742</td>\n",
              "      <td id=\"T_06b9d_row5_col3\" class=\"data row5 col3\" >151.2825</td>\n",
              "      <td id=\"T_06b9d_row5_col4\" class=\"data row5 col4\" >0.2096</td>\n",
              "      <td id=\"T_06b9d_row5_col5\" class=\"data row5 col5\" >1.5614</td>\n",
              "      <td id=\"T_06b9d_row5_col6\" class=\"data row5 col6\" >7.0685</td>\n",
              "      <td id=\"T_06b9d_row5_col7\" class=\"data row5 col7\" >3.1840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row6\" class=\"row_heading level0 row6\" >gbr</th>\n",
              "      <td id=\"T_06b9d_row6_col0\" class=\"data row6 col0\" >Gradient Boosting Regressor</td>\n",
              "      <td id=\"T_06b9d_row6_col1\" class=\"data row6 col1\" >59.6339</td>\n",
              "      <td id=\"T_06b9d_row6_col2\" class=\"data row6 col2\" >24076.1137</td>\n",
              "      <td id=\"T_06b9d_row6_col3\" class=\"data row6 col3\" >155.1372</td>\n",
              "      <td id=\"T_06b9d_row6_col4\" class=\"data row6 col4\" >0.1689</td>\n",
              "      <td id=\"T_06b9d_row6_col5\" class=\"data row6 col5\" >1.5893</td>\n",
              "      <td id=\"T_06b9d_row6_col6\" class=\"data row6 col6\" >8.2068</td>\n",
              "      <td id=\"T_06b9d_row6_col7\" class=\"data row6 col7\" >77.8910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row7\" class=\"row_heading level0 row7\" >knn</th>\n",
              "      <td id=\"T_06b9d_row7_col0\" class=\"data row7 col0\" >K Neighbors Regressor</td>\n",
              "      <td id=\"T_06b9d_row7_col1\" class=\"data row7 col1\" >63.9900</td>\n",
              "      <td id=\"T_06b9d_row7_col2\" class=\"data row7 col2\" >26721.4156</td>\n",
              "      <td id=\"T_06b9d_row7_col3\" class=\"data row7 col3\" >163.4444</td>\n",
              "      <td id=\"T_06b9d_row7_col4\" class=\"data row7 col4\" >0.0774</td>\n",
              "      <td id=\"T_06b9d_row7_col5\" class=\"data row7 col5\" >1.8694</td>\n",
              "      <td id=\"T_06b9d_row7_col6\" class=\"data row7 col6\" >5.2598</td>\n",
              "      <td id=\"T_06b9d_row7_col7\" class=\"data row7 col7\" >9.2520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row8\" class=\"row_heading level0 row8\" >dt</th>\n",
              "      <td id=\"T_06b9d_row8_col0\" class=\"data row8 col0\" >Decision Tree Regressor</td>\n",
              "      <td id=\"T_06b9d_row8_col1\" class=\"data row8 col1\" >68.3752</td>\n",
              "      <td id=\"T_06b9d_row8_col2\" class=\"data row8 col2\" >40239.4473</td>\n",
              "      <td id=\"T_06b9d_row8_col3\" class=\"data row8 col3\" >200.5720</td>\n",
              "      <td id=\"T_06b9d_row8_col4\" class=\"data row8 col4\" >-0.3907</td>\n",
              "      <td id=\"T_06b9d_row8_col5\" class=\"data row8 col5\" >1.2516</td>\n",
              "      <td id=\"T_06b9d_row8_col6\" class=\"data row8 col6\" >9.7241</td>\n",
              "      <td id=\"T_06b9d_row8_col7\" class=\"data row8 col7\" >7.4880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row9\" class=\"row_heading level0 row9\" >lr</th>\n",
              "      <td id=\"T_06b9d_row9_col0\" class=\"data row9 col0\" >Linear Regression</td>\n",
              "      <td id=\"T_06b9d_row9_col1\" class=\"data row9 col1\" >77.8328</td>\n",
              "      <td id=\"T_06b9d_row9_col2\" class=\"data row9 col2\" >26337.2801</td>\n",
              "      <td id=\"T_06b9d_row9_col3\" class=\"data row9 col3\" >162.2622</td>\n",
              "      <td id=\"T_06b9d_row9_col4\" class=\"data row9 col4\" >0.0908</td>\n",
              "      <td id=\"T_06b9d_row9_col5\" class=\"data row9 col5\" >2.5952</td>\n",
              "      <td id=\"T_06b9d_row9_col6\" class=\"data row9 col6\" >7.7996</td>\n",
              "      <td id=\"T_06b9d_row9_col7\" class=\"data row9 col7\" >3.0120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row10\" class=\"row_heading level0 row10\" >ridge</th>\n",
              "      <td id=\"T_06b9d_row10_col0\" class=\"data row10 col0\" >Ridge Regression</td>\n",
              "      <td id=\"T_06b9d_row10_col1\" class=\"data row10 col1\" >77.8912</td>\n",
              "      <td id=\"T_06b9d_row10_col2\" class=\"data row10 col2\" >26347.9121</td>\n",
              "      <td id=\"T_06b9d_row10_col3\" class=\"data row10 col3\" >162.2949</td>\n",
              "      <td id=\"T_06b9d_row10_col4\" class=\"data row10 col4\" >0.0904</td>\n",
              "      <td id=\"T_06b9d_row10_col5\" class=\"data row10 col5\" >2.5958</td>\n",
              "      <td id=\"T_06b9d_row10_col6\" class=\"data row10 col6\" >7.7714</td>\n",
              "      <td id=\"T_06b9d_row10_col7\" class=\"data row10 col7\" >2.3000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row11\" class=\"row_heading level0 row11\" >br</th>\n",
              "      <td id=\"T_06b9d_row11_col0\" class=\"data row11 col0\" >Bayesian Ridge</td>\n",
              "      <td id=\"T_06b9d_row11_col1\" class=\"data row11 col1\" >77.9080</td>\n",
              "      <td id=\"T_06b9d_row11_col2\" class=\"data row11 col2\" >26346.7133</td>\n",
              "      <td id=\"T_06b9d_row11_col3\" class=\"data row11 col3\" >162.2913</td>\n",
              "      <td id=\"T_06b9d_row11_col4\" class=\"data row11 col4\" >0.0905</td>\n",
              "      <td id=\"T_06b9d_row11_col5\" class=\"data row11 col5\" >2.5967</td>\n",
              "      <td id=\"T_06b9d_row11_col6\" class=\"data row11 col6\" >7.7707</td>\n",
              "      <td id=\"T_06b9d_row11_col7\" class=\"data row11 col7\" >2.9590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row12\" class=\"row_heading level0 row12\" >lasso</th>\n",
              "      <td id=\"T_06b9d_row12_col0\" class=\"data row12 col0\" >Lasso Regression</td>\n",
              "      <td id=\"T_06b9d_row12_col1\" class=\"data row12 col1\" >78.1307</td>\n",
              "      <td id=\"T_06b9d_row12_col2\" class=\"data row12 col2\" >26366.9942</td>\n",
              "      <td id=\"T_06b9d_row12_col3\" class=\"data row12 col3\" >162.3535</td>\n",
              "      <td id=\"T_06b9d_row12_col4\" class=\"data row12 col4\" >0.0898</td>\n",
              "      <td id=\"T_06b9d_row12_col5\" class=\"data row12 col5\" >2.6125</td>\n",
              "      <td id=\"T_06b9d_row12_col6\" class=\"data row12 col6\" >7.6131</td>\n",
              "      <td id=\"T_06b9d_row12_col7\" class=\"data row12 col7\" >4.1160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row13\" class=\"row_heading level0 row13\" >llar</th>\n",
              "      <td id=\"T_06b9d_row13_col0\" class=\"data row13 col0\" >Lasso Least Angle Regression</td>\n",
              "      <td id=\"T_06b9d_row13_col1\" class=\"data row13 col1\" >78.1307</td>\n",
              "      <td id=\"T_06b9d_row13_col2\" class=\"data row13 col2\" >26366.9942</td>\n",
              "      <td id=\"T_06b9d_row13_col3\" class=\"data row13 col3\" >162.3535</td>\n",
              "      <td id=\"T_06b9d_row13_col4\" class=\"data row13 col4\" >0.0898</td>\n",
              "      <td id=\"T_06b9d_row13_col5\" class=\"data row13 col5\" >2.6125</td>\n",
              "      <td id=\"T_06b9d_row13_col6\" class=\"data row13 col6\" >7.6131</td>\n",
              "      <td id=\"T_06b9d_row13_col7\" class=\"data row13 col7\" >2.2910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row14\" class=\"row_heading level0 row14\" >en</th>\n",
              "      <td id=\"T_06b9d_row14_col0\" class=\"data row14 col0\" >Elastic Net</td>\n",
              "      <td id=\"T_06b9d_row14_col1\" class=\"data row14 col1\" >78.3844</td>\n",
              "      <td id=\"T_06b9d_row14_col2\" class=\"data row14 col2\" >26407.7236</td>\n",
              "      <td id=\"T_06b9d_row14_col3\" class=\"data row14 col3\" >162.4787</td>\n",
              "      <td id=\"T_06b9d_row14_col4\" class=\"data row14 col4\" >0.0884</td>\n",
              "      <td id=\"T_06b9d_row14_col5\" class=\"data row14 col5\" >2.6237</td>\n",
              "      <td id=\"T_06b9d_row14_col6\" class=\"data row14 col6\" >7.5547</td>\n",
              "      <td id=\"T_06b9d_row14_col7\" class=\"data row14 col7\" >4.2640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row15\" class=\"row_heading level0 row15\" >omp</th>\n",
              "      <td id=\"T_06b9d_row15_col0\" class=\"data row15 col0\" >Orthogonal Matching Pursuit</td>\n",
              "      <td id=\"T_06b9d_row15_col1\" class=\"data row15 col1\" >79.3966</td>\n",
              "      <td id=\"T_06b9d_row15_col2\" class=\"data row15 col2\" >26618.8979</td>\n",
              "      <td id=\"T_06b9d_row15_col3\" class=\"data row15 col3\" >163.1271</td>\n",
              "      <td id=\"T_06b9d_row15_col4\" class=\"data row15 col4\" >0.0811</td>\n",
              "      <td id=\"T_06b9d_row15_col5\" class=\"data row15 col5\" >2.6550</td>\n",
              "      <td id=\"T_06b9d_row15_col6\" class=\"data row15 col6\" >7.5913</td>\n",
              "      <td id=\"T_06b9d_row15_col7\" class=\"data row15 col7\" >2.3310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row16\" class=\"row_heading level0 row16\" >dummy</th>\n",
              "      <td id=\"T_06b9d_row16_col0\" class=\"data row16 col0\" >Dummy Regressor</td>\n",
              "      <td id=\"T_06b9d_row16_col1\" class=\"data row16 col1\" >80.3622</td>\n",
              "      <td id=\"T_06b9d_row16_col2\" class=\"data row16 col2\" >28973.5682</td>\n",
              "      <td id=\"T_06b9d_row16_col3\" class=\"data row16 col3\" >170.1836</td>\n",
              "      <td id=\"T_06b9d_row16_col4\" class=\"data row16 col4\" >-0.0001</td>\n",
              "      <td id=\"T_06b9d_row16_col5\" class=\"data row16 col5\" >2.8841</td>\n",
              "      <td id=\"T_06b9d_row16_col6\" class=\"data row16 col6\" >7.2459</td>\n",
              "      <td id=\"T_06b9d_row16_col7\" class=\"data row16 col7\" >2.0490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row17\" class=\"row_heading level0 row17\" >ada</th>\n",
              "      <td id=\"T_06b9d_row17_col0\" class=\"data row17 col0\" >AdaBoost Regressor</td>\n",
              "      <td id=\"T_06b9d_row17_col1\" class=\"data row17 col1\" >89.8544</td>\n",
              "      <td id=\"T_06b9d_row17_col2\" class=\"data row17 col2\" >38590.1174</td>\n",
              "      <td id=\"T_06b9d_row17_col3\" class=\"data row17 col3\" >196.4203</td>\n",
              "      <td id=\"T_06b9d_row17_col4\" class=\"data row17 col4\" >-0.3328</td>\n",
              "      <td id=\"T_06b9d_row17_col5\" class=\"data row17 col5\" >1.5051</td>\n",
              "      <td id=\"T_06b9d_row17_col6\" class=\"data row17 col6\" >17.7295</td>\n",
              "      <td id=\"T_06b9d_row17_col7\" class=\"data row17 col7\" >12.2600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row18\" class=\"row_heading level0 row18\" >par</th>\n",
              "      <td id=\"T_06b9d_row18_col0\" class=\"data row18 col0\" >Passive Aggressive Regressor</td>\n",
              "      <td id=\"T_06b9d_row18_col1\" class=\"data row18 col1\" >415.1578</td>\n",
              "      <td id=\"T_06b9d_row18_col2\" class=\"data row18 col2\" >484213.4778</td>\n",
              "      <td id=\"T_06b9d_row18_col3\" class=\"data row18 col3\" >638.9320</td>\n",
              "      <td id=\"T_06b9d_row18_col4\" class=\"data row18 col4\" >-15.6746</td>\n",
              "      <td id=\"T_06b9d_row18_col5\" class=\"data row18 col5\" >3.8738</td>\n",
              "      <td id=\"T_06b9d_row18_col6\" class=\"data row18 col6\" >45.0007</td>\n",
              "      <td id=\"T_06b9d_row18_col7\" class=\"data row18 col7\" >2.5830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row19\" class=\"row_heading level0 row19\" >lar</th>\n",
              "      <td id=\"T_06b9d_row19_col0\" class=\"data row19 col0\" >Least Angle Regression</td>\n",
              "      <td id=\"T_06b9d_row19_col1\" class=\"data row19 col1\" >21984.0250</td>\n",
              "      <td id=\"T_06b9d_row19_col2\" class=\"data row19 col2\" >380444527917.7170</td>\n",
              "      <td id=\"T_06b9d_row19_col3\" class=\"data row19 col3\" >300440.2333</td>\n",
              "      <td id=\"T_06b9d_row19_col4\" class=\"data row19 col4\" >-12674944.7068</td>\n",
              "      <td id=\"T_06b9d_row19_col5\" class=\"data row19 col5\" >4.9152</td>\n",
              "      <td id=\"T_06b9d_row19_col6\" class=\"data row19 col6\" >1966.9274</td>\n",
              "      <td id=\"T_06b9d_row19_col7\" class=\"data row19 col7\" >2.3580</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x28d87c03640>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[HuberRegressor(),\n",
              " ExtraTreesRegressor(n_jobs=-1, random_state=724),\n",
              " LGBMRegressor(device='gpu', n_jobs=-1, random_state=724),\n",
              " RandomForestRegressor(n_jobs=-1, random_state=724),\n",
              " <catboost.core.CatBoostRegressor at 0x28d89c95040>]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best = compare_models(n_select=5, sort='MAE')\n",
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Initiated</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>14:34:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Status</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Searching Hyperparameters</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estimator</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Huber Regressor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         \n",
              "                                                                         \n",
              "Initiated  . . . . . . . . . . . . . . . . . .                   14:34:39\n",
              "Status     . . . . . . . . . . . . . . . . . .  Searching Hyperparameters\n",
              "Estimator  . . . . . . . . . . . . . . . . . .            Huber Regressor"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6d79c32f9c44b71888d8e4d7d844f7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cat_t \u001b[39m=\u001b[39m tune_model(best, optimize\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m'\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m) \u001b[39m#lgbma\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cat_t\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\functional.py:1197\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1006\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[0;32m   1007\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1026\u001b[0m ):\n\u001b[0;32m   1027\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \n\u001b[0;32m   1195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39mtune_model(\n\u001b[0;32m   1198\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1199\u001b[0m         fold\u001b[39m=\u001b[39mfold,\n\u001b[0;32m   1200\u001b[0m         \u001b[39mround\u001b[39m\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m,\n\u001b[0;32m   1201\u001b[0m         n_iter\u001b[39m=\u001b[39mn_iter,\n\u001b[0;32m   1202\u001b[0m         custom_grid\u001b[39m=\u001b[39mcustom_grid,\n\u001b[0;32m   1203\u001b[0m         optimize\u001b[39m=\u001b[39moptimize,\n\u001b[0;32m   1204\u001b[0m         custom_scorer\u001b[39m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1205\u001b[0m         search_library\u001b[39m=\u001b[39msearch_library,\n\u001b[0;32m   1206\u001b[0m         search_algorithm\u001b[39m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1207\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m   1208\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1209\u001b[0m         choose_better\u001b[39m=\u001b[39mchoose_better,\n\u001b[0;32m   1210\u001b[0m         fit_kwargs\u001b[39m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1211\u001b[0m         groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m   1212\u001b[0m         return_tuner\u001b[39m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1213\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   1214\u001b[0m         tuner_verbose\u001b[39m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1215\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1216\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1217\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\oop.py:1499\u001b[0m, in \u001b[0;36mRegressionExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[0;32m   1308\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1309\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1328\u001b[0m ):\n\u001b[0;32m   1329\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \n\u001b[0;32m   1497\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1499\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtune_model(\n\u001b[0;32m   1500\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1501\u001b[0m         fold\u001b[39m=\u001b[39mfold,\n\u001b[0;32m   1502\u001b[0m         \u001b[39mround\u001b[39m\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m,\n\u001b[0;32m   1503\u001b[0m         n_iter\u001b[39m=\u001b[39mn_iter,\n\u001b[0;32m   1504\u001b[0m         custom_grid\u001b[39m=\u001b[39mcustom_grid,\n\u001b[0;32m   1505\u001b[0m         optimize\u001b[39m=\u001b[39moptimize,\n\u001b[0;32m   1506\u001b[0m         custom_scorer\u001b[39m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1507\u001b[0m         search_library\u001b[39m=\u001b[39msearch_library,\n\u001b[0;32m   1508\u001b[0m         search_algorithm\u001b[39m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1509\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m   1510\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1511\u001b[0m         choose_better\u001b[39m=\u001b[39mchoose_better,\n\u001b[0;32m   1512\u001b[0m         fit_kwargs\u001b[39m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1513\u001b[0m         groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m   1514\u001b[0m         return_tuner\u001b[39m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1515\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   1516\u001b[0m         tuner_verbose\u001b[39m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1517\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1518\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1519\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:2665\u001b[0m, in \u001b[0;36m_SupervisedExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   2657\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\n\u001b[0;32m   2658\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._search.sample_without_replacement\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2659\u001b[0m         pycaret\u001b[39m.\u001b[39minternal\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39msklearn\u001b[39m.\u001b[39m_mp_sample_without_replacement,\n\u001b[0;32m   2660\u001b[0m     ):\n\u001b[0;32m   2661\u001b[0m         \u001b[39mwith\u001b[39;00m patch(\n\u001b[0;32m   2662\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._search.ParameterGrid.__getitem__\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2663\u001b[0m             pycaret\u001b[39m.\u001b[39minternal\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39msklearn\u001b[39m.\u001b[39m_mp_ParameterGrid_getitem,\n\u001b[0;32m   2664\u001b[0m         ):\n\u001b[1;32m-> 2665\u001b[0m             model_grid\u001b[39m.\u001b[39mfit(data_X, data_y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m   2666\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2667\u001b[0m     model_grid\u001b[39m.\u001b[39mfit(data_X, data_y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1769\u001b[0m         ParameterSampler(\n\u001b[0;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1771\u001b[0m         )\n\u001b[0;32m   1772\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_t = tune_model(best, optimize='MAE', n_iter=100) #lgbma\n",
        "cat_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Initiated</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>17:01:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Status</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Fitting 10 Folds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estimator</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Stacking Regressor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                  \n",
              "                                                                  \n",
              "Initiated  . . . . . . . . . . . . . . . . . .            17:01:36\n",
              "Status     . . . . . . . . . . . . . . . . . .    Fitting 10 Folds\n",
              "Estimator  . . . . . . . . . . . . . . . . . .  Stacking Regressor"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6462f95876a04861ae0550c84d964288",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stack_lr \u001b[39m=\u001b[39m stack_models(best, optimize\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m'\u001b[39;49m, choose_better\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stack_lr\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\functional.py:1530\u001b[0m, in \u001b[0;36mstack_models\u001b[1;34m(estimator_list, meta_model, meta_model_fold, fold, round, restack, choose_better, optimize, fit_kwargs, groups, verbose, return_train_score)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack_models\u001b[39m(\n\u001b[0;32m   1431\u001b[0m     estimator_list: \u001b[39mlist\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1442\u001b[0m     return_train_score: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1443\u001b[0m ):\n\u001b[0;32m   1444\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m \u001b[39m    This function trains a meta model over select estimators passed in\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[39m    the ``estimator_list`` parameter. The output of this function is a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1527\u001b[0m \n\u001b[0;32m   1528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1530\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39;49mstack_models(\n\u001b[0;32m   1531\u001b[0m         estimator_list\u001b[39m=\u001b[39;49mestimator_list,\n\u001b[0;32m   1532\u001b[0m         meta_model\u001b[39m=\u001b[39;49mmeta_model,\n\u001b[0;32m   1533\u001b[0m         meta_model_fold\u001b[39m=\u001b[39;49mmeta_model_fold,\n\u001b[0;32m   1534\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[0;32m   1535\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[0;32m   1536\u001b[0m         restack\u001b[39m=\u001b[39;49mrestack,\n\u001b[0;32m   1537\u001b[0m         choose_better\u001b[39m=\u001b[39;49mchoose_better,\n\u001b[0;32m   1538\u001b[0m         optimize\u001b[39m=\u001b[39;49moptimize,\n\u001b[0;32m   1539\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   1540\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   1541\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1542\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   1543\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\oop.py:1830\u001b[0m, in \u001b[0;36mRegressionExperiment.stack_models\u001b[1;34m(self, estimator_list, meta_model, meta_model_fold, fold, round, restack, choose_better, optimize, fit_kwargs, groups, verbose, return_train_score)\u001b[0m\n\u001b[0;32m   1729\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack_models\u001b[39m(\n\u001b[0;32m   1730\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1731\u001b[0m     estimator_list: \u001b[39mlist\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1742\u001b[0m     return_train_score: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1743\u001b[0m ):\n\u001b[0;32m   1744\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1745\u001b[0m \u001b[39m    This function trains a meta model over select estimators passed in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m    the ``estimator_list`` parameter. The output of this function is a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \n\u001b[0;32m   1828\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1830\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstack_models(\n\u001b[0;32m   1831\u001b[0m         estimator_list\u001b[39m=\u001b[39;49mestimator_list,\n\u001b[0;32m   1832\u001b[0m         meta_model\u001b[39m=\u001b[39;49mmeta_model,\n\u001b[0;32m   1833\u001b[0m         meta_model_fold\u001b[39m=\u001b[39;49mmeta_model_fold,\n\u001b[0;32m   1834\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[0;32m   1835\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[0;32m   1836\u001b[0m         method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1837\u001b[0m         restack\u001b[39m=\u001b[39;49mrestack,\n\u001b[0;32m   1838\u001b[0m         choose_better\u001b[39m=\u001b[39;49mchoose_better,\n\u001b[0;32m   1839\u001b[0m         optimize\u001b[39m=\u001b[39;49moptimize,\n\u001b[0;32m   1840\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   1841\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   1842\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1843\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   1844\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:3853\u001b[0m, in \u001b[0;36m_SupervisedExperiment.stack_models\u001b[1;34m(self, estimator_list, meta_model, meta_model_fold, fold, round, method, restack, choose_better, optimize, fit_kwargs, groups, probability_threshold, verbose, return_train_score)\u001b[0m\n\u001b[0;32m   3848\u001b[0m display\u001b[39m.\u001b[39mmove_progress()\n\u001b[0;32m   3850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   3851\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSubProcess create_model() called ==================================\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3852\u001b[0m )\n\u001b[1;32m-> 3853\u001b[0m model, model_fit_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model(\n\u001b[0;32m   3854\u001b[0m     estimator\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   3855\u001b[0m     system\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   3856\u001b[0m     display\u001b[39m=\u001b[39;49mdisplay,\n\u001b[0;32m   3857\u001b[0m     fold\u001b[39m=\u001b[39;49mfold,\n\u001b[0;32m   3858\u001b[0m     \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[0;32m   3859\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   3860\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   3861\u001b[0m     probability_threshold\u001b[39m=\u001b[39;49mprobability_threshold,\n\u001b[0;32m   3862\u001b[0m     return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   3863\u001b[0m )\n\u001b[0;32m   3864\u001b[0m model_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpull()\n\u001b[0;32m   3865\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   3866\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSubProcess create_model() end ==================================\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3867\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1527\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model\u001b[1;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1524\u001b[0m         \u001b[39mreturn\u001b[39;00m model, model_fit_time\n\u001b[0;32m   1525\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m-> 1527\u001b[0m model, model_fit_time, model_results, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model_with_cv(\n\u001b[0;32m   1528\u001b[0m     model,\n\u001b[0;32m   1529\u001b[0m     data_X,\n\u001b[0;32m   1530\u001b[0m     data_y,\n\u001b[0;32m   1531\u001b[0m     fit_kwargs,\n\u001b[0;32m   1532\u001b[0m     \u001b[39mround\u001b[39;49m,\n\u001b[0;32m   1533\u001b[0m     cv,\n\u001b[0;32m   1534\u001b[0m     groups,\n\u001b[0;32m   1535\u001b[0m     metrics,\n\u001b[0;32m   1536\u001b[0m     refit,\n\u001b[0;32m   1537\u001b[0m     system,\n\u001b[0;32m   1538\u001b[0m     display,\n\u001b[0;32m   1539\u001b[0m     return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   1540\u001b[0m )\n\u001b[0;32m   1542\u001b[0m \u001b[39m# end runtime\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m runtime_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1121\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model_with_cv\u001b[1;34m(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, return_train_score)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     model_fit_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   1120\u001b[0m     \u001b[39mwith\u001b[39;00m redirect_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger):\n\u001b[1;32m-> 1121\u001b[0m         scores \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m   1122\u001b[0m             pipeline_with_model,\n\u001b[0;32m   1123\u001b[0m             data_X,\n\u001b[0;32m   1124\u001b[0m             data_y,\n\u001b[0;32m   1125\u001b[0m             cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m   1126\u001b[0m             groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   1127\u001b[0m             scoring\u001b[39m=\u001b[39;49mmetrics_dict,\n\u001b[0;32m   1128\u001b[0m             fit_params\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   1129\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m   1130\u001b[0m             return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   1131\u001b[0m             error_score\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   1132\u001b[0m         )\n\u001b[0;32m   1134\u001b[0m model_fit_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   1135\u001b[0m model_fit_time \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(model_fit_end \u001b[39m-\u001b[39m model_fit_start)\u001b[39m.\u001b[39mround(\u001b[39m2\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\patches\\sklearn.py:122\u001b[0m, in \u001b[0;36mfit_and_score\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._validation._score\u001b[39m\u001b[39m\"\u001b[39m, score(_score)):\n\u001b[0;32m    120\u001b[0m         \u001b[39mreturn\u001b[39;00m _fit_and_score(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m wrapper(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\patches\\sklearn.py:120\u001b[0m, in \u001b[0;36mfit_and_score.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._validation._score\u001b[39m\u001b[39m\"\u001b[39m, score(_score)):\n\u001b[1;32m--> 120\u001b[0m         \u001b[39mreturn\u001b[39;00m _fit_and_score(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pipeline.py:267\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    266\u001b[0m     fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 267\u001b[0m     fitted_estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_memory_fit(\n\u001b[0;32m    268\u001b[0m         clone(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]), X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[39m# Hacky way to make sure that the state of the estimator\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[39m# loaded from cache is carried over to the estimator\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[39m# in steps\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     _copy_estimator_state(fitted_estimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pipeline.py:66\u001b[0m, in \u001b[0;36m_fit_one\u001b[1;34m(transformer, X, y, message, **fit_params)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m signature(transformer\u001b[39m.\u001b[39mfit)\u001b[39m.\u001b[39mparameters:\n\u001b[0;32m     65\u001b[0m             args\u001b[39m.\u001b[39mappend(y)\n\u001b[1;32m---> 66\u001b[0m         transformer\u001b[39m.\u001b[39mfit(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m transformer\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:958\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the estimators.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \n\u001b[0;32m    938\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[39m    Returns a fitted instance.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    957\u001b[0m y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 958\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m         cv\u001b[39m.\u001b[39mrandom_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState()\n\u001b[0;32m    249\u001b[0m     fit_params \u001b[39m=\u001b[39m (\n\u001b[0;32m    250\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39msample_weight\u001b[39m\u001b[39m\"\u001b[39m: sample_weight} \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     )\n\u001b[1;32m--> 252\u001b[0m     predictions \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[0;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[0;32m    254\u001b[0m             clone(est),\n\u001b[0;32m    255\u001b[0m             X,\n\u001b[0;32m    256\u001b[0m             y,\n\u001b[0;32m    257\u001b[0m             cv\u001b[39m=\u001b[39;49mdeepcopy(cv),\n\u001b[0;32m    258\u001b[0m             method\u001b[39m=\u001b[39;49mmeth,\n\u001b[0;32m    259\u001b[0m             n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    260\u001b[0m             fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    261\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    262\u001b[0m         )\n\u001b[0;32m    263\u001b[0m         \u001b[39mfor\u001b[39;49;00m est, meth \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(all_estimators, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_method_)\n\u001b[0;32m    264\u001b[0m         \u001b[39mif\u001b[39;49;00m est \u001b[39m!=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdrop\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    265\u001b[0m     )\n\u001b[0;32m    267\u001b[0m \u001b[39m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[39m# Remove the None from the method as well.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_method_ \u001b[39m=\u001b[39m [\n\u001b[0;32m    270\u001b[0m     meth\n\u001b[0;32m    271\u001b[0m     \u001b[39mfor\u001b[39;00m (meth, est) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_method_, all_estimators)\n\u001b[0;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m est \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdrop\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:986\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    985\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 986\u001b[0m predictions \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    987\u001b[0m     delayed(_fit_and_predict)(\n\u001b[0;32m    988\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m splits\n\u001b[0;32m    991\u001b[0m )\n\u001b[0;32m    993\u001b[0m inv_test_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(\u001b[39mlen\u001b[39m(test_indices), dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[0;32m    994\u001b[0m inv_test_indices[test_indices] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(test_indices))\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:1068\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m   1067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1068\u001b[0m     estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m   1069\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(estimator, method)\n\u001b[0;32m   1070\u001b[0m predictions \u001b[39m=\u001b[39m func(X_test)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "stack_lr = stack_models(best, optimize='MAE', choose_better=True)\n",
        "stack_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Estimator [HuberRegressor(), ExtraTreesRegressor(n_jobs=-1, random_state=724), LGBMRegressor(device='gpu', n_jobs=-1, random_state=724), RandomForestRegressor(n_jobs=-1, random_state=724), <catboost.core.CatBoostRegressor object at 0x0000028D89C95040>] does not have the required fit() method.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stack_finalized \u001b[39m=\u001b[39m finalize_model(best)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stack_finalized\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\functional.py:1956\u001b[0m, in \u001b[0;36mfinalize_model\u001b[1;34m(estimator, fit_kwargs, groups, model_only, experiment_custom_tags)\u001b[0m\n\u001b[0;32m   1903\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinalize_model\u001b[39m(\n\u001b[0;32m   1905\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1909\u001b[0m     experiment_custom_tags: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1910\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m   1911\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1912\u001b[0m \u001b[39m    This function trains a given estimator on the entire dataset including the\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \u001b[39m    holdout set.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1953\u001b[0m \n\u001b[0;32m   1954\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1956\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39;49mfinalize_model(\n\u001b[0;32m   1957\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1958\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   1959\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   1960\u001b[0m         model_only\u001b[39m=\u001b[39;49mmodel_only,\n\u001b[0;32m   1961\u001b[0m         experiment_custom_tags\u001b[39m=\u001b[39;49mexperiment_custom_tags,\n\u001b[0;32m   1962\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\oop.py:2248\u001b[0m, in \u001b[0;36mRegressionExperiment.finalize_model\u001b[1;34m(self, estimator, fit_kwargs, groups, model_only, experiment_custom_tags)\u001b[0m\n\u001b[0;32m   2197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinalize_model\u001b[39m(\n\u001b[0;32m   2198\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2199\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2203\u001b[0m     experiment_custom_tags: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2204\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m   2205\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m \u001b[39m    This function trains a given estimator on the entire dataset including the\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m \u001b[39m    holdout set.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2245\u001b[0m \n\u001b[0;32m   2246\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2248\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfinalize_model(\n\u001b[0;32m   2249\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   2250\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   2251\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   2252\u001b[0m         model_only\u001b[39m=\u001b[39;49mmodel_only,\n\u001b[0;32m   2253\u001b[0m         experiment_custom_tags\u001b[39m=\u001b[39;49mexperiment_custom_tags,\n\u001b[0;32m   2254\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:4723\u001b[0m, in \u001b[0;36m_SupervisedExperiment.finalize_model\u001b[1;34m(self, estimator, fit_kwargs, groups, model_only, experiment_custom_tags)\u001b[0m\n\u001b[0;32m   4720\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed)\n\u001b[0;32m   4722\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinalizing \u001b[39m\u001b[39m{\u001b[39;00mestimator\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 4723\u001b[0m pipeline_final, model_fit_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model(\n\u001b[0;32m   4724\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   4725\u001b[0m     cross_validation\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   4726\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   4727\u001b[0m     system\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   4728\u001b[0m     X_train_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX,\n\u001b[0;32m   4729\u001b[0m     y_train_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my,\n\u001b[0;32m   4730\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs \u001b[39mor\u001b[39;49;00m {},\n\u001b[0;32m   4731\u001b[0m     predict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   4732\u001b[0m     groups\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_groups(groups, data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX),\n\u001b[0;32m   4733\u001b[0m     add_to_model_list\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   4734\u001b[0m     model_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   4735\u001b[0m )\n\u001b[0;32m   4737\u001b[0m \u001b[39m# dashboard logging\u001b[39;00m\n\u001b[0;32m   4738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogging_param:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1339\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model\u001b[1;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1335\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1336\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEstimator \u001b[39m\u001b[39m{\u001b[39;00mestimator\u001b[39m}\u001b[39;00m\u001b[39m not available. Please see docstring for list of available estimators.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1337\u001b[0m         )\n\u001b[0;32m   1338\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1340\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEstimator \u001b[39m\u001b[39m{\u001b[39;00mestimator\u001b[39m}\u001b[39;00m\u001b[39m does not have the required fit() method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1341\u001b[0m     )\n\u001b[0;32m   1343\u001b[0m \u001b[39m# checking fold parameter\u001b[39;00m\n\u001b[0;32m   1344\u001b[0m \u001b[39mif\u001b[39;00m fold \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m   1345\u001b[0m     \u001b[39mtype\u001b[39m(fold) \u001b[39mis\u001b[39;00m \u001b[39mint\u001b[39m \u001b[39mor\u001b[39;00m is_sklearn_cv_generator(fold)\n\u001b[0;32m   1346\u001b[0m ):\n",
            "\u001b[1;31mValueError\u001b[0m: Estimator [HuberRegressor(), ExtraTreesRegressor(n_jobs=-1, random_state=724), LGBMRegressor(device='gpu', n_jobs=-1, random_state=724), RandomForestRegressor(n_jobs=-1, random_state=724), <catboost.core.CatBoostRegressor object at 0x0000028D89C95040>] does not have the required fit() method."
          ]
        }
      ],
      "source": [
        "stack_finalized = finalize_model(best)\n",
        "stack_finalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>90.917114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>339.572906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>24.792130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>130.493073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>383.237549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>1316.202637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID      CI_HOUR\n",
              "0       TEST_000000    90.917114\n",
              "1       TEST_000001   339.572906\n",
              "2       TEST_000002     0.000000\n",
              "3       TEST_000003     0.000000\n",
              "4       TEST_000004    24.792130\n",
              "...             ...          ...\n",
              "244984  TEST_244984   130.493073\n",
              "244985  TEST_244985   383.237549\n",
              "244986  TEST_244986     0.000000\n",
              "244987  TEST_244987     0.000000\n",
              "244988  TEST_244988  1316.202637\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission = pd.read_csv('(f)bestaccuracy_nofit_scaled_sample_submission.csv')\n",
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('(f)bestaccuracy_nofit_scaled_sample_submission_2.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>98.878138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>206.534507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>67.811210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>29.505385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>205.425372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>954.309087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID     CI_HOUR\n",
              "0       TEST_000000   98.878138\n",
              "1       TEST_000001  206.534507\n",
              "2       TEST_000002    0.000000\n",
              "3       TEST_000003    0.000000\n",
              "4       TEST_000004   67.811210\n",
              "...             ...         ...\n",
              "244984  TEST_244984   29.505385\n",
              "244985  TEST_244985  205.425372\n",
              "244986  TEST_244986    0.000000\n",
              "244987  TEST_244987    0.000000\n",
              "244988  TEST_244988  954.309087\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission['CI_HOUR'] = stack_finalized.predict(test)\n",
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('pycat.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoML directory: AutoML_1\n",
            "The task is regression with evaluation metric user_defined_metric\n",
            "AutoML will use algorithms: ['LightGBM', 'Xgboost', 'CatBoost']\n",
            "AutoML will stack models\n",
            "AutoML will ensemble available models\n",
            "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
            "* Step adjust_validation will try to check up to 1 model\n",
            "1_DecisionTree user_defined_metric 79.057654 trained in 0.52 seconds\n",
            "Disable stacking for split validation\n",
            "Skip simple_algorithms because no parameters were generated.\n",
            "* Step default_algorithms will try to check up to 3 models\n",
            "There was an error during 2_Default_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 3_Default_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 4_Default_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "* Step not_so_random will try to check up to 27 models\n",
            "There was an error during 11_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 2_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 20_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 12_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 3_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 21_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 13_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 4_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 22_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 14_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 5_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 23_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 15_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 6_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 24_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 16_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 7_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 25_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 17_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 8_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 26_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 18_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 9_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 27_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 19_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 10_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 28_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "* Step golden_features will try to check up to 1 model\n",
            "None 10\n",
            "Add Golden Feature: month_sin_ratio_BDI_ADJ\n",
            "Add Golden Feature: month_sin_sum_DUBAI\n",
            "Add Golden Feature: day_sin_multiply_WTI\n",
            "Add Golden Feature: BRENT_ratio_day_sin\n",
            "Add Golden Feature: month_sin_multiply_DUBAI\n",
            "Add Golden Feature: day_sin_multiply_BRENT\n",
            "Add Golden Feature: month_sin_multiply_BRENT\n",
            "Add Golden Feature: day_sin_ratio_BRENT\n",
            "Add Golden Feature: day_sin_multiply_DUBAI\n",
            "Add Golden Feature: DUBAI_ratio_WTI\n",
            "Created 10 Golden Features in 6.82 seconds.\n",
            "1_DecisionTree_GoldenFeatures user_defined_metric 79.345917 trained in 8.02 seconds\n",
            "* Step kmeans_features will try to check up to 1 model\n",
            "1_DecisionTree_KMeansFeatures user_defined_metric 79.945139 trained in 2.93 seconds\n",
            "* Step insert_random_feature will try to check up to 1 model\n",
            "1_DecisionTree_RandomFeature user_defined_metric 79.057654 trained in 1.12 seconds\n",
            "Drop features ['BRENT', 'day_sin', 'random_feature']\n",
            "Skip features_selection because no parameters were generated.\n",
            "* Step hill_climbing_1 will try to check up to 3 models\n",
            "2_DecisionTree user_defined_metric 79.057654 trained in 0.59 seconds\n",
            "3_DecisionTree_GoldenFeatures user_defined_metric 79.345917 trained in 1.51 seconds\n",
            "4_DecisionTree user_defined_metric 79.945139 trained in 3.12 seconds\n",
            "* Step hill_climbing_2 will try to check up to 3 models\n",
            "5_DecisionTree user_defined_metric 79.634907 trained in 0.58 seconds\n",
            "6_DecisionTree user_defined_metric 79.634907 trained in 0.57 seconds\n",
            "7_DecisionTree_GoldenFeatures user_defined_metric 79.569052 trained in 1.16 seconds\n",
            "* Step ensemble will try to check up to 1 model\n",
            "Ensemble user_defined_metric 78.900765 trained in 0.33 seconds\n",
            "AutoML fit time: 36.89 seconds\n",
            "AutoML best model: Ensemble\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([63.11427878, 77.93100383, 63.11427878, ..., 59.73600808,\n",
              "       63.11427878, 63.11427878])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from supervised.automl import AutoML\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "'''\n",
        "automl = AutoML(\n",
        "    algorithms=[\"CatBoost\", \"Xgboost\", \"LightGBM\"],\n",
        "    model_time_limit=30*60,\n",
        "    start_random_models=10,\n",
        "    hill_climbing_steps=3,\n",
        "    top_models_to_improve=3,\n",
        "    golden_features=True,\n",
        "    features_selection=False,\n",
        "    stack_models=True,\n",
        "    train_ensemble=True,\n",
        "    explain_level=0,\n",
        "    validation_strategy={\n",
        "        \"validation_type\": \"kfold\",\n",
        "        \"k_folds\": 4,\n",
        "        \"shuffle\": False,\n",
        "        \"stratify\": True,\n",
        "    }\n",
        ")\n",
        "'''\n",
        "automl = AutoML(mode = \"Compete\",\n",
        "                algorithms = ['LightGBM', 'Xgboost', 'CatBoost'], golden_features=True,\n",
        "                ml_task = \"regression\", eval_metric=mean_absolute_error, random_state = 42, total_time_limit=100)\n",
        "\n",
        "automl.fit(train_x, train_y)\n",
        "\n",
        "predictions = automl.predict(test)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# h2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h2o in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (3.42.0.3)\n",
            "Requirement already satisfied: requests in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from h2o) (2.30.0)\n",
            "Requirement already satisfied: tabulate in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from requests->h2o) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from requests->h2o) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from requests->h2o) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from requests->h2o) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install h2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n"
          ]
        },
        {
          "ename": "H2OStartupError",
          "evalue": "Cannot find Java. Please install the latest JRE from\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mH2OConnectionError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\h2o.py:268\u001b[0m, in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     h2oconn \u001b[39m=\u001b[39m H2OConnection\u001b[39m.\u001b[39;49mopen(url\u001b[39m=\u001b[39;49murl, ip\u001b[39m=\u001b[39;49mip, port\u001b[39m=\u001b[39;49mport, name\u001b[39m=\u001b[39;49mname, https\u001b[39m=\u001b[39;49mhttps,\n\u001b[0;32m    269\u001b[0m                                  verify_ssl_certificates\u001b[39m=\u001b[39;49mverify_ssl_certificates, cacert\u001b[39m=\u001b[39;49mcacert,\n\u001b[0;32m    270\u001b[0m                                  auth\u001b[39m=\u001b[39;49mauth, proxy\u001b[39m=\u001b[39;49mproxy, cookies\u001b[39m=\u001b[39;49mcookies, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    271\u001b[0m                                  msgs\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mChecking whether there is an H2O instance running at \u001b[39;49m\u001b[39m{url}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    272\u001b[0m                                        \u001b[39m\"\u001b[39;49m\u001b[39mconnected.\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mnot found.\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    273\u001b[0m                                  strict_version_check\u001b[39m=\u001b[39;49msvc)\n\u001b[0;32m    274\u001b[0m \u001b[39mexcept\u001b[39;00m H2OConnectionError:\n\u001b[0;32m    275\u001b[0m     \u001b[39m# Backward compatibility: in init() port parameter really meant \"baseport\" when starting a local server...\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\connection.py:406\u001b[0m, in \u001b[0;36mH2OConnection.open\u001b[1;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, cacert, proxy, cookies, verbose, msgs, strict_version_check)\u001b[0m\n\u001b[0;32m    405\u001b[0m conn\u001b[39m.\u001b[39m_timeout \u001b[39m=\u001b[39m \u001b[39m3.0\u001b[39m\n\u001b[1;32m--> 406\u001b[0m conn\u001b[39m.\u001b[39m_cluster \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49m_test_connection(retries, messages\u001b[39m=\u001b[39;49mmsgs)\n\u001b[0;32m    407\u001b[0m \u001b[39m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# setting for now, for no good reason other than to ignore all those bugs :(\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\connection.py:713\u001b[0m, in \u001b[0;36mH2OConnection._test_connection\u001b[1;34m(self, max_retries, messages)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 713\u001b[0m     \u001b[39mraise\u001b[39;00m H2OConnectionError(\u001b[39m\"\u001b[39m\u001b[39mCould not establish link to the H2O cloud \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m after \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m retries\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    714\u001b[0m                              \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_url, max_retries, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(errors)))\n",
            "\u001b[1;31mH2OConnectionError\u001b[0m: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries\n[26:18.53] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025451CE2EE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n[26:22.83] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000254527547F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n[26:27.10] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002545275F160>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n[26:31.42] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002545275FA60>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n[26:35.72] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002545542A400>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mH2OStartupError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mh2o\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mh2o\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautoml\u001b[39;00m \u001b[39mimport\u001b[39;00m H2OAutoML\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m h2o\u001b[39m.\u001b[39;49minit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Assuming your data is in a pandas DataFrame called 'df'\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m h2o_frame \u001b[39m=\u001b[39m h2o\u001b[39m.\u001b[39mH2OFrame(train)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\h2o.py:285\u001b[0m, in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m https:\n\u001b[0;32m    282\u001b[0m         \u001b[39mraise\u001b[39;00m H2OConnectionError(\u001b[39m'\u001b[39m\u001b[39mStarting local server is not available with https enabled. You may start local\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    283\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39m instance of H2O with https manually \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    284\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39m(https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#new-user-quick-start).\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 285\u001b[0m     hs \u001b[39m=\u001b[39m H2OLocalServer\u001b[39m.\u001b[39;49mstart(nthreads\u001b[39m=\u001b[39;49mnthreads, enable_assertions\u001b[39m=\u001b[39;49menable_assertions, max_mem_size\u001b[39m=\u001b[39;49mmmax,\n\u001b[0;32m    286\u001b[0m                               min_mem_size\u001b[39m=\u001b[39;49mmmin, ice_root\u001b[39m=\u001b[39;49mice_root, log_dir\u001b[39m=\u001b[39;49mlog_dir, log_level\u001b[39m=\u001b[39;49mlog_level,\n\u001b[0;32m    287\u001b[0m                               max_log_file_size\u001b[39m=\u001b[39;49mmax_log_file_size, port\u001b[39m=\u001b[39;49mport, name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    288\u001b[0m                               extra_classpath\u001b[39m=\u001b[39;49mextra_classpath, jvm_custom_args\u001b[39m=\u001b[39;49mjvm_custom_args,\n\u001b[0;32m    289\u001b[0m                               bind_to_localhost\u001b[39m=\u001b[39;49mbind_to_localhost)\n\u001b[0;32m    290\u001b[0m     h2oconn \u001b[39m=\u001b[39m H2OConnection\u001b[39m.\u001b[39mopen(server\u001b[39m=\u001b[39mhs, https\u001b[39m=\u001b[39mhttps, verify_ssl_certificates\u001b[39m=\u001b[39mverify_ssl_certificates,\n\u001b[0;32m    291\u001b[0m                                  cacert\u001b[39m=\u001b[39mcacert, auth\u001b[39m=\u001b[39mauth, proxy\u001b[39m=\u001b[39mproxy, cookies\u001b[39m=\u001b[39mcookies, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m                                  strict_version_check\u001b[39m=\u001b[39msvc)\n\u001b[0;32m    293\u001b[0m h2oconn\u001b[39m.\u001b[39mcluster\u001b[39m.\u001b[39mtimezone \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUTC\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\server.py:139\u001b[0m, in \u001b[0;36mH2OLocalServer.start\u001b[1;34m(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, log_dir, log_level, max_log_file_size, port, name, extra_classpath, verbose, jvm_custom_args, bind_to_localhost)\u001b[0m\n\u001b[0;32m    136\u001b[0m     hs\u001b[39m.\u001b[39m_tempdir \u001b[39m=\u001b[39m hs\u001b[39m.\u001b[39m_ice_root\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAttempting to start a local H2O server...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m hs\u001b[39m.\u001b[39;49m_launch_server(port\u001b[39m=\u001b[39;49mport, baseport\u001b[39m=\u001b[39;49mbaseport, nthreads\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(nthreads), ea\u001b[39m=\u001b[39;49menable_assertions,\n\u001b[0;32m    140\u001b[0m                   mmax\u001b[39m=\u001b[39;49mmax_mem_size, mmin\u001b[39m=\u001b[39;49mmin_mem_size, jvm_custom_args\u001b[39m=\u001b[39;49mjvm_custom_args,\n\u001b[0;32m    141\u001b[0m                   bind_to_localhost\u001b[39m=\u001b[39;49mbind_to_localhost, log_dir\u001b[39m=\u001b[39;49mlog_dir, log_level\u001b[39m=\u001b[39;49mlog_level, max_log_file_size\u001b[39m=\u001b[39;49mmax_log_file_size)\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  Server is running at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m://\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (hs\u001b[39m.\u001b[39mscheme, hs\u001b[39m.\u001b[39mip, hs\u001b[39m.\u001b[39mport))\n\u001b[0;32m    143\u001b[0m atexit\u001b[39m.\u001b[39mregister(\u001b[39mlambda\u001b[39;00m: hs\u001b[39m.\u001b[39mshutdown())\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\server.py:271\u001b[0m, in \u001b[0;36mH2OLocalServer._launch_server\u001b[1;34m(self, port, baseport, mmax, mmin, ea, nthreads, jvm_custom_args, bind_to_localhost, log_dir, log_level, max_log_file_size)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ip \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m127.0.0.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m \u001b[39m# Find Java and check version. (Note that subprocess.check_output returns the output as a bytes object)\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m java \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_java()\n\u001b[0;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_java(java, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose)\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\server.py:443\u001b[0m, in \u001b[0;36mH2OLocalServer._find_java\u001b[1;34m()\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dirpath, java)\n\u001b[0;32m    442\u001b[0m \u001b[39m# not found...\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m \u001b[39mraise\u001b[39;00m H2OStartupError(\u001b[39m\"\u001b[39m\u001b[39mCannot find Java. Please install the latest JRE from\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mH2OStartupError\u001b[0m: Cannot find Java. Please install the latest JRE from\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements"
          ]
        }
      ],
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "h2o.init()\n",
        "# Assuming your data is in a pandas DataFrame called 'df'\n",
        "h2o_frame = h2o.H2OFrame(train)\n",
        "target = 'CI_HOUR'\n",
        "features = h2o_frame.columns\n",
        "features.remove(target)\n",
        "# Set up the AutoML object\n",
        "aml = H2OAutoML(max_runtime_secs=3600, # Adjust as needed\n",
        "                seed=313,\n",
        "                stopping_metric=\"MAE\", # Setting MAE as the stopping metric\n",
        "                sort_metric=\"MAE\")     # Setting MAE as the sorting metric for the leaderboard\n",
        "\n",
        "# Train the model\n",
        "aml.train(x=features, y=target, training_frame=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lb = aml.leaderboard\n",
        "print(lb.head(rows=lb.nrows))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = aml.predict(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mae = h2o.h2o.mae(preds, test[target])\n",
        "print(f\"Mean Absolute Error on Test Set: {mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h2o.cluster().shutdown()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "#import xgboost as xgb\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def objective(trial: Trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "            'iterations':trial.suggest_int(\"iterations\", 300, 1000),\n",
        "            'learning_rate' : trial.suggest_uniform('learning_rate',0.1, 1),\n",
        "            'depth': trial.suggest_int('depth',5, 16),\n",
        "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
        "            'reg_lambda': trial.suggest_uniform('reg_lambda',30,100),\n",
        "            'subsample': trial.suggest_uniform('subsample',0.3,1),\n",
        "            'random_strength': trial.suggest_uniform('random_strength',10,100),\n",
        "            'od_wait':trial.suggest_int('od_wait', 10, 150),\n",
        "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,20),\n",
        "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 1, 100),\n",
        "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0., 1.0),\n",
        "            'random_state' : 313,\n",
        "            'verbose' : 0,\n",
        "        }\n",
        "    #'task_type' : 'GPU',\n",
        "    #\"eval_metric\":'RMSE',\n",
        "    cat = CatBoostRegressor(**params)\n",
        "    cat.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val,y_val)],cat_features=cat_features,\n",
        "              verbose=False)\n",
        "    cat_pred = cat.predict(X_val)\n",
        "    score = mean_absolute_error(y_val, cat_pred)\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import bisect\n",
        "'''\n",
        "# Categorical 컬럼 인코딩\n",
        "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "encoders = {}\n",
        "\n",
        "for feature in tqdm(categorical_features, desc=\"Encoding features\"):\n",
        "    le = LabelEncoder()\n",
        "    train[feature] = le.fit_transform(train[feature].astype(str))\n",
        "    le_classes_set = set(le.classes_)\n",
        "    test[feature] = test[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
        "    le_classes = le.classes_.tolist()\n",
        "    bisect.insort_left(le_classes, '-1')\n",
        "    le.classes_ = np.array(le_classes)\n",
        "    test[feature] = le.transform(test[feature].astype(str))\n",
        "    encoders[feature] = le\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "\n",
        "# 결측치 처리\n",
        "train_x.fillna(train_x.mean(), inplace=True)\n",
        "test.fillna(train_x.mean(), inplace=True)\n",
        "'''\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'country_cluster', 'PORT_SIZE_Zone', 'BREADTH', 'DEPTH', 'DRAUGHT', 'PO_Y_M_D', 'FLAG']\n",
        "# 수치형 변수만 대상으로 결측치 대체\n",
        "numeric_cols = train_x.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 훈련 데이터의 평균 계산\n",
        "mean_values = train_x[numeric_cols].mean()\n",
        "\n",
        "# 결측치 대체\n",
        "train_x[numeric_cols] = train_x[numeric_cols].fillna(mean_values)\n",
        "test[numeric_cols] = test[numeric_cols].fillna(mean_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모든 카테고리형 변수를 문자열로 변환\n",
        "categorical_cols = train_x.select_dtypes(include=['object', 'category']).columns\n",
        "train_x[categorical_cols] = train_x[categorical_cols].astype(str)\n",
        "test[categorical_cols] = test[categorical_cols].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-13 16:57:11,821] A new study created in memory with name: no-name-4f7cc427-49e0-4f9c-b8ec-3af8b8181bd9\n",
            "[W 2023-10-13 16:57:11,824] Trial 0 failed with parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306} because of the following error: ValueError(\"'ARI_CO' is not in list\").\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\ineeji\\AppData\\Local\\Temp\\ipykernel_4244\\2899142785.py\", line 12, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val.values), n_trials=20)\n",
            "  File \"C:\\Users\\ineeji\\AppData\\Local\\Temp\\ipykernel_4244\\1586171062.py\", line 33, in objective\n",
            "    cat.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val,y_val)],cat_features=cat_features,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 5734, in fit\n",
            "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 2341, in _fit\n",
            "    train_params = self._prepare_train_params(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 2222, in _prepare_train_params\n",
            "    train_pool = _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 1438, in _build_train_pool\n",
            "    train_pool = Pool(X, y, cat_features=cat_features, text_features=text_features, embedding_features=embedding_features, pairs=pairs, weight=sample_weight, group_id=group_id,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 792, in __init__\n",
            "    self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 1371, in _init\n",
            "    cat_features = _get_features_indices(cat_features, feature_names)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 269, in _get_features_indices\n",
            "    return [\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 270, in <listcomp>\n",
            "    feature_names.index(f) if isinstance(f, STRING_TYPES) else f\n",
            "ValueError: 'ARI_CO' is not in list\n",
            "[W 2023-10-13 16:57:11,827] Trial 0 failed with value None.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 1...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "'ARI_CO' is not in list",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m313\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: objective(trial, X_train, y_train, X_val, y_val\u001b[39m.\u001b[39;49mvalues), n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m     _optimize(\n\u001b[0;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    453\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m313\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val\u001b[39m.\u001b[39;49mvalues), n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 40\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#'task_type' : 'GPU',\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#\"eval_metric\":'RMSE',\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m cat \u001b[39m=\u001b[39m CatBoostRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m cat\u001b[39m.\u001b[39;49mfit(X_train, y_train, eval_set\u001b[39m=\u001b[39;49m[(X_train,y_train),(X_val,y_val)],cat_features\u001b[39m=\u001b[39;49mcat_features,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m           verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m cat_pred \u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39mpredict(X_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m score \u001b[39m=\u001b[39m mean_absolute_error(y_val, cat_pred)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:5734\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5731\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m   5732\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m-> 5734\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[0;32m   5735\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[0;32m   5736\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[0;32m   5737\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:2341\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2338\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, PATH_TYPES \u001b[39m+\u001b[39m (Pool,)):\n\u001b[0;32m   2339\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2341\u001b[0m train_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_train_params(\n\u001b[0;32m   2342\u001b[0m     X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, cat_features\u001b[39m=\u001b[39;49mcat_features, text_features\u001b[39m=\u001b[39;49mtext_features, embedding_features\u001b[39m=\u001b[39;49membedding_features,\n\u001b[0;32m   2343\u001b[0m     pairs\u001b[39m=\u001b[39;49mpairs, sample_weight\u001b[39m=\u001b[39;49msample_weight, group_id\u001b[39m=\u001b[39;49mgroup_id, group_weight\u001b[39m=\u001b[39;49mgroup_weight,\n\u001b[0;32m   2344\u001b[0m     subgroup_id\u001b[39m=\u001b[39;49msubgroup_id, pairs_weight\u001b[39m=\u001b[39;49mpairs_weight, baseline\u001b[39m=\u001b[39;49mbaseline, use_best_model\u001b[39m=\u001b[39;49muse_best_model,\n\u001b[0;32m   2345\u001b[0m     eval_set\u001b[39m=\u001b[39;49meval_set, verbose\u001b[39m=\u001b[39;49mverbose, logging_level\u001b[39m=\u001b[39;49mlogging_level, plot\u001b[39m=\u001b[39;49mplot, plot_file\u001b[39m=\u001b[39;49mplot_file,\n\u001b[0;32m   2346\u001b[0m     column_description\u001b[39m=\u001b[39;49mcolumn_description, verbose_eval\u001b[39m=\u001b[39;49mverbose_eval, metric_period\u001b[39m=\u001b[39;49mmetric_period,\n\u001b[0;32m   2347\u001b[0m     silent\u001b[39m=\u001b[39;49msilent, early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds, save_snapshot\u001b[39m=\u001b[39;49msave_snapshot,\n\u001b[0;32m   2348\u001b[0m     snapshot_file\u001b[39m=\u001b[39;49msnapshot_file, snapshot_interval\u001b[39m=\u001b[39;49msnapshot_interval, init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[0;32m   2349\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[0;32m   2350\u001b[0m )\n\u001b[0;32m   2351\u001b[0m params \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2352\u001b[0m train_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mtrain_pool\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:2222\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   2219\u001b[0m text_features \u001b[39m=\u001b[39m _process_feature_indices(text_features, X, params, \u001b[39m'\u001b[39m\u001b[39mtext_features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2220\u001b[0m embedding_features \u001b[39m=\u001b[39m _process_feature_indices(embedding_features, X, params, \u001b[39m'\u001b[39m\u001b[39membedding_features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 2222\u001b[0m train_pool \u001b[39m=\u001b[39m _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n\u001b[0;32m   2223\u001b[0m                                sample_weight, group_id, group_weight, subgroup_id, pairs_weight,\n\u001b[0;32m   2224\u001b[0m                                baseline, column_description)\n\u001b[0;32m   2225\u001b[0m \u001b[39mif\u001b[39;00m train_pool\u001b[39m.\u001b[39mis_empty_:\n\u001b[0;32m   2226\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mX is empty.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:1438\u001b[0m, in \u001b[0;36m_build_train_pool\u001b[1;34m(X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1437\u001b[0m         \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39my has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1438\u001b[0m     train_pool \u001b[39m=\u001b[39m Pool(X, y, cat_features\u001b[39m=\u001b[39;49mcat_features, text_features\u001b[39m=\u001b[39;49mtext_features, embedding_features\u001b[39m=\u001b[39;49membedding_features, pairs\u001b[39m=\u001b[39;49mpairs, weight\u001b[39m=\u001b[39;49msample_weight, group_id\u001b[39m=\u001b[39;49mgroup_id,\n\u001b[0;32m   1439\u001b[0m                       group_weight\u001b[39m=\u001b[39;49mgroup_weight, subgroup_id\u001b[39m=\u001b[39;49msubgroup_id, pairs_weight\u001b[39m=\u001b[39;49mpairs_weight, baseline\u001b[39m=\u001b[39;49mbaseline)\n\u001b[0;32m   1440\u001b[0m \u001b[39mreturn\u001b[39;00m train_pool\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:792\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[0;32m    787\u001b[0m             \u001b[39mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m    788\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpython objects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    790\u001b[0m             )\n\u001b[1;32m--> 792\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n\u001b[0;32m    793\u001b[0m                    group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[0;32m    794\u001b[0m \u001b[39msuper\u001b[39m(Pool, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:1371\u001b[0m, in \u001b[0;36mPool._init\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(feature_names, features_count)\n\u001b[0;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m cat_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m     cat_features \u001b[39m=\u001b[39m _get_features_indices(cat_features, feature_names)\n\u001b[0;32m   1372\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_string_feature_type(cat_features, \u001b[39m'\u001b[39m\u001b[39mcat_features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_string_feature_value(cat_features, features_count, \u001b[39m'\u001b[39m\u001b[39mcat_features\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:269\u001b[0m, in \u001b[0;36m_get_features_indices\u001b[1;34m(features, feature_names)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mfeature names should be a sequence, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mrepr\u001b[39m(features))\n\u001b[0;32m    268\u001b[0m \u001b[39mif\u001b[39;00m feature_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    270\u001b[0m         feature_names\u001b[39m.\u001b[39mindex(f) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, STRING_TYPES) \u001b[39melse\u001b[39;00m f\n\u001b[0;32m    271\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features\n\u001b[0;32m    272\u001b[0m     ]\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:270\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mfeature names should be a sequence, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mrepr\u001b[39m(features))\n\u001b[0;32m    268\u001b[0m \u001b[39mif\u001b[39;00m feature_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 270\u001b[0m         feature_names\u001b[39m.\u001b[39;49mindex(f) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, STRING_TYPES) \u001b[39melse\u001b[39;00m f\n\u001b[0;32m    271\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features\n\u001b[0;32m    272\u001b[0m     ]\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features:\n",
            "\u001b[1;31mValueError\u001b[0m: 'ARI_CO' is not in list"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=1008)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "# 각 Fold에 대해서 Optuna로 하이퍼파라미터 튜닝\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index, :], train_y.iloc[val_index, :]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=313))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val.values), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best RMSE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = CatBoostRegressor(**best_params)\n",
        "    model.fit(X_train, y_train, cat_features=cat_features)\n",
        "    test_pred = model.predict(test)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "# 평균 앙상블\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "# 결과 출력\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")\n",
        "\n",
        "# 최종 예측 저장\n",
        "final_prediction_df = pd.DataFrame(final_prediction, columns=['MLM'])\n",
        "submission['CI_HOUR'] = final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>97.257813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>336.809458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>68.130348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>156.044942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>417.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>1051.383338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID      CI_HOUR\n",
              "0       TEST_000000    97.257813\n",
              "1       TEST_000001   336.809458\n",
              "2       TEST_000002     0.000000\n",
              "3       TEST_000003     0.000000\n",
              "4       TEST_000004    68.130348\n",
              "...             ...          ...\n",
              "244984  TEST_244984   156.044942\n",
              "244985  TEST_244985   417.542500\n",
              "244986  TEST_244986     0.000000\n",
              "244987  TEST_244987     0.000000\n",
              "244988  TEST_244988  1051.383338\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('CAT_v2_1008.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "#import xgboost as xgb\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "def objective(trial: Trial, X_train, y_train, X_val, y_val):\n",
        "    param = {\n",
        "        'objective': 'regression', # 회귀\n",
        "        'verbose': -1,\n",
        "        'metric': 'mae', \n",
        "        'max_depth': trial.suggest_int('max_depth',3, 15),\n",
        "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1e-2),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.4, 1),\n",
        "    }\n",
        "    model = lgb.LGBMRegressor(**param)\n",
        "    model.fit(X_train, y_train,\n",
        "            eval_set=[(X_train, y_train), (X_val,y_val)],\n",
        "            early_stopping_rounds=50,\n",
        "            verbose=100)\n",
        "    y_pred = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    return mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=3732)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "# 각 Fold에 대해서 Optuna로 하이퍼파라미터 튜닝\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index, :], train_y.iloc[val_index, :]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=1234))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val.values), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best MAE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = lgb.LGBMRegressor(**best_params)\n",
        "    model.fit(X_train, y_train)\n",
        "    test_pred = model.predict(test)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "# 평균 앙상블\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "# 결과 출력\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")\n",
        "\n",
        "# 최종 예측 저장\n",
        "final_prediction_df = pd.DataFrame(final_prediction, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bHexh3orVzUq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AlogP</th>\n",
              "      <th>Molecular_Weight</th>\n",
              "      <th>Num_H_Acceptors</th>\n",
              "      <th>Num_H_Donors</th>\n",
              "      <th>Num_RotatableBonds</th>\n",
              "      <th>LogD</th>\n",
              "      <th>Molecular_PolarSurfaceArea</th>\n",
              "      <th>nAcid</th>\n",
              "      <th>nBase</th>\n",
              "      <th>SpAbs_A</th>\n",
              "      <th>...</th>\n",
              "      <th>MACCS_key_158</th>\n",
              "      <th>MACCS_key_159</th>\n",
              "      <th>MACCS_key_160</th>\n",
              "      <th>MACCS_key_161</th>\n",
              "      <th>MACCS_key_162</th>\n",
              "      <th>MACCS_key_163</th>\n",
              "      <th>MACCS_key_164</th>\n",
              "      <th>MACCS_key_165</th>\n",
              "      <th>MLM</th>\n",
              "      <th>HLM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.259</td>\n",
              "      <td>400.495</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3.259</td>\n",
              "      <td>117.37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.689316</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26.010</td>\n",
              "      <td>50.680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.169</td>\n",
              "      <td>301.407</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.172</td>\n",
              "      <td>73.47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.575899</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>29.270</td>\n",
              "      <td>50.590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.593</td>\n",
              "      <td>297.358</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.585</td>\n",
              "      <td>62.45</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>29.802128</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.586</td>\n",
              "      <td>80.892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.771</td>\n",
              "      <td>494.652</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3.475</td>\n",
              "      <td>92.60</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>45.884166</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.710</td>\n",
              "      <td>2.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.335</td>\n",
              "      <td>268.310</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.337</td>\n",
              "      <td>42.43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.308663</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>93.270</td>\n",
              "      <td>99.990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>3.409</td>\n",
              "      <td>396.195</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3.409</td>\n",
              "      <td>64.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.902711</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.556</td>\n",
              "      <td>3.079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>1.912</td>\n",
              "      <td>359.381</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.844</td>\n",
              "      <td>77.37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.887372</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.560</td>\n",
              "      <td>47.630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3468</th>\n",
              "      <td>1.941</td>\n",
              "      <td>261.320</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2.124</td>\n",
              "      <td>70.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.546531</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>56.150</td>\n",
              "      <td>1.790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3469</th>\n",
              "      <td>0.989</td>\n",
              "      <td>284.696</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.989</td>\n",
              "      <td>91.51</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.936088</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030</td>\n",
              "      <td>2.770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3470</th>\n",
              "      <td>4.321</td>\n",
              "      <td>295.399</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4.321</td>\n",
              "      <td>50.36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27.713581</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.450</td>\n",
              "      <td>2.650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3471 rows × 5313 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n",
              "0     3.259           400.495                5             2   \n",
              "1     2.169           301.407                2             1   \n",
              "2     1.593           297.358                5             0   \n",
              "3     4.771           494.652                6             0   \n",
              "4     2.335           268.310                3             0   \n",
              "...     ...               ...              ...           ...   \n",
              "3466  3.409           396.195                3             1   \n",
              "3467  1.912           359.381                4             1   \n",
              "3468  1.941           261.320                3             1   \n",
              "3469  0.989           284.696                5             1   \n",
              "3470  4.321           295.399                2             0   \n",
              "\n",
              "      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  nAcid  nBase  \\\n",
              "0                      8  3.259                      117.37      0      0   \n",
              "1                      2  2.172                       73.47      0      0   \n",
              "2                      3  1.585                       62.45      2      1   \n",
              "3                      5  3.475                       92.60      0      1   \n",
              "4                      1  2.337                       42.43      0      0   \n",
              "...                  ...    ...                         ...    ...    ...   \n",
              "3466                   5  3.409                       64.74      0      0   \n",
              "3467                   3  1.844                       77.37      0      0   \n",
              "3468                   6  2.124                       70.14      0      0   \n",
              "3469                   5  0.989                       91.51      0      0   \n",
              "3470                   4  4.321                       50.36      0      0   \n",
              "\n",
              "        SpAbs_A  ...  MACCS_key_158  MACCS_key_159  MACCS_key_160  \\\n",
              "0     35.689316  ...              1              1              1   \n",
              "1     26.575899  ...              1              0              1   \n",
              "2     29.802128  ...              1              0              1   \n",
              "3     45.884166  ...              1              1              1   \n",
              "4     26.308663  ...              1              1              1   \n",
              "...         ...  ...            ...            ...            ...   \n",
              "3466  30.902711  ...              1              0              1   \n",
              "3467  35.887372  ...              1              1              1   \n",
              "3468  23.546531  ...              1              1              1   \n",
              "3469  23.936088  ...              1              1              0   \n",
              "3470  27.713581  ...              0              0              1   \n",
              "\n",
              "      MACCS_key_161  MACCS_key_162  MACCS_key_163  MACCS_key_164  \\\n",
              "0                 1              1              1              1   \n",
              "1                 1              1              1              1   \n",
              "2                 1              1              1              0   \n",
              "3                 1              1              1              1   \n",
              "4                 1              1              1              1   \n",
              "...             ...            ...            ...            ...   \n",
              "3466              1              1              0              1   \n",
              "3467              1              1              1              1   \n",
              "3468              1              1              1              1   \n",
              "3469              1              1              1              1   \n",
              "3470              1              1              1              1   \n",
              "\n",
              "      MACCS_key_165     MLM     HLM  \n",
              "0                 1  26.010  50.680  \n",
              "1                 1  29.270  50.590  \n",
              "2                 1   5.586  80.892  \n",
              "3                 1   5.710   2.000  \n",
              "4                 1  93.270  99.990  \n",
              "...             ...     ...     ...  \n",
              "3466              1   1.556   3.079  \n",
              "3467              1  35.560  47.630  \n",
              "3468              1  56.150   1.790  \n",
              "3469              1   0.030   2.770  \n",
              "3470              1   0.450   2.650  \n",
              "\n",
              "[3471 rows x 5313 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
