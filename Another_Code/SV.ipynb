{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kvY2HmDexFj3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets, ensemble\n",
        "from catboost import CatBoostRegressor\n",
        "from tqdm import tqdm\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "import itertools\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>LENGTH</th>\n",
              "      <th>SHIPMANAGER</th>\n",
              "      <th>FLAG</th>\n",
              "      <th>U_WIND</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>30.881018</td>\n",
              "      <td>2018-12-17 21:29</td>\n",
              "      <td>Z618338</td>\n",
              "      <td>30.0</td>\n",
              "      <td>24</td>\n",
              "      <td>24300</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>CQSB78</td>\n",
              "      <td>Panama</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>3.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>IN</td>\n",
              "      <td>UJM2</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2014-09-23 6:59</td>\n",
              "      <td>X886125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>13</td>\n",
              "      <td>35900</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>SPNO34</td>\n",
              "      <td>Marshall Islands</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-02-03 22:00</td>\n",
              "      <td>T674582</td>\n",
              "      <td>50.0</td>\n",
              "      <td>12</td>\n",
              "      <td>146000</td>\n",
              "      <td>...</td>\n",
              "      <td>370.0</td>\n",
              "      <td>FNPK22</td>\n",
              "      <td>Malta</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>ZAG4</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-01-17 4:02</td>\n",
              "      <td>Y847238</td>\n",
              "      <td>20.0</td>\n",
              "      <td>18</td>\n",
              "      <td>6910</td>\n",
              "      <td>...</td>\n",
              "      <td>120.0</td>\n",
              "      <td>PBZV77</td>\n",
              "      <td>Bahamas</td>\n",
              "      <td>-3.18</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.629350</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>27.037650</td>\n",
              "      <td>2020-01-26 7:51</td>\n",
              "      <td>A872328</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10</td>\n",
              "      <td>116000</td>\n",
              "      <td>...</td>\n",
              "      <td>300.0</td>\n",
              "      <td>GUCE76</td>\n",
              "      <td>Liberia</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>25.6</td>\n",
              "      <td>2.495953</td>\n",
              "      <td>15</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>253.554444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391934</th>\n",
              "      <td>TRAIN_391934</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2017-06-06 5:02</td>\n",
              "      <td>Y375615</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>6820</td>\n",
              "      <td>...</td>\n",
              "      <td>110.0</td>\n",
              "      <td>KEJZ24</td>\n",
              "      <td>China, People's Republic Of</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391935</th>\n",
              "      <td>TRAIN_391935</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>5.884603</td>\n",
              "      <td>2019-10-16 0:36</td>\n",
              "      <td>K635567</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12</td>\n",
              "      <td>3160</td>\n",
              "      <td>...</td>\n",
              "      <td>80.0</td>\n",
              "      <td>JLTM64</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>27.3</td>\n",
              "      <td>1.253491</td>\n",
              "      <td>8</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>144.061389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391936</th>\n",
              "      <td>TRAIN_391936</td>\n",
              "      <td>US</td>\n",
              "      <td>QGN3</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>70.660241</td>\n",
              "      <td>2021-03-23 22:35</td>\n",
              "      <td>J284147</td>\n",
              "      <td>30.0</td>\n",
              "      <td>8</td>\n",
              "      <td>60300</td>\n",
              "      <td>...</td>\n",
              "      <td>200.0</td>\n",
              "      <td>YERJ68</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>-3.44</td>\n",
              "      <td>7.99</td>\n",
              "      <td>21.1</td>\n",
              "      <td>4.766257</td>\n",
              "      <td>18</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>41.482222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391937</th>\n",
              "      <td>TRAIN_391937</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>9.448179</td>\n",
              "      <td>2015-01-08 7:15</td>\n",
              "      <td>J644215</td>\n",
              "      <td>30.0</td>\n",
              "      <td>29</td>\n",
              "      <td>23800</td>\n",
              "      <td>...</td>\n",
              "      <td>170.0</td>\n",
              "      <td>HCZK58</td>\n",
              "      <td>Comoros</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>7.485278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391938</th>\n",
              "      <td>TRAIN_391938</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-06-08 23:30</td>\n",
              "      <td>D123358</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>50600</td>\n",
              "      <td>...</td>\n",
              "      <td>260.0</td>\n",
              "      <td>GRJG55</td>\n",
              "      <td>Hong Kong, China</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>391939 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     SG   GIW5          Container  30.881018   \n",
              "1       TRAIN_000001     IN   UJM2               Bulk   0.000000   \n",
              "2       TRAIN_000002     CN   EUC8          Container   0.000000   \n",
              "3       TRAIN_000003     JP   ZAG4          Container   0.000000   \n",
              "4       TRAIN_000004     SG   GIW5          Container  27.037650   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "391934  TRAIN_391934     JP   QYY1          Container   0.000000   \n",
              "391935  TRAIN_391935     SG   GIW5               Bulk   5.884603   \n",
              "391936  TRAIN_391936     US   QGN3               Bulk  70.660241   \n",
              "391937  TRAIN_391937     TW   JWI3          Container   9.448179   \n",
              "391938  TRAIN_391938     TW   JWI3          Container   0.000000   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  LENGTH  \\\n",
              "0       2018-12-17 21:29  Z618338     30.0     24       24300  ...   180.0   \n",
              "1        2014-09-23 6:59  X886125     30.0     13       35900  ...   180.0   \n",
              "2       2015-02-03 22:00  T674582     50.0     12      146000  ...   370.0   \n",
              "3        2020-01-17 4:02  Y847238     20.0     18        6910  ...   120.0   \n",
              "4        2020-01-26 7:51  A872328     50.0     10      116000  ...   300.0   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "391934   2017-06-06 5:02  Y375615     20.0     27        6820  ...   110.0   \n",
              "391935   2019-10-16 0:36  K635567     10.0     12        3160  ...    80.0   \n",
              "391936  2021-03-23 22:35  J284147     30.0      8       60300  ...   200.0   \n",
              "391937   2015-01-08 7:15  J644215     30.0     29       23800  ...   170.0   \n",
              "391938  2015-06-08 23:30  D123358     30.0     15       50600  ...   260.0   \n",
              "\n",
              "        SHIPMANAGER                         FLAG  U_WIND V_WIND  \\\n",
              "0            CQSB78                       Panama     NaN    NaN   \n",
              "1            SPNO34             Marshall Islands     NaN    NaN   \n",
              "2            FNPK22                        Malta     NaN    NaN   \n",
              "3            PBZV77                      Bahamas   -3.18  -1.61   \n",
              "4            GUCE76                      Liberia   -0.33  -3.28   \n",
              "...             ...                          ...     ...    ...   \n",
              "391934       KEJZ24  China, People's Republic Of     NaN    NaN   \n",
              "391935       JLTM64                      Vietnam   -0.66   0.97   \n",
              "391936       YERJ68                    Singapore   -3.44   7.99   \n",
              "391937       HCZK58                      Comoros     NaN    NaN   \n",
              "391938       GRJG55             Hong Kong, China     NaN    NaN   \n",
              "\n",
              "       AIR_TEMPERATURE        BN  ATA_LT  PORT_SIZE     CI_HOUR  \n",
              "0                  NaN       NaN       5   0.002615    3.450000  \n",
              "1                  NaN       NaN      12   0.000217    0.000000  \n",
              "2                  NaN       NaN       6   0.001614    0.000000  \n",
              "3                  6.7  2.629350      13   0.000356    0.000000  \n",
              "4                 25.6  2.495953      15   0.002615  253.554444  \n",
              "...                ...       ...     ...        ...         ...  \n",
              "391934             NaN       NaN      14   0.000552    0.000000  \n",
              "391935            27.3  1.253491       8   0.002615  144.061389  \n",
              "391936            21.1  4.766257      18   0.000155   41.482222  \n",
              "391937             NaN       NaN      15   0.000990    7.485278  \n",
              "391938             NaN       NaN       7   0.000990    0.000000  \n",
              "\n",
              "[391939 rows x 23 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>LENGTH</th>\n",
              "      <th>SHIPMANAGER</th>\n",
              "      <th>FLAG</th>\n",
              "      <th>U_WIND</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>30.881018</td>\n",
              "      <td>2018-12-17 21:29</td>\n",
              "      <td>Z618338</td>\n",
              "      <td>30.0</td>\n",
              "      <td>24</td>\n",
              "      <td>24300</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>CQSB78</td>\n",
              "      <td>Panama</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>3.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>IN</td>\n",
              "      <td>UJM2</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2014-09-23 6:59</td>\n",
              "      <td>X886125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>13</td>\n",
              "      <td>35900</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>SPNO34</td>\n",
              "      <td>Marshall Islands</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-02-03 22:00</td>\n",
              "      <td>T674582</td>\n",
              "      <td>50.0</td>\n",
              "      <td>12</td>\n",
              "      <td>146000</td>\n",
              "      <td>...</td>\n",
              "      <td>370.0</td>\n",
              "      <td>FNPK22</td>\n",
              "      <td>Malta</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>ZAG4</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-01-17 4:02</td>\n",
              "      <td>Y847238</td>\n",
              "      <td>20.0</td>\n",
              "      <td>18</td>\n",
              "      <td>6910</td>\n",
              "      <td>...</td>\n",
              "      <td>120.0</td>\n",
              "      <td>PBZV77</td>\n",
              "      <td>Bahamas</td>\n",
              "      <td>-3.18</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.629350</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>27.037650</td>\n",
              "      <td>2020-01-26 7:51</td>\n",
              "      <td>A872328</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10</td>\n",
              "      <td>116000</td>\n",
              "      <td>...</td>\n",
              "      <td>300.0</td>\n",
              "      <td>GUCE76</td>\n",
              "      <td>Liberia</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>25.6</td>\n",
              "      <td>2.495953</td>\n",
              "      <td>15</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>253.554444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391934</th>\n",
              "      <td>TRAIN_391934</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2017-06-06 5:02</td>\n",
              "      <td>Y375615</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>6820</td>\n",
              "      <td>...</td>\n",
              "      <td>110.0</td>\n",
              "      <td>KEJZ24</td>\n",
              "      <td>China, People's Republic Of</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391935</th>\n",
              "      <td>TRAIN_391935</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>5.884603</td>\n",
              "      <td>2019-10-16 0:36</td>\n",
              "      <td>K635567</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12</td>\n",
              "      <td>3160</td>\n",
              "      <td>...</td>\n",
              "      <td>80.0</td>\n",
              "      <td>JLTM64</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>27.3</td>\n",
              "      <td>1.253491</td>\n",
              "      <td>8</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>144.061389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391936</th>\n",
              "      <td>TRAIN_391936</td>\n",
              "      <td>US</td>\n",
              "      <td>QGN3</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>70.660241</td>\n",
              "      <td>2021-03-23 22:35</td>\n",
              "      <td>J284147</td>\n",
              "      <td>30.0</td>\n",
              "      <td>8</td>\n",
              "      <td>60300</td>\n",
              "      <td>...</td>\n",
              "      <td>200.0</td>\n",
              "      <td>YERJ68</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>-3.44</td>\n",
              "      <td>7.99</td>\n",
              "      <td>21.1</td>\n",
              "      <td>4.766257</td>\n",
              "      <td>18</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>41.482222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391937</th>\n",
              "      <td>TRAIN_391937</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>9.448179</td>\n",
              "      <td>2015-01-08 7:15</td>\n",
              "      <td>J644215</td>\n",
              "      <td>30.0</td>\n",
              "      <td>29</td>\n",
              "      <td>23800</td>\n",
              "      <td>...</td>\n",
              "      <td>170.0</td>\n",
              "      <td>HCZK58</td>\n",
              "      <td>Comoros</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>7.485278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391938</th>\n",
              "      <td>TRAIN_391938</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-06-08 23:30</td>\n",
              "      <td>D123358</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>50600</td>\n",
              "      <td>...</td>\n",
              "      <td>260.0</td>\n",
              "      <td>GRJG55</td>\n",
              "      <td>Hong Kong, China</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>391939 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     SG   GIW5          Container  30.881018   \n",
              "1       TRAIN_000001     IN   UJM2               Bulk   0.000000   \n",
              "2       TRAIN_000002     CN   EUC8          Container   0.000000   \n",
              "3       TRAIN_000003     JP   ZAG4          Container   0.000000   \n",
              "4       TRAIN_000004     SG   GIW5          Container  27.037650   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "391934  TRAIN_391934     JP   QYY1          Container   0.000000   \n",
              "391935  TRAIN_391935     SG   GIW5               Bulk   5.884603   \n",
              "391936  TRAIN_391936     US   QGN3               Bulk  70.660241   \n",
              "391937  TRAIN_391937     TW   JWI3          Container   9.448179   \n",
              "391938  TRAIN_391938     TW   JWI3          Container   0.000000   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  LENGTH  \\\n",
              "0       2018-12-17 21:29  Z618338     30.0     24       24300  ...   180.0   \n",
              "1        2014-09-23 6:59  X886125     30.0     13       35900  ...   180.0   \n",
              "2       2015-02-03 22:00  T674582     50.0     12      146000  ...   370.0   \n",
              "3        2020-01-17 4:02  Y847238     20.0     18        6910  ...   120.0   \n",
              "4        2020-01-26 7:51  A872328     50.0     10      116000  ...   300.0   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "391934   2017-06-06 5:02  Y375615     20.0     27        6820  ...   110.0   \n",
              "391935   2019-10-16 0:36  K635567     10.0     12        3160  ...    80.0   \n",
              "391936  2021-03-23 22:35  J284147     30.0      8       60300  ...   200.0   \n",
              "391937   2015-01-08 7:15  J644215     30.0     29       23800  ...   170.0   \n",
              "391938  2015-06-08 23:30  D123358     30.0     15       50600  ...   260.0   \n",
              "\n",
              "        SHIPMANAGER                         FLAG  U_WIND V_WIND  \\\n",
              "0            CQSB78                       Panama     NaN    NaN   \n",
              "1            SPNO34             Marshall Islands     NaN    NaN   \n",
              "2            FNPK22                        Malta     NaN    NaN   \n",
              "3            PBZV77                      Bahamas   -3.18  -1.61   \n",
              "4            GUCE76                      Liberia   -0.33  -3.28   \n",
              "...             ...                          ...     ...    ...   \n",
              "391934       KEJZ24  China, People's Republic Of     NaN    NaN   \n",
              "391935       JLTM64                      Vietnam   -0.66   0.97   \n",
              "391936       YERJ68                    Singapore   -3.44   7.99   \n",
              "391937       HCZK58                      Comoros     NaN    NaN   \n",
              "391938       GRJG55             Hong Kong, China     NaN    NaN   \n",
              "\n",
              "       AIR_TEMPERATURE        BN  ATA_LT  PORT_SIZE     CI_HOUR  \n",
              "0                  NaN       NaN       5   0.002615    3.450000  \n",
              "1                  NaN       NaN      12   0.000217    0.000000  \n",
              "2                  NaN       NaN       6   0.001614    0.000000  \n",
              "3                  6.7  2.629350      13   0.000356    0.000000  \n",
              "4                 25.6  2.495953      15   0.002615  253.554444  \n",
              "...                ...       ...     ...        ...         ...  \n",
              "391934             NaN       NaN      14   0.000552    0.000000  \n",
              "391935            27.3  1.253491       8   0.002615  144.061389  \n",
              "391936            21.1  4.766257      18   0.000155   41.482222  \n",
              "391937             NaN       NaN      15   0.000990    7.485278  \n",
              "391938             NaN       NaN       7   0.000990    0.000000  \n",
              "\n",
              "[391939 rows x 23 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#train = train[train['DIST'] != 0]\n",
        "#train = train.reset_index(drop = True)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['SAMPLE_ID'],axis=1,inplace=True)\n",
        "test.drop(['SAMPLE_ID'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "      <th>rounded_hour</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018</td>\n",
              "      <td>10</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  month  day  weekday  rounded_hour\n",
              "0  2020      6   18        3            12\n",
              "1  2021      5   26        2            22\n",
              "2  2019     12   16        0             0\n",
              "3  2015     11   16        0             6\n",
              "4  2018     10   24        2             1"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['ATA'] = pd.to_datetime(train['ATA'])\n",
        "test['ATA'] = pd.to_datetime(test['ATA'])\n",
        "train['year'] = train['ATA'].dt.year\n",
        "train['month'] = train['ATA'].dt.month\n",
        "train['day'] = train['ATA'].dt.day\n",
        "train['weekday'] = train['ATA'].dt.dayofweek\n",
        "train['rounded_hour'] = (train['ATA'].dt.hour + (train['ATA'].dt.minute // 30)).apply(lambda x: 0 if x == 24 else x)\n",
        "test['year'] = test['ATA'].dt.year\n",
        "test['month'] = test['ATA'].dt.month\n",
        "test['day'] = test['ATA'].dt.day\n",
        "test['weekday'] = test['ATA'].dt.dayofweek\n",
        "test['rounded_hour'] = (test['ATA'].dt.hour + (test['ATA'].dt.minute // 30)).apply(lambda x: 0 if x == 24 else x)\n",
        "\n",
        "# sin, cos 변환 함수 정의\n",
        "def encode_cyclic_feature(data, column, max_val):\n",
        "    data[column + '_sin'] = np.sin(2 * np.pi * data[column] / max_val)\n",
        "    data[column + '_cos'] = np.cos(2 * np.pi * data[column] / max_val)\n",
        "    return data\n",
        "\n",
        "# 각 피처에 대해 sin, cos 변환 수행\n",
        "train = encode_cyclic_feature(train, 'month', 12)\n",
        "train = encode_cyclic_feature(train, 'day', 31)\n",
        "train = encode_cyclic_feature(train, 'weekday', 7)\n",
        "train = encode_cyclic_feature(train, 'rounded_hour', 24)\n",
        "test = encode_cyclic_feature(test, 'month', 12)\n",
        "test = encode_cyclic_feature(test, 'day', 31)\n",
        "test = encode_cyclic_feature(test, 'weekday', 7)\n",
        "test = encode_cyclic_feature(test, 'rounded_hour', 24)\n",
        "\n",
        "train.drop(['ATA'],axis=1,inplace=True)\n",
        "test.drop(['ATA'],axis=1,inplace=True)\n",
        "\n",
        "test[['year', 'month', 'day', 'weekday', 'rounded_hour']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding features: 100%|██████████| 6/6 [00:12<00:00,  2.12s/it]\n"
          ]
        }
      ],
      "source": [
        "# Categorical 컬럼 인코딩\n",
        "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "\n",
        "\n",
        "for feature in tqdm(categorical_features, desc=\"Encoding features\"):\n",
        "    encoder = LabelEncoder()\n",
        "    train[feature] = encoder.fit_transform(train[feature])\n",
        "    for label in np.unique(test[feature]):\n",
        "        if label not in encoder.classes_:\n",
        "            encoder.classes_ = np.append(encoder.classes_, label)\n",
        "    test[feature] = encoder.transform(test[feature])\n",
        "\n",
        "# 결측치 처리\n",
        "train.fillna(train.mean(), inplace=True)\n",
        "test.fillna(train.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x = train.drop(['CI_HOUR'],axis=1)\n",
        "train_y = train[['CI_HOUR']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# ARI_CO + weekday 결합 열 생성\\ntrain[\\'fold_column\\'] = train[\\'ARI_CO\\'].astype(str) + \"_\" + train[\\'weekday\\'].astype(str)\\n\\n# Stratified 5-fold 생성\\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=313)\\n\\n# 각 폴드마다의 예측값과 MAE를 저장할 리스트\\npredictions = []\\nmae_scores = []\\n\\n# StratifiedKFold는 레이블 분포를 기반으로 하기 때문에, 너무 적은 빈도의 조합은 오류를 발생시킬 수 있습니다.\\nmin_samples = train[\\'fold_column\\'].value_counts().min()\\nfiltered_train = train[train[\\'fold_column\\'].map(train[\\'fold_column\\'].value_counts()) > min_samples]\\n\\nfor train_idx, val_idx in skf.split(filtered_train, filtered_train[\\'fold_column\\']):\\n    train_fold = filtered_train.iloc[train_idx].drop(columns=[\\'fold_column\\'])\\n    val_fold = filtered_train.iloc[val_idx].drop(columns=[\\'fold_column\\'])\\n    train_data = TabularDataset(train_fold)\\n    val_data = TabularDataset(val_fold)\\n    test_data = TabularDataset(test)\\n    \\n    # 각 폴드마다 모델 학습\\n    predictor = TabularPredictor(label=\\'CI_HOUR\\', eval_metric=\\'mean_absolute_error\\').fit(\\n        train_data, \\n        presets=\\'medium_quality\\',\\n        ag_args_fit={\\'num_gpus\\': 0},\\n        included_model_types=[\\'CAT\\']\\n    )\\n    \\n    # validation set에 대한 예측값 계산\\n    val_predictions = predictor.predict(val_data.drop(columns=\\'CI_HOUR\\'))\\n    \\n    # MAE 계산 후 저장\\n    mae = mean_absolute_error(val_data[\\'CI_HOUR\\'], val_predictions)\\n    mae_scores.append(mae)\\n    \\n    # test set에 대한 예측값 저장\\n    predictions.append(predictor.predict(test_data))\\n\\n# 예측값의 평균 계산\\nfinal_predictions = np.mean(predictions, axis=0)\\ny_pred = pd.DataFrame(final_predictions, columns=[\\'CI_HOUR\\'])\\nsubmission[\\'CI_HOUR\\'] = y_pred[\\'CI_HOUR\\']\\nmae_scores, np.mean(mae_scores)\\n'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "'''\n",
        "# ARI_CO + weekday 결합 열 생성\n",
        "train['fold_column'] = train['ARI_CO'].astype(str) + \"_\" + train['weekday'].astype(str)\n",
        "\n",
        "# Stratified 5-fold 생성\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=313)\n",
        "\n",
        "# 각 폴드마다의 예측값과 MAE를 저장할 리스트\n",
        "predictions = []\n",
        "mae_scores = []\n",
        "\n",
        "# StratifiedKFold는 레이블 분포를 기반으로 하기 때문에, 너무 적은 빈도의 조합은 오류를 발생시킬 수 있습니다.\n",
        "min_samples = train['fold_column'].value_counts().min()\n",
        "filtered_train = train[train['fold_column'].map(train['fold_column'].value_counts()) > min_samples]\n",
        "\n",
        "for train_idx, val_idx in skf.split(filtered_train, filtered_train['fold_column']):\n",
        "    train_fold = filtered_train.iloc[train_idx].drop(columns=['fold_column'])\n",
        "    val_fold = filtered_train.iloc[val_idx].drop(columns=['fold_column'])\n",
        "    train_data = TabularDataset(train_fold)\n",
        "    val_data = TabularDataset(val_fold)\n",
        "    test_data = TabularDataset(test)\n",
        "    \n",
        "    # 각 폴드마다 모델 학습\n",
        "    predictor = TabularPredictor(label='CI_HOUR', eval_metric='mean_absolute_error').fit(\n",
        "        train_data, \n",
        "        presets='medium_quality',\n",
        "        ag_args_fit={'num_gpus': 0},\n",
        "        included_model_types=['CAT']\n",
        "    )\n",
        "    \n",
        "    # validation set에 대한 예측값 계산\n",
        "    val_predictions = predictor.predict(val_data.drop(columns='CI_HOUR'))\n",
        "    \n",
        "    # MAE 계산 후 저장\n",
        "    mae = mean_absolute_error(val_data['CI_HOUR'], val_predictions)\n",
        "    mae_scores.append(mae)\n",
        "    \n",
        "    # test set에 대한 예측값 저장\n",
        "    predictions.append(predictor.predict(test_data))\n",
        "\n",
        "# 예측값의 평균 계산\n",
        "final_predictions = np.mean(predictions, axis=0)\n",
        "y_pred = pd.DataFrame(final_predictions, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred['CI_HOUR']\n",
        "mae_scores, np.mean(mae_scores)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_031040\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_031040\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.96 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    41214\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2158.901944, 0.0, 108.52434, 288.64109)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21710.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 36.84 MB (0.2% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GIW5\n",
            "GIW5\n",
            "GIW5\n",
            "GIW5\n",
            "GIW5\n",
            "41214\n",
            "41214\n",
            "41214\n",
            "41214\n",
            "41214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.4s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 11.87 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.45s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.060658999369146406, Train Rows: 38714, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-60.2822\t = Validation score   (-mean_absolute_error)\n",
            "\t670.36s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-60.2822\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 671.08s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_031040\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_032152\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_032152\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.89 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    27243\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2116.295556, 0.0, 42.78751, 123.31688)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21823.32 MB\n",
            "\tTrain Data (Original)  Memory Usage: 24.44 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NGG6\n",
            "NGG6\n",
            "NGG6\n",
            "NGG6\n",
            "NGG6\n",
            "27243\n",
            "27243\n",
            "27243\n",
            "27243\n",
            "27243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.4s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 7.85 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.49s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.09176669236134052, Train Rows: 24743, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-31.9993\t = Validation score   (-mean_absolute_error)\n",
            "\t163.88s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-31.9993\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 164.5s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_032152\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_032437\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_032437\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.87 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    26070\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2148.446389, 0.0, 34.94739, 101.04002)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21814.07 MB\n",
            "\tTrain Data (Original)  Memory Usage: 23.36 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EKP8\n",
            "EKP8\n",
            "EKP8\n",
            "EKP8\n",
            "EKP8\n",
            "26070\n",
            "26070\n",
            "26070\n",
            "26070\n",
            "26070\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 4): ['ARI_PO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 3): ['mean_enc_ARI_CO', 'std_enc_ARI_CO', 'PORT_SIZE_Zone']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('float', [])  : 2 | ['mean_enc_ARI_CO', 'std_enc_ARI_CO']\n",
            "\t\t('object', []) : 1 | ['PORT_SIZE_Zone']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 29 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  8 | ['ARI_CO', 'SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['ARI_CO', 'PORT_SIZE']\n",
            "\t0.6s = Fit runtime\n",
            "\t46 features in original data used to generate 46 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 7.56 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.69s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.09589566551591867, Train Rows: 23570, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-26.9454\t = Validation score   (-mean_absolute_error)\n",
            "\t573.55s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-26.9454\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 574.5s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_032437\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033411\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033411\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.84 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    22995\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2126.3375, 0.0, 23.12988, 104.99103)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22032.46 MB\n",
            "\tTrain Data (Original)  Memory Usage: 20.58 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JWI3\n",
            "JWI3\n",
            "JWI3\n",
            "JWI3\n",
            "JWI3\n",
            "22995\n",
            "22995\n",
            "22995\n",
            "22995\n",
            "22995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.4s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 6.63 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.43s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 20695, Val Rows: 2300\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-23.4259\t = Validation score   (-mean_absolute_error)\n",
            "\t20.1s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-23.4259\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 20.64s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033411\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033432\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033432\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.84 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    21263\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2157.861389, 0.0, 55.46718, 172.33473)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22027.17 MB\n",
            "\tTrain Data (Original)  Memory Usage: 19.07 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EUC8\n",
            "EUC8\n",
            "EUC8\n",
            "EUC8\n",
            "EUC8\n",
            "21263\n",
            "21263\n",
            "21263\n",
            "21263\n",
            "21263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.3s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 6.13 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.4s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 19136, Val Rows: 2127\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-45.4334\t = Validation score   (-mean_absolute_error)\n",
            "\t13.61s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-45.4334\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033432\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033447\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033447\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.83 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    15803\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2105.045, 0.0, 19.3953, 95.21709)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22042.27 MB\n",
            "\tTrain Data (Original)  Memory Usage: 14.12 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QYY1\n",
            "QYY1\n",
            "QYY1\n",
            "QYY1\n",
            "QYY1\n",
            "15803\n",
            "15803\n",
            "15803\n",
            "15803\n",
            "15803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.3s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 4.55 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.35s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 14222, Val Rows: 1581\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-16.1147\t = Validation score   (-mean_absolute_error)\n",
            "\t12.08s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-16.1147\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.52s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033447\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033459\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033459\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.82 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    10253\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
            "\tLabel info (max, min, mean, stddev): (1362.646111, 0.0, 1.38527, 25.97867)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22006.53 MB\n",
            "\tTrain Data (Original)  Memory Usage: 9.19 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NCU8\n",
            "NCU8\n",
            "NCU8\n",
            "NCU8\n",
            "NCU8\n",
            "10253\n",
            "10253\n",
            "10253\n",
            "10253\n",
            "10253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.3s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.96 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.3s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9227, Val Rows: 1026\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-1.7321\t = Validation score   (-mean_absolute_error)\n",
            "\t12.78s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-1.7321\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.19s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033459\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033513\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033513\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.82 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    9408\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1159.246111, 0.0, 90.14209, 91.9508)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21990.39 MB\n",
            "\tTrain Data (Original)  Memory Usage: 8.37 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WHH4\n",
            "WHH4\n",
            "WHH4\n",
            "WHH4\n",
            "WHH4\n",
            "9408\n",
            "9408\n",
            "9408\n",
            "9408\n",
            "9408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.71 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.28s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8467, Val Rows: 941\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-53.2302\t = Validation score   (-mean_absolute_error)\n",
            "\t90.25s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-53.2302\t = Validation score   (-mean_absolute_error)\n",
            "\t0.0s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 90.63s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033513\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033643\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033643\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.81 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    7992\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1956.061111, 0.0, 34.19422, 110.19482)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22036.85 MB\n",
            "\tTrain Data (Original)  Memory Usage: 7.17 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WEY7\n",
            "WEY7\n",
            "WEY7\n",
            "WEY7\n",
            "WEY7\n",
            "7992\n",
            "7992\n",
            "7992\n",
            "7992\n",
            "7992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.31 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.27s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7192, Val Rows: 800\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-27.0674\t = Validation score   (-mean_absolute_error)\n",
            "\t22.29s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-27.0674\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 22.67s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033643\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033706\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033706\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.80 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    7959\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2082.405, 0.0, 59.47002, 119.03786)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    22020.12 MB\n",
            "\tTrain Data (Original)  Memory Usage: 7.15 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JEN5\n",
            "JEN5\n",
            "JEN5\n",
            "JEN5\n",
            "JEN5\n",
            "7959\n",
            "7959\n",
            "7959\n",
            "7959\n",
            "7959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.3 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.27s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 7163, Val Rows: 796\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-49.1888\t = Validation score   (-mean_absolute_error)\n",
            "\t26.53s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-49.1888\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 26.89s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033706\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033733\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033733\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.80 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    7568\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2102.433333, 0.0, 46.34137, 153.57297)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21997.66 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.73 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ZAG4\n",
            "ZAG4\n",
            "ZAG4\n",
            "ZAG4\n",
            "ZAG4\n",
            "7568\n",
            "7568\n",
            "7568\n",
            "7568\n",
            "7568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.18 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.27s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6811, Val Rows: 757\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-42.8422\t = Validation score   (-mean_absolute_error)\n",
            "\t12.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-42.8422\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.99s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033733\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033746\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033746\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.80 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    7056\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2129.140833, 0.0, 66.71269, 121.37896)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21985.39 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.34 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JTD1\n",
            "JTD1\n",
            "JTD1\n",
            "JTD1\n",
            "JTD1\n",
            "7056\n",
            "7056\n",
            "7056\n",
            "7056\n",
            "7056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.04 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.26s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6350, Val Rows: 706\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-46.6242\t = Validation score   (-mean_absolute_error)\n",
            "\t19.61s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-46.6242\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.96s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033746\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033807\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033807\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.79 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    6791\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1982.005278, 0.0, 42.67987, 119.85316)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21965.36 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QQW1\n",
            "QQW1\n",
            "QQW1\n",
            "QQW1\n",
            "QQW1\n",
            "6791\n",
            "6791\n",
            "6791\n",
            "6791\n",
            "6791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.96 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.26s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6111, Val Rows: 680\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-34.4332\t = Validation score   (-mean_absolute_error)\n",
            "\t12.24s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-34.4332\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.6s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033807\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033819\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033819\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.79 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    6695\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2132.006111, 0.0, 95.34582, 139.73533)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21959.66 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.05 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YRT6\n",
            "YRT6\n",
            "YRT6\n",
            "YRT6\n",
            "YRT6\n",
            "6695\n",
            "6695\n",
            "6695\n",
            "6695\n",
            "6695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.93 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.25s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 6025, Val Rows: 670\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-54.1797\t = Validation score   (-mean_absolute_error)\n",
            "\t67.87s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-54.1797\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 68.23s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033819\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_033928\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_033928\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.78 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    6146\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2136.308889, 0.0, 63.59897, 124.29261)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21968.78 MB\n",
            "\tTrain Data (Original)  Memory Usage: 5.5 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TDA5\n",
            "TDA5\n",
            "TDA5\n",
            "TDA5\n",
            "TDA5\n",
            "6146\n",
            "6146\n",
            "6146\n",
            "6146\n",
            "6146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.77 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.26s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 5531, Val Rows: 615\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-45.9086\t = Validation score   (-mean_absolute_error)\n",
            "\t49.68s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-45.9086\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 50.05s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_033928\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034018\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034018\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.78 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    5311\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2113.031111, 0.0, 47.52771, 144.75331)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21966.95 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.72 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MIA8\n",
            "MIA8\n",
            "MIA8\n",
            "MIA8\n",
            "MIA8\n",
            "5311\n",
            "5311\n",
            "5311\n",
            "5311\n",
            "5311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.53 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 4779, Val Rows: 532\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-42.2842\t = Validation score   (-mean_absolute_error)\n",
            "\t17.34s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-42.2842\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.67s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034018\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034036\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034036\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.78 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4913\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1360.705833, 0.0, 42.80297, 84.08974)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21940.64 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.37 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIB7\n",
            "AIB7\n",
            "AIB7\n",
            "AIB7\n",
            "AIB7\n",
            "4913\n",
            "4913\n",
            "4913\n",
            "4913\n",
            "4913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.42 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.24s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1017708121310808, Train Rows: 4413, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-28.9496\t = Validation score   (-mean_absolute_error)\n",
            "\t12.67s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-28.9496\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.01s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034036\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034049\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034049\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.77 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4787\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2109.445, 0.0, 81.79908, 143.94335)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21898.8 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.27 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFM2\n",
            "FFM2\n",
            "FFM2\n",
            "FFM2\n",
            "FFM2\n",
            "4787\n",
            "4787\n",
            "4787\n",
            "4787\n",
            "4787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.38 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.10444955086693128, Train Rows: 4286, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-60.114\t = Validation score   (-mean_absolute_error)\n",
            "\t11.29s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-60.114\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.6s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034049\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034101\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034101\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.77 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4694\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2083.849722, 0.0, 36.2394, 112.59804)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21925.23 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.17 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HYG5\n",
            "HYG5\n",
            "HYG5\n",
            "HYG5\n",
            "HYG5\n",
            "4694\n",
            "4694\n",
            "4694\n",
            "4694\n",
            "4694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.36 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.10651896037494674, Train Rows: 4194, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-35.1917\t = Validation score   (-mean_absolute_error)\n",
            "\t14.12s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-35.1917\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.42s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034101\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034115\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034115\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.77 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4138\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2073.600556, 0.0, 35.07902, 136.91397)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21926.55 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.69 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BGD2\n",
            "BGD2\n",
            "BGD2\n",
            "BGD2\n",
            "BGD2\n",
            "4138\n",
            "4138\n",
            "4138\n",
            "4138\n",
            "4138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.2 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1208313194780087, Train Rows: 3638, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-43.2944\t = Validation score   (-mean_absolute_error)\n",
            "\t22.88s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-43.2944\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 23.36s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034115\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034139\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034139\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.77 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    4010\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2150.8, 0.0, 122.56764, 168.50125)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21949.75 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.57 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NQO4\n",
            "NQO4\n",
            "NQO4\n",
            "NQO4\n",
            "NQO4\n",
            "4010\n",
            "4010\n",
            "4010\n",
            "4010\n",
            "4010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.16 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.12468827930174564, Train Rows: 3510, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-92.9599\t = Validation score   (-mean_absolute_error)\n",
            "\t16.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-92.9599\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 16.82s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034139\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034156\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034156\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.76 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3885\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2147.478056, 0.0, 129.74604, 299.71952)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21907.91 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.46 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FCD5\n",
            "FCD5\n",
            "FCD5\n",
            "FCD5\n",
            "FCD5\n",
            "3885\n",
            "3885\n",
            "3885\n",
            "3885\n",
            "3885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.12 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1287001287001287, Train Rows: 3385, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-119.0399\t = Validation score   (-mean_absolute_error)\n",
            "\t224.55s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-119.0399\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 224.9s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034156\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034541\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034541\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.75 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3805\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2102.570833, 0.0, 39.37323, 170.90898)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21985.82 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.4 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JER7\n",
            "JER7\n",
            "JER7\n",
            "JER7\n",
            "JER7\n",
            "3805\n",
            "3805\n",
            "3805\n",
            "3805\n",
            "3805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.1 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1314060446780552, Train Rows: 3305, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-43.0196\t = Validation score   (-mean_absolute_error)\n",
            "\t11.08s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-43.0196\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.39s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034541\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034552\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034552\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.75 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3554\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2134.278333, 0.0, 84.48681, 208.12173)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21949.59 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LXJ7\n",
            "LXJ7\n",
            "LXJ7\n",
            "LXJ7\n",
            "LXJ7\n",
            "3554\n",
            "3554\n",
            "3554\n",
            "3554\n",
            "3554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.14068655036578503, Train Rows: 3054, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-77.0219\t = Validation score   (-mean_absolute_error)\n",
            "\t12.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-77.0219\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.72s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034552\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034605\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034605\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.75 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3520\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1871.841667, 0.0, 221.06208, 218.10843)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21985.74 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.13 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KSF1\n",
            "KSF1\n",
            "KSF1\n",
            "KSF1\n",
            "KSF1\n",
            "3520\n",
            "3520\n",
            "3520\n",
            "3520\n",
            "3520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.02 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.14204545454545456, Train Rows: 3019, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-136.9769\t = Validation score   (-mean_absolute_error)\n",
            "\t17.83s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-136.9769\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 18.12s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034605\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034623\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034623\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.75 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3515\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2078.504722, 0.0, 178.71886, 223.27654)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21972.23 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.13 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TMW2\n",
            "TMW2\n",
            "TMW2\n",
            "TMW2\n",
            "TMW2\n",
            "3515\n",
            "3515\n",
            "3515\n",
            "3515\n",
            "3515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.02 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1422475106685633, Train Rows: 3014, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-108.8703\t = Validation score   (-mean_absolute_error)\n",
            "\t114.91s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-108.8703\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 115.25s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034623\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034819\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034819\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.74 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3349\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1568.539167, 0.0, 44.52857, 96.45209)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21979.17 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.0 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XVZ3\n",
            "XVZ3\n",
            "XVZ3\n",
            "XVZ3\n",
            "XVZ3\n",
            "3349\n",
            "3349\n",
            "3349\n",
            "3349\n",
            "3349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.97 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1492982979994028, Train Rows: 2849, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-39.1893\t = Validation score   (-mean_absolute_error)\n",
            "\t17.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-39.1893\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.95s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034819\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034837\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034837\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.74 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3342\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1121.085, 0.0, 90.40821, 95.40416)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21942.77 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.97 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YDP4\n",
            "YDP4\n",
            "YDP4\n",
            "YDP4\n",
            "YDP4\n",
            "3342\n",
            "3342\n",
            "3342\n",
            "3342\n",
            "3342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.97 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.14961101137043686, Train Rows: 2842, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-52.9934\t = Validation score   (-mean_absolute_error)\n",
            "\t25.56s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-52.9934\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 25.87s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034837\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034903\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034903\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.74 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3137\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2039.986944, 0.0, 190.8984, 242.32361)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21919.81 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.79 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VFD8\n",
            "VFD8\n",
            "VFD8\n",
            "VFD8\n",
            "VFD8\n",
            "3137\n",
            "3137\n",
            "3137\n",
            "3137\n",
            "3137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.91 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1593879502709595, Train Rows: 2637, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-136.9176\t = Validation score   (-mean_absolute_error)\n",
            "\t54.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-136.9176\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 54.94s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034903\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_034958\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_034958\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.73 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3064\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1862.48, 0.0, 69.99151, 140.95876)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21835.64 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.73 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QTU5\n",
            "QTU5\n",
            "QTU5\n",
            "QTU5\n",
            "QTU5\n",
            "3064\n",
            "3064\n",
            "3064\n",
            "3064\n",
            "3064\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.89 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.16318537859007834, Train Rows: 2563, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-56.8586\t = Validation score   (-mean_absolute_error)\n",
            "\t13.66s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-56.8586\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.96s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_034958\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035012\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035012\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.73 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    3032\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1943.883611, 0.0, 63.90981, 153.28187)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21839.75 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.7 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UJM2\n",
            "UJM2\n",
            "UJM2\n",
            "UJM2\n",
            "UJM2\n",
            "3032\n",
            "3032\n",
            "3032\n",
            "3032\n",
            "3032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.88 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.16490765171503957, Train Rows: 2532, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-54.0115\t = Validation score   (-mean_absolute_error)\n",
            "\t19.5s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-54.0115\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.8s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035012\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035032\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035032\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.73 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2950\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2074.1025, 0.0, 66.00011, 84.48183)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21839.47 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.62 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WXQ2\n",
            "WXQ2\n",
            "WXQ2\n",
            "WXQ2\n",
            "WXQ2\n",
            "2950\n",
            "2950\n",
            "2950\n",
            "2950\n",
            "2950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.85 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1694915254237288, Train Rows: 2450, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-39.6865\t = Validation score   (-mean_absolute_error)\n",
            "\t14.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-39.6865\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.01s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035032\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035047\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035047\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2887\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2151.602778, 0.0, 48.69182, 153.9367)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21810.06 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TMR7\n",
            "TMR7\n",
            "TMR7\n",
            "TMR7\n",
            "TMR7\n",
            "2887\n",
            "2887\n",
            "2887\n",
            "2887\n",
            "2887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.83 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.17319016279875304, Train Rows: 2386, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-49.7143\t = Validation score   (-mean_absolute_error)\n",
            "\t10.46s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-49.7143\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.72s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035047\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035058\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035058\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2853\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.0, 68.77393, 196.47248)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21866.64 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.54 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VYJ1\n",
            "VYJ1\n",
            "VYJ1\n",
            "VYJ1\n",
            "VYJ1\n",
            "2853\n",
            "2853\n",
            "2853\n",
            "2853\n",
            "2853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.83 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1752541184717841, Train Rows: 2352, Val Rows: 501\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-73.4274\t = Validation score   (-mean_absolute_error)\n",
            "\t11.55s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-73.4274\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.84s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035058\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035110\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035110\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2746\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2069.403889, 0.0, 47.43914, 129.15143)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21865.95 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.44 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SLZ5\n",
            "SLZ5\n",
            "SLZ5\n",
            "SLZ5\n",
            "SLZ5\n",
            "2746\n",
            "2746\n",
            "2746\n",
            "2746\n",
            "2746\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.79 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1820830298616169, Train Rows: 2246, Val Rows: 500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-42.0307\t = Validation score   (-mean_absolute_error)\n",
            "\t13.79s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-42.0307\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.08s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035110\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035124\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035124\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2489\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2104.006944, 0.0, 79.38305, 165.08043)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21887.52 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.22 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NNC2\n",
            "NNC2\n",
            "NNC2\n",
            "NNC2\n",
            "NNC2\n",
            "2489\n",
            "2489\n",
            "2489\n",
            "2489\n",
            "2489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.72 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1991, Val Rows: 498\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-66.3843\t = Validation score   (-mean_absolute_error)\n",
            "\t16.17s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-66.3843\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 16.46s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035124\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035140\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035140\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2470\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1965.065556, 0.0, 17.86022, 69.16307)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21871.49 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.21 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UVK6\n",
            "UVK6\n",
            "UVK6\n",
            "UVK6\n",
            "UVK6\n",
            "2470\n",
            "2470\n",
            "2470\n",
            "2470\n",
            "2470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.71 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1976, Val Rows: 494\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-16.4269\t = Validation score   (-mean_absolute_error)\n",
            "\t15.35s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-16.4269\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.64s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035140\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035156\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035156\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2430\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1882.755833, 0.0, 32.62602, 108.89686)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21881.56 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DMD4\n",
            "DMD4\n",
            "DMD4\n",
            "DMD4\n",
            "DMD4\n",
            "2430\n",
            "2430\n",
            "2430\n",
            "2430\n",
            "2430\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.7 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1944, Val Rows: 486\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-29.2154\t = Validation score   (-mean_absolute_error)\n",
            "\t12.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-29.2154\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.81s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035156\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035209\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035209\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.72 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2422\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2141.815, 0.0, 30.43492, 118.45549)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21891.28 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.16 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TXO3\n",
            "TXO3\n",
            "TXO3\n",
            "TXO3\n",
            "TXO3\n",
            "2422\n",
            "2422\n",
            "2422\n",
            "2422\n",
            "2422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.7 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1937, Val Rows: 485\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-34.0619\t = Validation score   (-mean_absolute_error)\n",
            "\t12.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-34.0619\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.16s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035209\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035222\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035222\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2354\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2125.051667, 0.0, 75.3961, 217.78511)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21861.94 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.1 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VXM8\n",
            "VXM8\n",
            "VXM8\n",
            "VXM8\n",
            "VXM8\n",
            "2354\n",
            "2354\n",
            "2354\n",
            "2354\n",
            "2354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.68 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1883, Val Rows: 471\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-77.796\t = Validation score   (-mean_absolute_error)\n",
            "\t19.65s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-77.796\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.98s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035222\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035242\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035242\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2303\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2116.804167, 0.0, 77.67734, 197.6834)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21836.21 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.05 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AZU6\n",
            "AZU6\n",
            "AZU6\n",
            "AZU6\n",
            "AZU6\n",
            "2303\n",
            "2303\n",
            "2303\n",
            "2303\n",
            "2303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1842, Val Rows: 461\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-79.0597\t = Validation score   (-mean_absolute_error)\n",
            "\t17.55s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-79.0597\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.87s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035242\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035300\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035300\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2300\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
            "\tLabel info (max, min, mean, stddev): (1397.805556, 0.0, 1.10592, 32.89574)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21853.58 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.06 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RKA2\n",
            "RKA2\n",
            "RKA2\n",
            "RKA2\n",
            "RKA2\n",
            "2300\n",
            "2300\n",
            "2300\n",
            "2300\n",
            "2300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.67 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1840, Val Rows: 460\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-3.1716\t = Validation score   (-mean_absolute_error)\n",
            "\t11.68s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-3.1716\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.97s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035300\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035313\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035313\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2281\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1905.061944, 0.0, 69.69806, 158.17039)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21844.8 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.02 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IEW6\n",
            "IEW6\n",
            "IEW6\n",
            "IEW6\n",
            "IEW6\n",
            "2281\n",
            "2281\n",
            "2281\n",
            "2281\n",
            "2281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.66 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1824, Val Rows: 457\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-61.2653\t = Validation score   (-mean_absolute_error)\n",
            "\t11.97s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-61.2653\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.25s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035313\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035325\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035325\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2083\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1499.908056, 0.0, 30.56187, 81.47758)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21852.45 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.87 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WAF5\n",
            "WAF5\n",
            "WAF5\n",
            "WAF5\n",
            "WAF5\n",
            "2083\n",
            "2083\n",
            "2083\n",
            "2083\n",
            "2083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.6 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1666, Val Rows: 417\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-31.7412\t = Validation score   (-mean_absolute_error)\n",
            "\t12.96s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-31.7412\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.25s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035325\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035338\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035338\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    2073\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1831.358889, 0.0, 21.18246, 93.75764)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21893.9 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.85 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URE2\n",
            "URE2\n",
            "URE2\n",
            "URE2\n",
            "URE2\n",
            "2073\n",
            "2073\n",
            "2073\n",
            "2073\n",
            "2073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.6 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1658, Val Rows: 415\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-24.4263\t = Validation score   (-mean_absolute_error)\n",
            "\t12.31s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-24.4263\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.61s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035338\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035351\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035351\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1959\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2144.961944, 0.0, 94.37881, 177.51358)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21884.58 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.75 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MMW5\n",
            "MMW5\n",
            "MMW5\n",
            "MMW5\n",
            "MMW5\n",
            "1959\n",
            "1959\n",
            "1959\n",
            "1959\n",
            "1959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.57 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1567, Val Rows: 392\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-74.0738\t = Validation score   (-mean_absolute_error)\n",
            "\t14.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-74.0738\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.42s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035351\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035406\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035406\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.71 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1876\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1913.126389, 0.0, 50.18176, 134.07432)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21888.97 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.67 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UIR7\n",
            "UIR7\n",
            "UIR7\n",
            "UIR7\n",
            "UIR7\n",
            "1876\n",
            "1876\n",
            "1876\n",
            "1876\n",
            "1876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.54 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1500, Val Rows: 376\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-38.6465\t = Validation score   (-mean_absolute_error)\n",
            "\t10.82s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-38.6465\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035406\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035417\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035417\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1704\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1814.791389, 0.0, 64.76176, 154.89062)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21889.54 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.52 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TYY2\n",
            "TYY2\n",
            "TYY2\n",
            "TYY2\n",
            "TYY2\n",
            "1704\n",
            "1704\n",
            "1704\n",
            "1704\n",
            "1704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.49 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.22s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1363, Val Rows: 341\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-51.7079\t = Validation score   (-mean_absolute_error)\n",
            "\t9.83s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-51.7079\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.13s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035417\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035427\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035427\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1694\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2019.872778, 0.0, 51.59201, 179.39584)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21887.19 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.5 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KIU2\n",
            "KIU2\n",
            "KIU2\n",
            "KIU2\n",
            "KIU2\n",
            "1694\n",
            "1694\n",
            "1694\n",
            "1694\n",
            "1694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.49 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1355, Val Rows: 339\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-61.0615\t = Validation score   (-mean_absolute_error)\n",
            "\t12.21s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-61.0615\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.47s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035427\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035440\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035440\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1660\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2080.562222, 0.0, 44.19496, 164.82891)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21892.2 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.47 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MCG4\n",
            "MCG4\n",
            "MCG4\n",
            "MCG4\n",
            "MCG4\n",
            "1660\n",
            "1660\n",
            "1660\n",
            "1660\n",
            "1660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.48 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1328, Val Rows: 332\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-40.8351\t = Validation score   (-mean_absolute_error)\n",
            "\t11.48s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-40.8351\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.74s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035440\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035452\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035452\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1608\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2091.578889, 0.0, 106.35088, 270.13582)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21872.5 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.42 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CEI5\n",
            "CEI5\n",
            "CEI5\n",
            "CEI5\n",
            "CEI5\n",
            "1608\n",
            "1608\n",
            "1608\n",
            "1608\n",
            "1608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.47 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1286, Val Rows: 322\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-104.5481\t = Validation score   (-mean_absolute_error)\n",
            "\t32.15s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-104.5481\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 32.42s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035452\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035524\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035524\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1560\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (972.4280556, 0.0, 80.23576, 104.68262)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21884.05 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CXL1\n",
            "CXL1\n",
            "CXL1\n",
            "CXL1\n",
            "CXL1\n",
            "1560\n",
            "1560\n",
            "1560\n",
            "1560\n",
            "1560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1248, Val Rows: 312\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-45.7441\t = Validation score   (-mean_absolute_error)\n",
            "\t14.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-45.7441\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.89s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035524\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035539\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035539\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1553\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2156.765833, 0.0, 111.84682, 339.81966)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21842.26 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.38 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PEL6\n",
            "PEL6\n",
            "PEL6\n",
            "PEL6\n",
            "PEL6\n",
            "1553\n",
            "1553\n",
            "1553\n",
            "1553\n",
            "1553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1242, Val Rows: 311\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-123.612\t = Validation score   (-mean_absolute_error)\n",
            "\t24.86s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-123.612\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 25.14s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035539\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035604\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035604\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1552\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2152.193889, 0.0, 81.88805, 215.22815)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21877.57 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.38 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SPG1\n",
            "SPG1\n",
            "SPG1\n",
            "SPG1\n",
            "SPG1\n",
            "1552\n",
            "1552\n",
            "1552\n",
            "1552\n",
            "1552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.45 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1241, Val Rows: 311\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-74.7191\t = Validation score   (-mean_absolute_error)\n",
            "\t18.84s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-74.7191\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 19.12s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035604\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035624\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035624\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1432\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1960.149722, 0.0, 55.98423, 137.18119)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21871.27 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.28 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QGN3\n",
            "QGN3\n",
            "QGN3\n",
            "QGN3\n",
            "QGN3\n",
            "1432\n",
            "1432\n",
            "1432\n",
            "1432\n",
            "1432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.42 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1145, Val Rows: 287\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-52.3726\t = Validation score   (-mean_absolute_error)\n",
            "\t14.57s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-52.3726\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.84s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035624\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035639\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035639\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1410\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1452.077222, 0.0, 62.95691, 144.19564)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21865.06 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.26 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HGH2\n",
            "HGH2\n",
            "HGH2\n",
            "HGH2\n",
            "HGH2\n",
            "1410\n",
            "1410\n",
            "1410\n",
            "1410\n",
            "1410\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.41 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1128, Val Rows: 282\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-57.6197\t = Validation score   (-mean_absolute_error)\n",
            "\t15.34s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-57.6197\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.62s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035639\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035654\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035654\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1350\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1250.076389, 0.0, 90.39754, 160.23247)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21847.62 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.21 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BAZ5\n",
            "BAZ5\n",
            "BAZ5\n",
            "BAZ5\n",
            "BAZ5\n",
            "1350\n",
            "1350\n",
            "1350\n",
            "1350\n",
            "1350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1080, Val Rows: 270\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-72.6868\t = Validation score   (-mean_absolute_error)\n",
            "\t10.59s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-72.6868\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.87s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035654\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035705\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035705\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1250\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2068.820556, 0.0, 24.29908, 96.44792)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21862.92 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.12 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OBZ3\n",
            "OBZ3\n",
            "OBZ3\n",
            "OBZ3\n",
            "OBZ3\n",
            "1250\n",
            "1250\n",
            "1250\n",
            "1250\n",
            "1250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1000, Val Rows: 250\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-24.3678\t = Validation score   (-mean_absolute_error)\n",
            "\t15.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-24.3678\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.78s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035705\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035721\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035721\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.70 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1243\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1554.027778, 0.0, 33.10661, 103.38623)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21851.66 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.11 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EIA2\n",
            "EIA2\n",
            "EIA2\n",
            "EIA2\n",
            "EIA2\n",
            "1243\n",
            "1243\n",
            "1243\n",
            "1243\n",
            "1243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 994, Val Rows: 249\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-33.4218\t = Validation score   (-mean_absolute_error)\n",
            "\t12.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-33.4218\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.92s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035721\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035734\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035734\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1213\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1679.275556, 0.0, 89.90488, 163.09488)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21834.01 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PAF4\n",
            "PAF4\n",
            "PAF4\n",
            "PAF4\n",
            "PAF4\n",
            "1213\n",
            "1213\n",
            "1213\n",
            "1213\n",
            "1213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.35 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 970, Val Rows: 243\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-71.1421\t = Validation score   (-mean_absolute_error)\n",
            "\t12.67s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-71.1421\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.93s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035734\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035747\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035747\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1178\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1537.126389, 0.0, 24.53383, 87.89059)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21855.81 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.05 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVL6\n",
            "EVL6\n",
            "EVL6\n",
            "EVL6\n",
            "EVL6\n",
            "1178\n",
            "1178\n",
            "1178\n",
            "1178\n",
            "1178\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.34 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 942, Val Rows: 236\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-32.0007\t = Validation score   (-mean_absolute_error)\n",
            "\t9.21s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-32.0007\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 9.48s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035747\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035757\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035757\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1115\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1664.400556, 0.0, 38.42316, 122.65795)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21871.47 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.99 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OKM4\n",
            "OKM4\n",
            "OKM4\n",
            "OKM4\n",
            "OKM4\n",
            "1115\n",
            "1115\n",
            "1115\n",
            "1115\n",
            "1115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  4 | ['ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  3 | ['SHIP_TYPE_CATEGORY', 'DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 892, Val Rows: 223\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-35.7681\t = Validation score   (-mean_absolute_error)\n",
            "\t15.05s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-35.7681\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.33s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035757\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035812\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035812\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1110\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1824.479444, 0.0, 65.53662, 169.49702)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21868.77 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.99 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IVU2\n",
            "IVU2\n",
            "IVU2\n",
            "IVU2\n",
            "IVU2\n",
            "1110\n",
            "1110\n",
            "1110\n",
            "1110\n",
            "1110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 888, Val Rows: 222\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-72.9076\t = Validation score   (-mean_absolute_error)\n",
            "\t16.03s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-72.9076\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 16.28s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035812\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035829\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035829\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1106\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1660.383333, 0.0, 89.72936, 151.27811)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21853.38 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.98 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XIU1\n",
            "XIU1\n",
            "XIU1\n",
            "XIU1\n",
            "XIU1\n",
            "1106\n",
            "1106\n",
            "1106\n",
            "1106\n",
            "1106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 884, Val Rows: 222\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-72.1894\t = Validation score   (-mean_absolute_error)\n",
            "\t13.23s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-72.1894\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.51s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035829\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035842\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035842\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1100\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1086.762778, 0.0, 79.24395, 133.35781)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21845.51 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.98 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TBL3\n",
            "TBL3\n",
            "TBL3\n",
            "TBL3\n",
            "TBL3\n",
            "1100\n",
            "1100\n",
            "1100\n",
            "1100\n",
            "1100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 880, Val Rows: 220\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-64.2456\t = Validation score   (-mean_absolute_error)\n",
            "\t14.46s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-64.2456\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.72s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035842\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035857\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035857\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1075\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1783.734444, 0.0, 15.48255, 85.40852)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21734.21 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.95 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QEA4\n",
            "QEA4\n",
            "QEA4\n",
            "QEA4\n",
            "QEA4\n",
            "1075\n",
            "1075\n",
            "1075\n",
            "1075\n",
            "1075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.31 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 860, Val Rows: 215\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-14.8479\t = Validation score   (-mean_absolute_error)\n",
            "\t11.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-14.8479\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.69s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035857\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035909\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035909\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1027\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1501.324167, 0.0, 87.40313, 146.14811)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21791.82 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.91 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QGL7\n",
            "QGL7\n",
            "QGL7\n",
            "QGL7\n",
            "QGL7\n",
            "1027\n",
            "1027\n",
            "1027\n",
            "1027\n",
            "1027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.3 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 821, Val Rows: 206\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-66.8908\t = Validation score   (-mean_absolute_error)\n",
            "\t11.1s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-66.8908\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.38s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035909\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035920\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035920\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1012\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1999.665278, 0.0, 24.37512, 115.5882)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21854.77 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.9 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SXD2\n",
            "SXD2\n",
            "SXD2\n",
            "SXD2\n",
            "SXD2\n",
            "1012\n",
            "1012\n",
            "1012\n",
            "1012\n",
            "1012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.29 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 809, Val Rows: 203\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-19.2367\t = Validation score   (-mean_absolute_error)\n",
            "\t11.95s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-19.2367\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.21s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035920\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035933\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035933\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    1002\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2140.12, 0.0, 165.75148, 352.85272)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21872.38 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.89 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPM2\n",
            "PPM2\n",
            "PPM2\n",
            "PPM2\n",
            "PPM2\n",
            "1002\n",
            "1002\n",
            "1002\n",
            "1002\n",
            "1002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['ID', 'BREADTH', 'DEPTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['SHIP_TYPE_CATEGORY']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.29 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 801, Val Rows: 201\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-134.6236\t = Validation score   (-mean_absolute_error)\n",
            "\t11.45s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-134.6236\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.75s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035933\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035945\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035945\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    882\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2098.553611, 0.0, 97.44456, 225.74055)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21833.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.79 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BGX4\n",
            "BGX4\n",
            "BGX4\n",
            "BGX4\n",
            "BGX4\n",
            "882\n",
            "882\n",
            "882\n",
            "882\n",
            "882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.26 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 705, Val Rows: 177\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-95.8753\t = Validation score   (-mean_absolute_error)\n",
            "\t13.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-95.8753\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.34s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035945\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_035958\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_035958\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    857\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1979.001389, 0.0, 81.53111, 234.05573)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21865.43 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.76 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REJ1\n",
            "REJ1\n",
            "REJ1\n",
            "REJ1\n",
            "REJ1\n",
            "857\n",
            "857\n",
            "857\n",
            "857\n",
            "857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 685, Val Rows: 172\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-103.0737\t = Validation score   (-mean_absolute_error)\n",
            "\t10.73s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-103.0737\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.99s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_035958\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040009\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040009\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    822\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2116.382778, 0.0, 118.91655, 258.5082)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21872.94 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.73 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVX2\n",
            "EVX2\n",
            "EVX2\n",
            "EVX2\n",
            "EVX2\n",
            "822\n",
            "822\n",
            "822\n",
            "822\n",
            "822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  5 | ['ID', 'BREADTH', 'DEPTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['SHIP_TYPE_CATEGORY', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.24 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 657, Val Rows: 165\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-146.5937\t = Validation score   (-mean_absolute_error)\n",
            "\t10.59s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-146.5937\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.84s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040009\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040020\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040020\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    792\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1710.938056, 0.0, 61.71432, 181.83172)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21867.36 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.7 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QRN3\n",
            "QRN3\n",
            "QRN3\n",
            "QRN3\n",
            "QRN3\n",
            "792\n",
            "792\n",
            "792\n",
            "792\n",
            "792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.23 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 633, Val Rows: 159\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-60.4714\t = Validation score   (-mean_absolute_error)\n",
            "\t11.86s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-60.4714\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040020\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040032\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040032\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.69 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    781\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1479.897778, 0.0, 76.27168, 136.60538)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21848.26 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.7 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OZD2\n",
            "OZD2\n",
            "OZD2\n",
            "OZD2\n",
            "OZD2\n",
            "781\n",
            "781\n",
            "781\n",
            "781\n",
            "781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\t\t('float', [])    : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])      :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.23 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 624, Val Rows: 157\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-57.2167\t = Validation score   (-mean_absolute_error)\n",
            "\t14.82s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-57.2167\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.09s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040032\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040047\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040047\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    720\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1743.36, 0.0, 85.17842, 138.91108)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21676.2 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.64 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRQ5\n",
            "GRQ5\n",
            "GRQ5\n",
            "GRQ5\n",
            "GRQ5\n",
            "720\n",
            "720\n",
            "720\n",
            "720\n",
            "720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.21 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 576, Val Rows: 144\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-59.1572\t = Validation score   (-mean_absolute_error)\n",
            "\t12.12s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-59.1572\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.37s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040047\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040100\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040100\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    677\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2120.121111, 0.0, 72.14794, 240.13837)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21731.01 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.6 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JGL5\n",
            "JGL5\n",
            "JGL5\n",
            "JGL5\n",
            "JGL5\n",
            "677\n",
            "677\n",
            "677\n",
            "677\n",
            "677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.2 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 541, Val Rows: 136\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-102.5958\t = Validation score   (-mean_absolute_error)\n",
            "\t12.41s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-102.5958\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.66s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040100\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040113\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040113\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    578\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1604.490278, 0.0, 28.71235, 114.58595)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21746.68 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.51 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 10): ['ARI_CO', 'ARI_PO', 'DEPTH', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  3 | ['ID', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  3 | ['SHIP_TYPE_CATEGORY', 'BREADTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t43 features in original data used to generate 43 features in processed data.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSP6\n",
            "CSP6\n",
            "CSP6\n",
            "CSP6\n",
            "CSP6\n",
            "578\n",
            "578\n",
            "578\n",
            "578\n",
            "578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tTrain Data (Processed) Memory Usage: 0.17 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.16s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 462, Val Rows: 116\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-36.5879\t = Validation score   (-mean_absolute_error)\n",
            "\t10.75s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-36.5879\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.97s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040113\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040124\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040124\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    577\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1966.637778, 0.0, 100.56406, 259.61028)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21767.69 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.51 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UPI6\n",
            "UPI6\n",
            "UPI6\n",
            "UPI6\n",
            "UPI6\n",
            "577\n",
            "577\n",
            "577\n",
            "577\n",
            "577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DEPTH']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.17 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 461, Val Rows: 116\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-96.9723\t = Validation score   (-mean_absolute_error)\n",
            "\t15.85s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-96.9723\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 16.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040124\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040140\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040140\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    568\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2136.011111, 0.0, 100.67017, 274.03948)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21789.48 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.5 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.17 MB (0.0% of available memory)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MOC5\n",
            "MOC5\n",
            "MOC5\n",
            "MOC5\n",
            "MOC5\n",
            "568\n",
            "568\n",
            "568\n",
            "568\n",
            "568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Data preprocessing and feature engineering runtime = 0.16s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 454, Val Rows: 114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-81.0674\t = Validation score   (-mean_absolute_error)\n",
            "\t10.44s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-81.0674\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.68s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040140\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040151\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040151\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    533\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (473.1002778, 0.0, 39.18026, 65.69818)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21812.05 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.47 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 10): ['ARI_CO', 'ARI_PO', 'DRAUGHT', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  3 | ['ID', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HYV6\n",
            "HYV6\n",
            "HYV6\n",
            "HYV6\n",
            "HYV6\n",
            "533\n",
            "533\n",
            "533\n",
            "533\n",
            "533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t('int', ['bool']) :  3 | ['SHIP_TYPE_CATEGORY', 'BREADTH', 'DEPTH']\n",
            "\t0.1s = Fit runtime\n",
            "\t43 features in original data used to generate 43 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.15 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 426, Val Rows: 107\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-28.7005\t = Validation score   (-mean_absolute_error)\n",
            "\t13.36s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-28.7005\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.6s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040151\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040205\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040205\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    512\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (605.1586111, 0.0, 53.38431, 83.78321)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21808.59 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.46 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PBQ1\n",
            "PBQ1\n",
            "PBQ1\n",
            "PBQ1\n",
            "PBQ1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DRAUGHT']\n",
            "\t0.2s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.15 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 409, Val Rows: 103\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-50.1445\t = Validation score   (-mean_absolute_error)\n",
            "\t10.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-50.1445\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.15s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040205\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040216\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040216\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    480\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (129.9619444, 0.0, 2.56958, 11.44516)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21822.81 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.43 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PUF3\n",
            "PUF3\n",
            "PUF3\n",
            "PUF3\n",
            "PUF3\n",
            "480\n",
            "480\n",
            "480\n",
            "480\n",
            "480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  6 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['DEPTH']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 384, Val Rows: 96\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-1.056\t = Validation score   (-mean_absolute_error)\n",
            "\t90.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-1.056\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 90.98s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040216\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040347\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040347\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    433\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1322.722222, 0.0, 90.32039, 144.56287)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21740.91 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LHD1\n",
            "LHD1\n",
            "LHD1\n",
            "LHD1\n",
            "LHD1\n",
            "433\n",
            "433\n",
            "433\n",
            "433\n",
            "433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tUseless Original Features (Count: 10): ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  6 | ['ID', 'BREADTH', 'DEPTH', 'DRAUGHT', 'FLAG', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  4 | ['ID', 'BREADTH', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  2 | ['DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t43 features in original data used to generate 43 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.13 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 346, Val Rows: 87\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-62.3691\t = Validation score   (-mean_absolute_error)\n",
            "\t12.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-62.3691\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.89s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040347\\\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_040400\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_040400\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   18.68 GB / 511.09 GB (3.7%)\n",
            "Train Data Rows:    416\n",
            "Train Data Columns: 53\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (600.8713889, 0.0, 8.11309, 38.64977)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    21787.76 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.37 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 9): ['ARI_CO', 'ARI_PO', 'PORT_SIZE', 'mean_enc_ARI_CO', 'std_enc_ARI_CO', 'mean_enc_ARI_PO', 'std_enc_ARI_PO', 'country_cluster', 'PORT_SIZE_Zone']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOV8\n",
            "OOV8\n",
            "OOV8\n",
            "OOV8\n",
            "OOV8\n",
            "416\n",
            "416\n",
            "416\n",
            "416\n",
            "416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])    :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('object', []) :  7 | ['SHIP_TYPE_CATEGORY', 'ID', 'BREADTH', 'DEPTH', 'DRAUGHT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :  4 | ['SHIP_TYPE_CATEGORY', 'ID', 'FLAG', 'PO_Y_M_D']\n",
            "\t\t('float', [])     : 28 | ['DIST', 'LENGTH', 'DUBAI', 'BRENT', 'WTI', ...]\n",
            "\t\t('int', [])       :  9 | ['DEADWEIGHT', 'GT', 'ATA_LT', 'year', 'month', ...]\n",
            "\t\t('int', ['bool']) :  3 | ['BREADTH', 'DEPTH', 'DRAUGHT']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.12 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 332, Val Rows: 84\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost ...\n",
            "\t-6.5497\t = Validation score   (-mean_absolute_error)\n",
            "\t12.12s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-6.5497\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.37s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_040400\\\")\n"
          ]
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "# Load train, test, and sample_submission data\n",
        "train_data = train.copy()\n",
        "test_data = test.copy()\n",
        "sample_submission = submission.copy()\n",
        "\n",
        "# Create an empty DataFrame for the final predictions\n",
        "final_predictions = pd.DataFrame(index=test_data.index)\n",
        "final_predictions['CI_HOUR'] = 0  # Initialize with 0\n",
        "\n",
        "# Filter the ARI_PO values in train_data that have at least 400 occurrences\n",
        "ari_po_values_to_model = train_data['ARI_PO'].value_counts()\n",
        "ari_po_values_to_model = ari_po_values_to_model[ari_po_values_to_model >= 400].index.tolist()\n",
        "\n",
        "# Loop through each ARI_PO value and train a model for it\n",
        "for ari_po in ari_po_values_to_model:\n",
        "    train_subset = train_data[train_data['ARI_PO'] == ari_po]\n",
        "    test_subset = test_data[test_data['ARI_PO'] == ari_po]\n",
        "    print(ari_po)\n",
        "    print(ari_po)\n",
        "    print(ari_po)\n",
        "    print(ari_po)\n",
        "    print(ari_po)\n",
        "\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    \n",
        "    # Convert to AutoGluon Dataset\n",
        "    train_subset_ag = TabularDataset(train_subset)\n",
        "    test_subset_ag = TabularDataset(test_subset)\n",
        "\n",
        "    # Train the model\n",
        "    predictor = TabularPredictor(label='CI_HOUR', eval_metric='mean_absolute_error').fit(\n",
        "        train_subset_ag, presets='medium_quality', ag_args_fit={'num_gpus': 0}, included_model_types=['CAT']\n",
        "    )\n",
        "\n",
        "    # Predict on the test subset\n",
        "    y_pred = predictor.predict(test_subset_ag)\n",
        "    \n",
        "    # Assign the predictions to the final_predictions DataFrame\n",
        "    final_predictions.loc[test_subset.index, 'CI_HOUR'] = y_pred.values\n",
        "\n",
        "# Merge the final predictions with sample_submission on the index\n",
        "sample_submission['CI_HOUR'] = final_predictions['CI_HOUR']\n",
        "sample_submission.to_csv(\"testing.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_073236\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (367441 samples, 20.58 MB).\n",
            "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_073236\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   14.72 GB / 511.09 GB (2.9%)\n",
            "Train Data Rows:    367441\n",
            "Train Data Columns: 6\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.0, 61.87712, 170.57522)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    19320.5 MB\n",
            "\tTrain Data (Original)  Memory Usage: 17.64 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 6 | ['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 6 | ['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin', ...]\n",
            "\t0.3s = Fit runtime\n",
            "\t6 features in original data used to generate 6 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 17.64 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.4s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 363766, Val Rows: 3675\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['RF', 'LGBM'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "\tThe models types ['LGBM'] are not present in the model list specified by the user and will be ignored:\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-17.8258\t = Validation score   (-mean_absolute_error)\n",
            "\t46.57s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-17.8258\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 47.92s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231012_073236\\\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_test</th>\n",
              "      <th>score_val</th>\n",
              "      <th>pred_time_test</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_test_marginal</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForestMSE</td>\n",
              "      <td>-10.569477</td>\n",
              "      <td>-17.825751</td>\n",
              "      <td>3.979716</td>\n",
              "      <td>0.060516</td>\n",
              "      <td>46.566810</td>\n",
              "      <td>3.979716</td>\n",
              "      <td>0.060516</td>\n",
              "      <td>46.566810</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>-10.569477</td>\n",
              "      <td>-17.825751</td>\n",
              "      <td>4.009856</td>\n",
              "      <td>0.061610</td>\n",
              "      <td>46.573476</td>\n",
              "      <td>0.030141</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.006665</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 model  score_test  score_val  pred_time_test  pred_time_val  \\\n",
              "0      RandomForestMSE  -10.569477 -17.825751        3.979716       0.060516   \n",
              "1  WeightedEnsemble_L2  -10.569477 -17.825751        4.009856       0.061610   \n",
              "\n",
              "    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
              "0  46.566810                 3.979716                0.060516   \n",
              "1  46.573476                 0.030141                0.001094   \n",
              "\n",
              "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
              "0          46.566810            1       True          1  \n",
              "1           0.006665            2       True          2  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_data = TabularDataset(train)\n",
        "test_data = TabularDataset(test)\n",
        "\n",
        "\n",
        "#predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, included_model_types = ['CAT'])\n",
        "#predictor = TabularPredictor(label='CI_HsOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0})\n",
        "predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, included_model_types = ['RF', 'LGBM'])\n",
        "\n",
        "predictor.leaderboard(train, silent=True)\n",
        "\n",
        "\n",
        "y_pred = predictor.predict(test_data)\n",
        "y_pred = pd.DataFrame(y_pred, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred['CI_HOUR']\n",
        "predictor.leaderboard(train, silent=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# feature_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231012_073722\\\"\n",
            "Presets specified: ['medium_quality']\n",
            "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (367441 samples, 17.64 MB).\n",
            "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231012_073722\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   13.94 GB / 511.09 GB (2.7%)\n",
            "Train Data Rows:    367441\n",
            "Train Data Columns: 5\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.0, 61.87712, 170.57522)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    18761.72 MB\n",
            "\tTrain Data (Original)  Memory Usage: 14.7 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 5 | ['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 5 | ['WTI', 'DUBAI', 'BRENT', 'month_sin', 'day_sin']\n",
            "\t0.4s = Fit runtime\n",
            "\t5 features in original data used to generate 5 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 14.7 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.5s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 363766, Val Rows: 3675\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Included models: ['RF', 'GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "Fitting 5 L1 models ...\n",
            "Fitting model: LightGBMXT ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1000]\tvalid_set's l1: 77.3625\n",
            "[2000]\tvalid_set's l1: 73.131\n",
            "[3000]\tvalid_set's l1: 70.167\n",
            "[4000]\tvalid_set's l1: 67.8511\n",
            "[5000]\tvalid_set's l1: 65.8352\n",
            "[6000]\tvalid_set's l1: 64.1543\n",
            "[7000]\tvalid_set's l1: 62.7938\n",
            "[8000]\tvalid_set's l1: 61.5584\n",
            "[9000]\tvalid_set's l1: 60.4681\n",
            "[10000]\tvalid_set's l1: 59.4697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\t-59.4697\t = Validation score   (-mean_absolute_error)\n",
            "\t53.31s\t = Training   runtime\n",
            "\t0.97s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1000]\tvalid_set's l1: 57.0162\n",
            "[2000]\tvalid_set's l1: 47.248\n",
            "[3000]\tvalid_set's l1: 41.5582\n",
            "[4000]\tvalid_set's l1: 37.6642\n",
            "[5000]\tvalid_set's l1: 35.1605\n",
            "[6000]\tvalid_set's l1: 33.3518\n",
            "[7000]\tvalid_set's l1: 31.6402\n",
            "[8000]\tvalid_set's l1: 30.4382\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m subset_data \u001b[39m=\u001b[39m train_data[[target] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(feature_combination)]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m predictor \u001b[39m=\u001b[39m TabularPredictor(label\u001b[39m=\u001b[39;49mtarget, eval_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean_absolute_error\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(subset_data, presets\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmedium_quality\u001b[39;49m\u001b[39m'\u001b[39;49m, ag_args_fit\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mnum_gpus\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0\u001b[39;49m}, included_model_types\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mRF\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mGBM\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mXGB\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m lb \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mleaderboard(subset_data, silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X46sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m score \u001b[39m=\u001b[39m lb[\u001b[39m'\u001b[39m\u001b[39mscore_val\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmax()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49mgargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:986\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    987\u001b[0m     X\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m    988\u001b[0m     X_val\u001b[39m=\u001b[39;49mtuning_data,\n\u001b[0;32m    989\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[0;32m    990\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[0;32m    991\u001b[0m     num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds,\n\u001b[0;32m    992\u001b[0m     num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[0;32m    993\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    994\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    995\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    996\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    997\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    998\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    999\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m   1000\u001b[0m     verbosity\u001b[39m=\u001b[39;49mverbosity,\n\u001b[0;32m   1001\u001b[0m     use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout,\n\u001b[0;32m   1002\u001b[0m )\n\u001b[0;32m   1003\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[0;32m   1006\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1007\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[0;32m   1013\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[1;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X\u001b[39m=\u001b[39;49mX, X_val\u001b[39m=\u001b[39;49mX_val, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:157\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m--> 157\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    158\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    159\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    160\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    161\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    162\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    163\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[0;32m    164\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit_trainer,\n\u001b[0;32m    165\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    166\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    167\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    168\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrainer_fit_kwargs,\n\u001b[0;32m    169\u001b[0m )\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[0;32m    171\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:114\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m log_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m20\u001b[39m, log_str)\n\u001b[1;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(\n\u001b[0;32m    115\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    116\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    117\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    118\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    119\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    120\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    121\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    122\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    123\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    124\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    125\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    126\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    127\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    128\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2371\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[0;32m   2369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_rows_val \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_val)\n\u001b[0;32m   2370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_cols_train \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(X\u001b[39m.\u001b[39mcolumns))\n\u001b[1;32m-> 2371\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_multi_levels(\n\u001b[0;32m   2372\u001b[0m     X,\n\u001b[0;32m   2373\u001b[0m     y,\n\u001b[0;32m   2374\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m   2375\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m   2376\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m   2377\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m   2378\u001b[0m     level_start\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2379\u001b[0m     level_end\u001b[39m=\u001b[39;49mnum_stack_levels \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2380\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2381\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2382\u001b[0m )\n\u001b[0;32m   2383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:395\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    393\u001b[0m         core_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_core)\n\u001b[0;32m    394\u001b[0m         aux_kwargs_level[\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs_level\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_limit\u001b[39m\u001b[39m\"\u001b[39m, time_limit_aux)\n\u001b[1;32m--> 395\u001b[0m     base_model_names, aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level(\n\u001b[0;32m    396\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    397\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    398\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    399\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    400\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    401\u001b[0m         models\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    402\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    403\u001b[0m         base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[0;32m    404\u001b[0m         core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs_level,\n\u001b[0;32m    405\u001b[0m         aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs_level,\n\u001b[0;32m    406\u001b[0m         name_suffix\u001b[39m=\u001b[39;49mname_suffix,\n\u001b[0;32m    407\u001b[0m         infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    408\u001b[0m         infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m     model_names_fit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m base_model_names \u001b[39m+\u001b[39m aux_models\n\u001b[0;32m    411\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_best \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(model_names_fit) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:539\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    537\u001b[0m     core_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m core_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[0;32m    538\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m aux_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_suffix\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m name_suffix\n\u001b[1;32m--> 539\u001b[0m core_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_new_level_core(\n\u001b[0;32m    540\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    541\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    542\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    543\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    544\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    545\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m    546\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    547\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    548\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    549\u001b[0m     base_model_names\u001b[39m=\u001b[39;49mbase_model_names,\n\u001b[0;32m    550\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcore_kwargs,\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m \u001b[39mif\u001b[39;00m X_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     aux_models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_new_level_aux(\n\u001b[0;32m    555\u001b[0m         X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my, base_model_names\u001b[39m=\u001b[39mcore_models, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, infer_limit\u001b[39m=\u001b[39minfer_limit, infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maux_kwargs\n\u001b[0;32m    556\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:673\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m fit_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m    672\u001b[0m \u001b[39m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi(\n\u001b[0;32m    674\u001b[0m     X\u001b[39m=\u001b[39;49mX_init,\n\u001b[0;32m    675\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    676\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    677\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    678\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    679\u001b[0m     models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m    680\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    681\u001b[0m     stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[0;32m    682\u001b[0m     compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[0;32m    683\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m    684\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    685\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2321\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[39mif\u001b[39;00m n_repeat_start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2320\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m-> 2321\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_initial(\n\u001b[0;32m   2322\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   2323\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   2324\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m   2325\u001b[0m         k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[0;32m   2326\u001b[0m         n_repeats\u001b[39m=\u001b[39;49mn_repeats_initial,\n\u001b[0;32m   2327\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[0;32m   2328\u001b[0m         feature_prune_kwargs\u001b[39m=\u001b[39;49mfeature_prune_kwargs,\n\u001b[0;32m   2329\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2330\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2331\u001b[0m     )\n\u001b[0;32m   2332\u001b[0m     n_repeat_start \u001b[39m=\u001b[39m n_repeats_initial\n\u001b[0;32m   2333\u001b[0m     \u001b[39mif\u001b[39;00m time_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2160\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m bagged:\n\u001b[0;32m   2159\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 2160\u001b[0m     models \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_fold(\n\u001b[0;32m   2161\u001b[0m         models\u001b[39m=\u001b[39;49mmodels,\n\u001b[0;32m   2162\u001b[0m         hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs,\n\u001b[0;32m   2163\u001b[0m         time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2164\u001b[0m         time_split\u001b[39m=\u001b[39;49mtime_split,\n\u001b[0;32m   2165\u001b[0m         time_ratio\u001b[39m=\u001b[39;49mtime_ratio,\n\u001b[0;32m   2166\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_args,\n\u001b[0;32m   2167\u001b[0m     )\n\u001b[0;32m   2168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2169\u001b[0m     time_ratio \u001b[39m=\u001b[39m hpo_time_ratio \u001b[39mif\u001b[39;00m hpo_enabled \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2278\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2276\u001b[0m         time_start_model \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2277\u001b[0m         time_left \u001b[39m=\u001b[39m time_limit \u001b[39m-\u001b[39m (time_start_model \u001b[39m-\u001b[39m time_start)\n\u001b[1;32m-> 2278\u001b[0m model_name_trained_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single_full(\n\u001b[0;32m   2279\u001b[0m     X, y, model, time_limit\u001b[39m=\u001b[39;49mtime_left, hyperparameter_tune_kwargs\u001b[39m=\u001b[39;49mhyperparameter_tune_kwargs_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m   2280\u001b[0m )\n\u001b[0;32m   2282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m   2283\u001b[0m     \u001b[39mdel\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2051\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[0;32m   2047\u001b[0m         bagged_model_fit_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[0;32m   2048\u001b[0m             k_fold\u001b[39m=\u001b[39mk_fold, k_fold_start\u001b[39m=\u001b[39mk_fold_start, k_fold_end\u001b[39m=\u001b[39mk_fold_end, n_repeats\u001b[39m=\u001b[39mn_repeats, n_repeat_start\u001b[39m=\u001b[39mn_repeat_start\n\u001b[0;32m   2049\u001b[0m         )\n\u001b[0;32m   2050\u001b[0m         model_fit_kwargs\u001b[39m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[1;32m-> 2051\u001b[0m     model_names_trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_and_save(\n\u001b[0;32m   2052\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   2053\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   2054\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   2055\u001b[0m         X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m   2056\u001b[0m         y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m   2057\u001b[0m         X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m   2058\u001b[0m         stack_name\u001b[39m=\u001b[39;49mstack_name,\n\u001b[0;32m   2059\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   2060\u001b[0m         compute_score\u001b[39m=\u001b[39;49mcompute_score,\n\u001b[0;32m   2061\u001b[0m         total_resources\u001b[39m=\u001b[39;49mtotal_resources,\n\u001b[0;32m   2062\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs,\n\u001b[0;32m   2063\u001b[0m     )\n\u001b[0;32m   2064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m   2065\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_trained\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1733\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[1;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1731\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1732\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1733\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single(X, y, model, X_val, y_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[0;32m   1735\u001b[0m fit_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   1736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_evaluation:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1684\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[1;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train_single\u001b[39m(\u001b[39mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_val\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, total_resources\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_fit_kwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AbstractModel:\n\u001b[0;32m   1680\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m \u001b[39m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[0;32m   1682\u001b[0m \u001b[39m    Returns trained model object.\u001b[39;00m\n\u001b[0;32m   1683\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1684\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, total_resources\u001b[39m=\u001b[39;49mtotal_resources, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_fit_kwargs)\n\u001b[0;32m   1685\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:829\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_fit_resources(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    828\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_memory_usage(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 829\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    831\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\lgb\\lgb_model.py:194\u001b[0m, in \u001b[0;36mLGBModel._fit\u001b[1;34m(self, X, y, X_val, y_val, time_limit, num_gpus, num_cpus, sample_weight, sample_weight_val, verbosity, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_column in param dict is overridden.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m train_lgb_model(early_stopping_callback_kwargs\u001b[39m=\u001b[39;49mearly_stopping_callback_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_params)\n\u001b[0;32m    195\u001b[0m \u001b[39mexcept\u001b[39;00m LightGBMError:\n\u001b[0;32m    196\u001b[0m     \u001b[39mif\u001b[39;00m train_params[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\lgb\\lgb_utils.py:124\u001b[0m, in \u001b[0;36mtrain_lgb_model\u001b[1;34m(early_stopping_callback_kwargs, **train_params)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m booster\u001b[39m.\u001b[39mfit(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_params)\n\u001b[0;32m    123\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m lgb\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrain_params)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[0;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "# 데이터 로드\n",
        "train_data = TabularDataset(train)\n",
        "\n",
        "# 타겟 변수와 모든 피처를 리스트로 변환\n",
        "target = 'CI_HOUR'\n",
        "all_features = [col for col in train_data.columns if col != target]\n",
        "\n",
        "# 모든 피처들에서 하나씩 제외한 조합 생성\n",
        "all_combinations = list(combinations(all_features, len(all_features) - 1))\n",
        "\n",
        "best_score = float('inf')  # MAE는 낮을수록 좋으므로 초기값을 무한대로 설정\n",
        "best_combination = None\n",
        "iteration = 0\n",
        "output_path = 'results.txt'\n",
        "\n",
        "for feature_combination in all_combinations:\n",
        "    iteration += 1\n",
        "    subset_data = train_data[[target] + list(feature_combination)]\n",
        "    \n",
        "    predictor = TabularPredictor(label=target, eval_metric='mean_absolute_error').fit(subset_data, presets='medium_quality', ag_args_fit={'num_gpus': 0}, included_model_types=['RF', 'GBM', 'XGB'])\n",
        "    lb = predictor.leaderboard(subset_data, silent=True)\n",
        "    score = lb['score_val'].max()\n",
        "    \n",
        "    # 결과를 txt 파일에 계속 추가\n",
        "    with open(output_path, 'a') as f:\n",
        "        f.write(\"Iteration: \" + str(iteration) + \"\\n\")\n",
        "        f.write(\"Excluded feature: \" + ', '.join(set(all_features) - set(feature_combination)) + \"\\n\")\n",
        "        f.write(\"MAE: \" + str(score) + \"\\n\")\n",
        "        f.write(\"-\" * 50 + \"\\n\")  # 구분선\n",
        "    \n",
        "    if score < best_score:\n",
        "        best_score = score\n",
        "        best_combination = feature_combination\n",
        "\n",
        "print(\"Best feature combination:\", best_combination)\n",
        "print(\"Best MAE:\", best_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pycaret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_bdb6d_row10_col1, #T_bdb6d_row19_col1 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_bdb6d\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_bdb6d_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
              "      <th id=\"T_bdb6d_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_bdb6d_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
              "      <td id=\"T_bdb6d_row0_col1\" class=\"data row0 col1\" >724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_bdb6d_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
              "      <td id=\"T_bdb6d_row1_col1\" class=\"data row1 col1\" >CI_HOUR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_bdb6d_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
              "      <td id=\"T_bdb6d_row2_col1\" class=\"data row2 col1\" >Regression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_bdb6d_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
              "      <td id=\"T_bdb6d_row3_col1\" class=\"data row3 col1\" >(391939, 34)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_bdb6d_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
              "      <td id=\"T_bdb6d_row4_col1\" class=\"data row4 col1\" >(391939, 61)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_bdb6d_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
              "      <td id=\"T_bdb6d_row5_col1\" class=\"data row5 col1\" >(274357, 61)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_bdb6d_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
              "      <td id=\"T_bdb6d_row6_col1\" class=\"data row6 col1\" >(117582, 61)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_bdb6d_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
              "      <td id=\"T_bdb6d_row7_col1\" class=\"data row7 col1\" >27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_bdb6d_row8_col0\" class=\"data row8 col0\" >Categorical features</td>\n",
              "      <td id=\"T_bdb6d_row8_col1\" class=\"data row8 col1\" >6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_bdb6d_row9_col0\" class=\"data row9 col0\" >Rows with missing values</td>\n",
              "      <td id=\"T_bdb6d_row9_col1\" class=\"data row9 col1\" >42.0%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_bdb6d_row10_col0\" class=\"data row10 col0\" >Preprocess</td>\n",
              "      <td id=\"T_bdb6d_row10_col1\" class=\"data row10 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_bdb6d_row11_col0\" class=\"data row11 col0\" >Imputation type</td>\n",
              "      <td id=\"T_bdb6d_row11_col1\" class=\"data row11 col1\" >simple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_bdb6d_row12_col0\" class=\"data row12 col0\" >Numeric imputation</td>\n",
              "      <td id=\"T_bdb6d_row12_col1\" class=\"data row12 col1\" >mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_bdb6d_row13_col0\" class=\"data row13 col0\" >Categorical imputation</td>\n",
              "      <td id=\"T_bdb6d_row13_col1\" class=\"data row13 col1\" >mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_bdb6d_row14_col0\" class=\"data row14 col0\" >Maximum one-hot encoding</td>\n",
              "      <td id=\"T_bdb6d_row14_col1\" class=\"data row14 col1\" >25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_bdb6d_row15_col0\" class=\"data row15 col0\" >Encoding method</td>\n",
              "      <td id=\"T_bdb6d_row15_col1\" class=\"data row15 col1\" >None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_bdb6d_row16_col0\" class=\"data row16 col0\" >Fold Generator</td>\n",
              "      <td id=\"T_bdb6d_row16_col1\" class=\"data row16 col1\" >KFold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_bdb6d_row17_col0\" class=\"data row17 col0\" >Fold Number</td>\n",
              "      <td id=\"T_bdb6d_row17_col1\" class=\"data row17 col1\" >10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_bdb6d_row18_col0\" class=\"data row18 col0\" >CPU Jobs</td>\n",
              "      <td id=\"T_bdb6d_row18_col1\" class=\"data row18 col1\" >-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_bdb6d_row19_col0\" class=\"data row19 col0\" >Use GPU</td>\n",
              "      <td id=\"T_bdb6d_row19_col1\" class=\"data row19 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_bdb6d_row20_col0\" class=\"data row20 col0\" >Log Experiment</td>\n",
              "      <td id=\"T_bdb6d_row20_col1\" class=\"data row20 col1\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_bdb6d_row21_col0\" class=\"data row21 col0\" >Experiment Name</td>\n",
              "      <td id=\"T_bdb6d_row21_col1\" class=\"data row21 col1\" >reg-default-name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bdb6d_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "      <td id=\"T_bdb6d_row22_col0\" class=\"data row22 col0\" >USI</td>\n",
              "      <td id=\"T_bdb6d_row22_col1\" class=\"data row22 col1\" >532b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x28d87dd1340>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 0\n",
            "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
            "[LightGBM] [Info] Start training from score 0.500000\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;BREADTH&#x27;, &#x27;BUILT&#x27;,\n",
              "                                             &#x27;DEADWEIGHT&#x27;, &#x27;DEPTH&#x27;, &#x27;DRAUGHT&#x27;,\n",
              "                                             &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;U_WIND&#x27;, &#x27;V_WIND&#x27;,\n",
              "                                             &#x27;AIR_TEMPERATURE&#x27;, &#x27;BN&#x27;, &#x27;ATA_LT&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;da...\n",
              "                 TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                    transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;,\n",
              "                                             &#x27;FLAG&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;SHIPMANAGER&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;)))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;BREADTH&#x27;, &#x27;BUILT&#x27;,\n",
              "                                             &#x27;DEADWEIGHT&#x27;, &#x27;DEPTH&#x27;, &#x27;DRAUGHT&#x27;,\n",
              "                                             &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;U_WIND&#x27;, &#x27;V_WIND&#x27;,\n",
              "                                             &#x27;AIR_TEMPERATURE&#x27;, &#x27;BN&#x27;, &#x27;ATA_LT&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;da...\n",
              "                 TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                    transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;,\n",
              "                                             &#x27;FLAG&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;SHIPMANAGER&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;)))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;BREADTH&#x27;, &#x27;BUILT&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;DEPTH&#x27;,\n",
              "                            &#x27;DRAUGHT&#x27;, &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;U_WIND&#x27;, &#x27;V_WIND&#x27;,\n",
              "                            &#x27;AIR_TEMPERATURE&#x27;, &#x27;BN&#x27;, &#x27;ATA_LT&#x27;, &#x27;PORT_SIZE&#x27;,\n",
              "                            &#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                            &#x27;month_sin&#x27;, &#x27;month_cos&#x27;, &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                            &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;, &#x27;rounded_hour_sin&#x27;,\n",
              "                            &#x27;rounded_hour_cos&#x27;],\n",
              "                   transformer=SimpleImputer())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;ARI_PO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;ID&#x27;,\n",
              "                            &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "                   transformer=SimpleImputer(strategy=&#x27;most_frequent&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehot_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                   transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                   &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "                                             handle_missing=&#x27;return_nan&#x27;,\n",
              "                                             use_cat_names=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">rest_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "                   transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;,\n",
              "                                                   &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "                                             handle_missing=&#x27;return_nan&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;SHIPMANAGER&#x27;, &#x27;FLAG&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(memory=FastMemory(location=C:\\Users\\ineeji\\AppData\\Local\\Temp\\joblib),\n",
              "         steps=[('numerical_imputer',\n",
              "                 TransformerWrapper(include=['DIST', 'BREADTH', 'BUILT',\n",
              "                                             'DEADWEIGHT', 'DEPTH', 'DRAUGHT',\n",
              "                                             'GT', 'LENGTH', 'U_WIND', 'V_WIND',\n",
              "                                             'AIR_TEMPERATURE', 'BN', 'ATA_LT',\n",
              "                                             'PORT_SIZE', 'year', 'month',\n",
              "                                             'day', 'weekday', 'rounded_hour',\n",
              "                                             'month_sin', 'month_cos',\n",
              "                                             'day_sin', 'da...\n",
              "                 TransformerWrapper(include=['ARI_CO', 'SHIP_TYPE_CATEGORY'],\n",
              "                                    transformer=OneHotEncoder(cols=['ARI_CO',\n",
              "                                                                    'SHIP_TYPE_CATEGORY'],\n",
              "                                                              handle_missing='return_nan',\n",
              "                                                              use_cat_names=True))),\n",
              "                ('rest_encoding',\n",
              "                 TransformerWrapper(include=['ARI_PO', 'ID', 'SHIPMANAGER',\n",
              "                                             'FLAG'],\n",
              "                                    transformer=TargetEncoder(cols=['ARI_PO',\n",
              "                                                                    'ID',\n",
              "                                                                    'SHIPMANAGER',\n",
              "                                                                    'FLAG'],\n",
              "                                                              handle_missing='return_nan')))])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pycaret.regression import *\n",
        "import category_encoders\n",
        "\n",
        "reg = setup(data=train, target='CI_HOUR', use_gpu=True, session_id=724)\n",
        "reg.pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_06b9d th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_06b9d_row0_col0, #T_06b9d_row0_col2, #T_06b9d_row0_col3, #T_06b9d_row0_col4, #T_06b9d_row0_col5, #T_06b9d_row1_col0, #T_06b9d_row1_col1, #T_06b9d_row1_col2, #T_06b9d_row1_col3, #T_06b9d_row1_col4, #T_06b9d_row1_col5, #T_06b9d_row1_col6, #T_06b9d_row2_col0, #T_06b9d_row2_col1, #T_06b9d_row2_col2, #T_06b9d_row2_col3, #T_06b9d_row2_col4, #T_06b9d_row2_col5, #T_06b9d_row2_col6, #T_06b9d_row3_col0, #T_06b9d_row3_col1, #T_06b9d_row3_col2, #T_06b9d_row3_col3, #T_06b9d_row3_col4, #T_06b9d_row3_col6, #T_06b9d_row4_col0, #T_06b9d_row4_col1, #T_06b9d_row4_col5, #T_06b9d_row4_col6, #T_06b9d_row5_col0, #T_06b9d_row5_col1, #T_06b9d_row5_col2, #T_06b9d_row5_col3, #T_06b9d_row5_col4, #T_06b9d_row5_col5, #T_06b9d_row5_col6, #T_06b9d_row6_col0, #T_06b9d_row6_col1, #T_06b9d_row6_col2, #T_06b9d_row6_col3, #T_06b9d_row6_col4, #T_06b9d_row6_col5, #T_06b9d_row6_col6, #T_06b9d_row7_col0, #T_06b9d_row7_col1, #T_06b9d_row7_col2, #T_06b9d_row7_col3, #T_06b9d_row7_col4, #T_06b9d_row7_col5, #T_06b9d_row7_col6, #T_06b9d_row8_col0, #T_06b9d_row8_col1, #T_06b9d_row8_col2, #T_06b9d_row8_col3, #T_06b9d_row8_col4, #T_06b9d_row8_col5, #T_06b9d_row8_col6, #T_06b9d_row9_col0, #T_06b9d_row9_col1, #T_06b9d_row9_col2, #T_06b9d_row9_col3, #T_06b9d_row9_col4, #T_06b9d_row9_col5, #T_06b9d_row9_col6, #T_06b9d_row10_col0, #T_06b9d_row10_col1, #T_06b9d_row10_col2, #T_06b9d_row10_col3, #T_06b9d_row10_col4, #T_06b9d_row10_col5, #T_06b9d_row10_col6, #T_06b9d_row11_col0, #T_06b9d_row11_col1, #T_06b9d_row11_col2, #T_06b9d_row11_col3, #T_06b9d_row11_col4, #T_06b9d_row11_col5, #T_06b9d_row11_col6, #T_06b9d_row12_col0, #T_06b9d_row12_col1, #T_06b9d_row12_col2, #T_06b9d_row12_col3, #T_06b9d_row12_col4, #T_06b9d_row12_col5, #T_06b9d_row12_col6, #T_06b9d_row13_col0, #T_06b9d_row13_col1, #T_06b9d_row13_col2, #T_06b9d_row13_col3, #T_06b9d_row13_col4, #T_06b9d_row13_col5, #T_06b9d_row13_col6, #T_06b9d_row14_col0, #T_06b9d_row14_col1, #T_06b9d_row14_col2, #T_06b9d_row14_col3, #T_06b9d_row14_col4, #T_06b9d_row14_col5, #T_06b9d_row14_col6, #T_06b9d_row15_col0, #T_06b9d_row15_col1, #T_06b9d_row15_col2, #T_06b9d_row15_col3, #T_06b9d_row15_col4, #T_06b9d_row15_col5, #T_06b9d_row15_col6, #T_06b9d_row16_col0, #T_06b9d_row16_col1, #T_06b9d_row16_col2, #T_06b9d_row16_col3, #T_06b9d_row16_col4, #T_06b9d_row16_col5, #T_06b9d_row16_col6, #T_06b9d_row17_col0, #T_06b9d_row17_col1, #T_06b9d_row17_col2, #T_06b9d_row17_col3, #T_06b9d_row17_col4, #T_06b9d_row17_col5, #T_06b9d_row17_col6, #T_06b9d_row18_col0, #T_06b9d_row18_col1, #T_06b9d_row18_col2, #T_06b9d_row18_col3, #T_06b9d_row18_col4, #T_06b9d_row18_col5, #T_06b9d_row18_col6, #T_06b9d_row19_col0, #T_06b9d_row19_col1, #T_06b9d_row19_col2, #T_06b9d_row19_col3, #T_06b9d_row19_col4, #T_06b9d_row19_col5, #T_06b9d_row19_col6 {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_06b9d_row0_col1, #T_06b9d_row0_col6, #T_06b9d_row3_col5, #T_06b9d_row4_col2, #T_06b9d_row4_col3, #T_06b9d_row4_col4 {\n",
              "  text-align: left;\n",
              "  background-color: yellow;\n",
              "}\n",
              "#T_06b9d_row0_col7, #T_06b9d_row1_col7, #T_06b9d_row2_col7, #T_06b9d_row3_col7, #T_06b9d_row4_col7, #T_06b9d_row5_col7, #T_06b9d_row6_col7, #T_06b9d_row7_col7, #T_06b9d_row8_col7, #T_06b9d_row9_col7, #T_06b9d_row10_col7, #T_06b9d_row11_col7, #T_06b9d_row12_col7, #T_06b9d_row13_col7, #T_06b9d_row14_col7, #T_06b9d_row15_col7, #T_06b9d_row17_col7, #T_06b9d_row18_col7, #T_06b9d_row19_col7 {\n",
              "  text-align: left;\n",
              "  background-color: lightgrey;\n",
              "}\n",
              "#T_06b9d_row16_col7 {\n",
              "  text-align: left;\n",
              "  background-color: yellow;\n",
              "  background-color: lightgrey;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_06b9d\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_06b9d_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_06b9d_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
              "      <th id=\"T_06b9d_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
              "      <th id=\"T_06b9d_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
              "      <th id=\"T_06b9d_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
              "      <th id=\"T_06b9d_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
              "      <th id=\"T_06b9d_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
              "      <th id=\"T_06b9d_level0_col7\" class=\"col_heading level0 col7\" >TT (Sec)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row0\" class=\"row_heading level0 row0\" >huber</th>\n",
              "      <td id=\"T_06b9d_row0_col0\" class=\"data row0 col0\" >Huber Regressor</td>\n",
              "      <td id=\"T_06b9d_row0_col1\" class=\"data row0 col1\" >54.9854</td>\n",
              "      <td id=\"T_06b9d_row0_col2\" class=\"data row0 col2\" >29459.0022</td>\n",
              "      <td id=\"T_06b9d_row0_col3\" class=\"data row0 col3\" >171.6006</td>\n",
              "      <td id=\"T_06b9d_row0_col4\" class=\"data row0 col4\" >-0.0167</td>\n",
              "      <td id=\"T_06b9d_row0_col5\" class=\"data row0 col5\" >1.6046</td>\n",
              "      <td id=\"T_06b9d_row0_col6\" class=\"data row0 col6\" >1.5096</td>\n",
              "      <td id=\"T_06b9d_row0_col7\" class=\"data row0 col7\" >8.5900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row1\" class=\"row_heading level0 row1\" >et</th>\n",
              "      <td id=\"T_06b9d_row1_col0\" class=\"data row1 col0\" >Extra Trees Regressor</td>\n",
              "      <td id=\"T_06b9d_row1_col1\" class=\"data row1 col1\" >55.6215</td>\n",
              "      <td id=\"T_06b9d_row1_col2\" class=\"data row1 col2\" >23147.8583</td>\n",
              "      <td id=\"T_06b9d_row1_col3\" class=\"data row1 col3\" >152.1201</td>\n",
              "      <td id=\"T_06b9d_row1_col4\" class=\"data row1 col4\" >0.2009</td>\n",
              "      <td id=\"T_06b9d_row1_col5\" class=\"data row1 col5\" >1.1782</td>\n",
              "      <td id=\"T_06b9d_row1_col6\" class=\"data row1 col6\" >7.9439</td>\n",
              "      <td id=\"T_06b9d_row1_col7\" class=\"data row1 col7\" >49.7030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row2\" class=\"row_heading level0 row2\" >lightgbm</th>\n",
              "      <td id=\"T_06b9d_row2_col0\" class=\"data row2 col0\" >Light Gradient Boosting Machine</td>\n",
              "      <td id=\"T_06b9d_row2_col1\" class=\"data row2 col1\" >56.1851</td>\n",
              "      <td id=\"T_06b9d_row2_col2\" class=\"data row2 col2\" >23288.6768</td>\n",
              "      <td id=\"T_06b9d_row2_col3\" class=\"data row2 col3\" >152.5767</td>\n",
              "      <td id=\"T_06b9d_row2_col4\" class=\"data row2 col4\" >0.1961</td>\n",
              "      <td id=\"T_06b9d_row2_col5\" class=\"data row2 col5\" >1.3281</td>\n",
              "      <td id=\"T_06b9d_row2_col6\" class=\"data row2 col6\" >8.2002</td>\n",
              "      <td id=\"T_06b9d_row2_col7\" class=\"data row2 col7\" >3.1570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row3\" class=\"row_heading level0 row3\" >rf</th>\n",
              "      <td id=\"T_06b9d_row3_col0\" class=\"data row3 col0\" >Random Forest Regressor</td>\n",
              "      <td id=\"T_06b9d_row3_col1\" class=\"data row3 col1\" >56.2211</td>\n",
              "      <td id=\"T_06b9d_row3_col2\" class=\"data row3 col2\" >23626.3472</td>\n",
              "      <td id=\"T_06b9d_row3_col3\" class=\"data row3 col3\" >153.6832</td>\n",
              "      <td id=\"T_06b9d_row3_col4\" class=\"data row3 col4\" >0.1843</td>\n",
              "      <td id=\"T_06b9d_row3_col5\" class=\"data row3 col5\" >1.1180</td>\n",
              "      <td id=\"T_06b9d_row3_col6\" class=\"data row3 col6\" >9.1540</td>\n",
              "      <td id=\"T_06b9d_row3_col7\" class=\"data row3 col7\" >74.7390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row4\" class=\"row_heading level0 row4\" >catboost</th>\n",
              "      <td id=\"T_06b9d_row4_col0\" class=\"data row4 col0\" >CatBoost Regressor</td>\n",
              "      <td id=\"T_06b9d_row4_col1\" class=\"data row4 col1\" >56.5754</td>\n",
              "      <td id=\"T_06b9d_row4_col2\" class=\"data row4 col2\" >22502.2461</td>\n",
              "      <td id=\"T_06b9d_row4_col3\" class=\"data row4 col3\" >149.9807</td>\n",
              "      <td id=\"T_06b9d_row4_col4\" class=\"data row4 col4\" >0.2232</td>\n",
              "      <td id=\"T_06b9d_row4_col5\" class=\"data row4 col5\" >1.5418</td>\n",
              "      <td id=\"T_06b9d_row4_col6\" class=\"data row4 col6\" >7.7854</td>\n",
              "      <td id=\"T_06b9d_row4_col7\" class=\"data row4 col7\" >7.3630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row5\" class=\"row_heading level0 row5\" >xgboost</th>\n",
              "      <td id=\"T_06b9d_row5_col0\" class=\"data row5 col0\" >Extreme Gradient Boosting</td>\n",
              "      <td id=\"T_06b9d_row5_col1\" class=\"data row5 col1\" >56.9273</td>\n",
              "      <td id=\"T_06b9d_row5_col2\" class=\"data row5 col2\" >22891.6742</td>\n",
              "      <td id=\"T_06b9d_row5_col3\" class=\"data row5 col3\" >151.2825</td>\n",
              "      <td id=\"T_06b9d_row5_col4\" class=\"data row5 col4\" >0.2096</td>\n",
              "      <td id=\"T_06b9d_row5_col5\" class=\"data row5 col5\" >1.5614</td>\n",
              "      <td id=\"T_06b9d_row5_col6\" class=\"data row5 col6\" >7.0685</td>\n",
              "      <td id=\"T_06b9d_row5_col7\" class=\"data row5 col7\" >3.1840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row6\" class=\"row_heading level0 row6\" >gbr</th>\n",
              "      <td id=\"T_06b9d_row6_col0\" class=\"data row6 col0\" >Gradient Boosting Regressor</td>\n",
              "      <td id=\"T_06b9d_row6_col1\" class=\"data row6 col1\" >59.6339</td>\n",
              "      <td id=\"T_06b9d_row6_col2\" class=\"data row6 col2\" >24076.1137</td>\n",
              "      <td id=\"T_06b9d_row6_col3\" class=\"data row6 col3\" >155.1372</td>\n",
              "      <td id=\"T_06b9d_row6_col4\" class=\"data row6 col4\" >0.1689</td>\n",
              "      <td id=\"T_06b9d_row6_col5\" class=\"data row6 col5\" >1.5893</td>\n",
              "      <td id=\"T_06b9d_row6_col6\" class=\"data row6 col6\" >8.2068</td>\n",
              "      <td id=\"T_06b9d_row6_col7\" class=\"data row6 col7\" >77.8910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row7\" class=\"row_heading level0 row7\" >knn</th>\n",
              "      <td id=\"T_06b9d_row7_col0\" class=\"data row7 col0\" >K Neighbors Regressor</td>\n",
              "      <td id=\"T_06b9d_row7_col1\" class=\"data row7 col1\" >63.9900</td>\n",
              "      <td id=\"T_06b9d_row7_col2\" class=\"data row7 col2\" >26721.4156</td>\n",
              "      <td id=\"T_06b9d_row7_col3\" class=\"data row7 col3\" >163.4444</td>\n",
              "      <td id=\"T_06b9d_row7_col4\" class=\"data row7 col4\" >0.0774</td>\n",
              "      <td id=\"T_06b9d_row7_col5\" class=\"data row7 col5\" >1.8694</td>\n",
              "      <td id=\"T_06b9d_row7_col6\" class=\"data row7 col6\" >5.2598</td>\n",
              "      <td id=\"T_06b9d_row7_col7\" class=\"data row7 col7\" >9.2520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row8\" class=\"row_heading level0 row8\" >dt</th>\n",
              "      <td id=\"T_06b9d_row8_col0\" class=\"data row8 col0\" >Decision Tree Regressor</td>\n",
              "      <td id=\"T_06b9d_row8_col1\" class=\"data row8 col1\" >68.3752</td>\n",
              "      <td id=\"T_06b9d_row8_col2\" class=\"data row8 col2\" >40239.4473</td>\n",
              "      <td id=\"T_06b9d_row8_col3\" class=\"data row8 col3\" >200.5720</td>\n",
              "      <td id=\"T_06b9d_row8_col4\" class=\"data row8 col4\" >-0.3907</td>\n",
              "      <td id=\"T_06b9d_row8_col5\" class=\"data row8 col5\" >1.2516</td>\n",
              "      <td id=\"T_06b9d_row8_col6\" class=\"data row8 col6\" >9.7241</td>\n",
              "      <td id=\"T_06b9d_row8_col7\" class=\"data row8 col7\" >7.4880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row9\" class=\"row_heading level0 row9\" >lr</th>\n",
              "      <td id=\"T_06b9d_row9_col0\" class=\"data row9 col0\" >Linear Regression</td>\n",
              "      <td id=\"T_06b9d_row9_col1\" class=\"data row9 col1\" >77.8328</td>\n",
              "      <td id=\"T_06b9d_row9_col2\" class=\"data row9 col2\" >26337.2801</td>\n",
              "      <td id=\"T_06b9d_row9_col3\" class=\"data row9 col3\" >162.2622</td>\n",
              "      <td id=\"T_06b9d_row9_col4\" class=\"data row9 col4\" >0.0908</td>\n",
              "      <td id=\"T_06b9d_row9_col5\" class=\"data row9 col5\" >2.5952</td>\n",
              "      <td id=\"T_06b9d_row9_col6\" class=\"data row9 col6\" >7.7996</td>\n",
              "      <td id=\"T_06b9d_row9_col7\" class=\"data row9 col7\" >3.0120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row10\" class=\"row_heading level0 row10\" >ridge</th>\n",
              "      <td id=\"T_06b9d_row10_col0\" class=\"data row10 col0\" >Ridge Regression</td>\n",
              "      <td id=\"T_06b9d_row10_col1\" class=\"data row10 col1\" >77.8912</td>\n",
              "      <td id=\"T_06b9d_row10_col2\" class=\"data row10 col2\" >26347.9121</td>\n",
              "      <td id=\"T_06b9d_row10_col3\" class=\"data row10 col3\" >162.2949</td>\n",
              "      <td id=\"T_06b9d_row10_col4\" class=\"data row10 col4\" >0.0904</td>\n",
              "      <td id=\"T_06b9d_row10_col5\" class=\"data row10 col5\" >2.5958</td>\n",
              "      <td id=\"T_06b9d_row10_col6\" class=\"data row10 col6\" >7.7714</td>\n",
              "      <td id=\"T_06b9d_row10_col7\" class=\"data row10 col7\" >2.3000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row11\" class=\"row_heading level0 row11\" >br</th>\n",
              "      <td id=\"T_06b9d_row11_col0\" class=\"data row11 col0\" >Bayesian Ridge</td>\n",
              "      <td id=\"T_06b9d_row11_col1\" class=\"data row11 col1\" >77.9080</td>\n",
              "      <td id=\"T_06b9d_row11_col2\" class=\"data row11 col2\" >26346.7133</td>\n",
              "      <td id=\"T_06b9d_row11_col3\" class=\"data row11 col3\" >162.2913</td>\n",
              "      <td id=\"T_06b9d_row11_col4\" class=\"data row11 col4\" >0.0905</td>\n",
              "      <td id=\"T_06b9d_row11_col5\" class=\"data row11 col5\" >2.5967</td>\n",
              "      <td id=\"T_06b9d_row11_col6\" class=\"data row11 col6\" >7.7707</td>\n",
              "      <td id=\"T_06b9d_row11_col7\" class=\"data row11 col7\" >2.9590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row12\" class=\"row_heading level0 row12\" >lasso</th>\n",
              "      <td id=\"T_06b9d_row12_col0\" class=\"data row12 col0\" >Lasso Regression</td>\n",
              "      <td id=\"T_06b9d_row12_col1\" class=\"data row12 col1\" >78.1307</td>\n",
              "      <td id=\"T_06b9d_row12_col2\" class=\"data row12 col2\" >26366.9942</td>\n",
              "      <td id=\"T_06b9d_row12_col3\" class=\"data row12 col3\" >162.3535</td>\n",
              "      <td id=\"T_06b9d_row12_col4\" class=\"data row12 col4\" >0.0898</td>\n",
              "      <td id=\"T_06b9d_row12_col5\" class=\"data row12 col5\" >2.6125</td>\n",
              "      <td id=\"T_06b9d_row12_col6\" class=\"data row12 col6\" >7.6131</td>\n",
              "      <td id=\"T_06b9d_row12_col7\" class=\"data row12 col7\" >4.1160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row13\" class=\"row_heading level0 row13\" >llar</th>\n",
              "      <td id=\"T_06b9d_row13_col0\" class=\"data row13 col0\" >Lasso Least Angle Regression</td>\n",
              "      <td id=\"T_06b9d_row13_col1\" class=\"data row13 col1\" >78.1307</td>\n",
              "      <td id=\"T_06b9d_row13_col2\" class=\"data row13 col2\" >26366.9942</td>\n",
              "      <td id=\"T_06b9d_row13_col3\" class=\"data row13 col3\" >162.3535</td>\n",
              "      <td id=\"T_06b9d_row13_col4\" class=\"data row13 col4\" >0.0898</td>\n",
              "      <td id=\"T_06b9d_row13_col5\" class=\"data row13 col5\" >2.6125</td>\n",
              "      <td id=\"T_06b9d_row13_col6\" class=\"data row13 col6\" >7.6131</td>\n",
              "      <td id=\"T_06b9d_row13_col7\" class=\"data row13 col7\" >2.2910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row14\" class=\"row_heading level0 row14\" >en</th>\n",
              "      <td id=\"T_06b9d_row14_col0\" class=\"data row14 col0\" >Elastic Net</td>\n",
              "      <td id=\"T_06b9d_row14_col1\" class=\"data row14 col1\" >78.3844</td>\n",
              "      <td id=\"T_06b9d_row14_col2\" class=\"data row14 col2\" >26407.7236</td>\n",
              "      <td id=\"T_06b9d_row14_col3\" class=\"data row14 col3\" >162.4787</td>\n",
              "      <td id=\"T_06b9d_row14_col4\" class=\"data row14 col4\" >0.0884</td>\n",
              "      <td id=\"T_06b9d_row14_col5\" class=\"data row14 col5\" >2.6237</td>\n",
              "      <td id=\"T_06b9d_row14_col6\" class=\"data row14 col6\" >7.5547</td>\n",
              "      <td id=\"T_06b9d_row14_col7\" class=\"data row14 col7\" >4.2640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row15\" class=\"row_heading level0 row15\" >omp</th>\n",
              "      <td id=\"T_06b9d_row15_col0\" class=\"data row15 col0\" >Orthogonal Matching Pursuit</td>\n",
              "      <td id=\"T_06b9d_row15_col1\" class=\"data row15 col1\" >79.3966</td>\n",
              "      <td id=\"T_06b9d_row15_col2\" class=\"data row15 col2\" >26618.8979</td>\n",
              "      <td id=\"T_06b9d_row15_col3\" class=\"data row15 col3\" >163.1271</td>\n",
              "      <td id=\"T_06b9d_row15_col4\" class=\"data row15 col4\" >0.0811</td>\n",
              "      <td id=\"T_06b9d_row15_col5\" class=\"data row15 col5\" >2.6550</td>\n",
              "      <td id=\"T_06b9d_row15_col6\" class=\"data row15 col6\" >7.5913</td>\n",
              "      <td id=\"T_06b9d_row15_col7\" class=\"data row15 col7\" >2.3310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row16\" class=\"row_heading level0 row16\" >dummy</th>\n",
              "      <td id=\"T_06b9d_row16_col0\" class=\"data row16 col0\" >Dummy Regressor</td>\n",
              "      <td id=\"T_06b9d_row16_col1\" class=\"data row16 col1\" >80.3622</td>\n",
              "      <td id=\"T_06b9d_row16_col2\" class=\"data row16 col2\" >28973.5682</td>\n",
              "      <td id=\"T_06b9d_row16_col3\" class=\"data row16 col3\" >170.1836</td>\n",
              "      <td id=\"T_06b9d_row16_col4\" class=\"data row16 col4\" >-0.0001</td>\n",
              "      <td id=\"T_06b9d_row16_col5\" class=\"data row16 col5\" >2.8841</td>\n",
              "      <td id=\"T_06b9d_row16_col6\" class=\"data row16 col6\" >7.2459</td>\n",
              "      <td id=\"T_06b9d_row16_col7\" class=\"data row16 col7\" >2.0490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row17\" class=\"row_heading level0 row17\" >ada</th>\n",
              "      <td id=\"T_06b9d_row17_col0\" class=\"data row17 col0\" >AdaBoost Regressor</td>\n",
              "      <td id=\"T_06b9d_row17_col1\" class=\"data row17 col1\" >89.8544</td>\n",
              "      <td id=\"T_06b9d_row17_col2\" class=\"data row17 col2\" >38590.1174</td>\n",
              "      <td id=\"T_06b9d_row17_col3\" class=\"data row17 col3\" >196.4203</td>\n",
              "      <td id=\"T_06b9d_row17_col4\" class=\"data row17 col4\" >-0.3328</td>\n",
              "      <td id=\"T_06b9d_row17_col5\" class=\"data row17 col5\" >1.5051</td>\n",
              "      <td id=\"T_06b9d_row17_col6\" class=\"data row17 col6\" >17.7295</td>\n",
              "      <td id=\"T_06b9d_row17_col7\" class=\"data row17 col7\" >12.2600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row18\" class=\"row_heading level0 row18\" >par</th>\n",
              "      <td id=\"T_06b9d_row18_col0\" class=\"data row18 col0\" >Passive Aggressive Regressor</td>\n",
              "      <td id=\"T_06b9d_row18_col1\" class=\"data row18 col1\" >415.1578</td>\n",
              "      <td id=\"T_06b9d_row18_col2\" class=\"data row18 col2\" >484213.4778</td>\n",
              "      <td id=\"T_06b9d_row18_col3\" class=\"data row18 col3\" >638.9320</td>\n",
              "      <td id=\"T_06b9d_row18_col4\" class=\"data row18 col4\" >-15.6746</td>\n",
              "      <td id=\"T_06b9d_row18_col5\" class=\"data row18 col5\" >3.8738</td>\n",
              "      <td id=\"T_06b9d_row18_col6\" class=\"data row18 col6\" >45.0007</td>\n",
              "      <td id=\"T_06b9d_row18_col7\" class=\"data row18 col7\" >2.5830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_06b9d_level0_row19\" class=\"row_heading level0 row19\" >lar</th>\n",
              "      <td id=\"T_06b9d_row19_col0\" class=\"data row19 col0\" >Least Angle Regression</td>\n",
              "      <td id=\"T_06b9d_row19_col1\" class=\"data row19 col1\" >21984.0250</td>\n",
              "      <td id=\"T_06b9d_row19_col2\" class=\"data row19 col2\" >380444527917.7170</td>\n",
              "      <td id=\"T_06b9d_row19_col3\" class=\"data row19 col3\" >300440.2333</td>\n",
              "      <td id=\"T_06b9d_row19_col4\" class=\"data row19 col4\" >-12674944.7068</td>\n",
              "      <td id=\"T_06b9d_row19_col5\" class=\"data row19 col5\" >4.9152</td>\n",
              "      <td id=\"T_06b9d_row19_col6\" class=\"data row19 col6\" >1966.9274</td>\n",
              "      <td id=\"T_06b9d_row19_col7\" class=\"data row19 col7\" >2.3580</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x28d87c03640>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[HuberRegressor(),\n",
              " ExtraTreesRegressor(n_jobs=-1, random_state=724),\n",
              " LGBMRegressor(device='gpu', n_jobs=-1, random_state=724),\n",
              " RandomForestRegressor(n_jobs=-1, random_state=724),\n",
              " <catboost.core.CatBoostRegressor at 0x28d89c95040>]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best = compare_models(n_select=5, sort='MAE')\n",
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Initiated</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>14:34:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Status</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Searching Hyperparameters</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estimator</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Huber Regressor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                         \n",
              "                                                                         \n",
              "Initiated  . . . . . . . . . . . . . . . . . .                   14:34:39\n",
              "Status     . . . . . . . . . . . . . . . . . .  Searching Hyperparameters\n",
              "Estimator  . . . . . . . . . . . . . . . . . .            Huber Regressor"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6d79c32f9c44b71888d8e4d7d844f7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cat_t \u001b[39m=\u001b[39m tune_model(best, optimize\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m'\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m) \u001b[39m#lgbma\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cat_t\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\functional.py:1197\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1006\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[0;32m   1007\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1026\u001b[0m ):\n\u001b[0;32m   1027\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \n\u001b[0;32m   1195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39mtune_model(\n\u001b[0;32m   1198\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1199\u001b[0m         fold\u001b[39m=\u001b[39mfold,\n\u001b[0;32m   1200\u001b[0m         \u001b[39mround\u001b[39m\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m,\n\u001b[0;32m   1201\u001b[0m         n_iter\u001b[39m=\u001b[39mn_iter,\n\u001b[0;32m   1202\u001b[0m         custom_grid\u001b[39m=\u001b[39mcustom_grid,\n\u001b[0;32m   1203\u001b[0m         optimize\u001b[39m=\u001b[39moptimize,\n\u001b[0;32m   1204\u001b[0m         custom_scorer\u001b[39m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1205\u001b[0m         search_library\u001b[39m=\u001b[39msearch_library,\n\u001b[0;32m   1206\u001b[0m         search_algorithm\u001b[39m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1207\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m   1208\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1209\u001b[0m         choose_better\u001b[39m=\u001b[39mchoose_better,\n\u001b[0;32m   1210\u001b[0m         fit_kwargs\u001b[39m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1211\u001b[0m         groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m   1212\u001b[0m         return_tuner\u001b[39m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1213\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   1214\u001b[0m         tuner_verbose\u001b[39m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1215\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1216\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1217\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\regression\\oop.py:1499\u001b[0m, in \u001b[0;36mRegressionExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[0;32m   1308\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1309\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1328\u001b[0m ):\n\u001b[0;32m   1329\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \n\u001b[0;32m   1497\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1499\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtune_model(\n\u001b[0;32m   1500\u001b[0m         estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m   1501\u001b[0m         fold\u001b[39m=\u001b[39mfold,\n\u001b[0;32m   1502\u001b[0m         \u001b[39mround\u001b[39m\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m,\n\u001b[0;32m   1503\u001b[0m         n_iter\u001b[39m=\u001b[39mn_iter,\n\u001b[0;32m   1504\u001b[0m         custom_grid\u001b[39m=\u001b[39mcustom_grid,\n\u001b[0;32m   1505\u001b[0m         optimize\u001b[39m=\u001b[39moptimize,\n\u001b[0;32m   1506\u001b[0m         custom_scorer\u001b[39m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1507\u001b[0m         search_library\u001b[39m=\u001b[39msearch_library,\n\u001b[0;32m   1508\u001b[0m         search_algorithm\u001b[39m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1509\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m   1510\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1511\u001b[0m         choose_better\u001b[39m=\u001b[39mchoose_better,\n\u001b[0;32m   1512\u001b[0m         fit_kwargs\u001b[39m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1513\u001b[0m         groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m   1514\u001b[0m         return_tuner\u001b[39m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1515\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   1516\u001b[0m         tuner_verbose\u001b[39m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1517\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1518\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1519\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:2665\u001b[0m, in \u001b[0;36m_SupervisedExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   2657\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\n\u001b[0;32m   2658\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._search.sample_without_replacement\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2659\u001b[0m         pycaret\u001b[39m.\u001b[39minternal\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39msklearn\u001b[39m.\u001b[39m_mp_sample_without_replacement,\n\u001b[0;32m   2660\u001b[0m     ):\n\u001b[0;32m   2661\u001b[0m         \u001b[39mwith\u001b[39;00m patch(\n\u001b[0;32m   2662\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._search.ParameterGrid.__getitem__\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2663\u001b[0m             pycaret\u001b[39m.\u001b[39minternal\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39msklearn\u001b[39m.\u001b[39m_mp_ParameterGrid_getitem,\n\u001b[0;32m   2664\u001b[0m         ):\n\u001b[1;32m-> 2665\u001b[0m             model_grid\u001b[39m.\u001b[39mfit(data_X, data_y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m   2666\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2667\u001b[0m     model_grid\u001b[39m.\u001b[39mfit(data_X, data_y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1769\u001b[0m         ParameterSampler(\n\u001b[0;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1771\u001b[0m         )\n\u001b[0;32m   1772\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\caret\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_t = tune_model(best, optimize='MAE', n_iter=100) #lgbma\n",
        "cat_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Initiated</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>17:01:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Status</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Fitting 10 Folds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estimator</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Stacking Regressor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                  \n",
              "                                                                  \n",
              "Initiated  . . . . . . . . . . . . . . . . . .            17:01:36\n",
              "Status     . . . . . . . . . . . . . . . . . .    Fitting 10 Folds\n",
              "Estimator  . . . . . . . . . . . . . . . . . .  Stacking Regressor"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6462f95876a04861ae0550c84d964288",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "stack_lr = stack_models(best, optimize='MAE', choose_better=True)\n",
        "stack_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(memory=Memory(location=None),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;,\n",
              "                                             &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;, &#x27;DUBAI&#x27;,\n",
              "                                             &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                                             &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;,\n",
              "                                             &#x27;rounded_hour_sin&#x27;,\n",
              "                                             &#x27;rounded_hour_cos&#x27;,\n",
              "                                             &#x27;mean_enc_ARI...\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                                    &#x27;BREADTH&#x27;,\n",
              "                                                                    &#x27;DEPTH&#x27;,\n",
              "                                                                    &#x27;DRAUGHT&#x27;,\n",
              "                                                                    &#x27;country_cluster&#x27;,\n",
              "                                                                    &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                             &#x27;PO_Y_M_D&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;,\n",
              "                                                                    &#x27;PO_Y_M_D&#x27;]))),\n",
              "                (&#x27;actual_estimator&#x27;,\n",
              "                 &lt;catboost.core.CatBoostRegressor object at 0x000001F8D5A571C0&gt;)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(memory=Memory(location=None),\n",
              "         steps=[(&#x27;numerical_imputer&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;,\n",
              "                                             &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;, &#x27;DUBAI&#x27;,\n",
              "                                             &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;,\n",
              "                                             &#x27;PORT_SIZE&#x27;, &#x27;year&#x27;, &#x27;month&#x27;,\n",
              "                                             &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                                             &#x27;month_sin&#x27;, &#x27;month_cos&#x27;,\n",
              "                                             &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                                             &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;,\n",
              "                                             &#x27;rounded_hour_sin&#x27;,\n",
              "                                             &#x27;rounded_hour_cos&#x27;,\n",
              "                                             &#x27;mean_enc_ARI...\n",
              "                                                                    &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                                    &#x27;BREADTH&#x27;,\n",
              "                                                                    &#x27;DEPTH&#x27;,\n",
              "                                                                    &#x27;DRAUGHT&#x27;,\n",
              "                                                                    &#x27;country_cluster&#x27;,\n",
              "                                                                    &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                                              handle_missing=&#x27;return_nan&#x27;,\n",
              "                                                              use_cat_names=True))),\n",
              "                (&#x27;rest_encoding&#x27;,\n",
              "                 TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                             &#x27;PO_Y_M_D&#x27;],\n",
              "                                    transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;,\n",
              "                                                                    &#x27;ID&#x27;,\n",
              "                                                                    &#x27;FLAG&#x27;,\n",
              "                                                                    &#x27;PO_Y_M_D&#x27;]))),\n",
              "                (&#x27;actual_estimator&#x27;,\n",
              "                 &lt;catboost.core.CatBoostRegressor object at 0x000001F8D5A571C0&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;DIST&#x27;, &#x27;DEADWEIGHT&#x27;, &#x27;GT&#x27;, &#x27;LENGTH&#x27;, &#x27;ATA_LT&#x27;,\n",
              "                            &#x27;DUBAI&#x27;, &#x27;BRENT&#x27;, &#x27;WTI&#x27;, &#x27;BDI_ADJ&#x27;, &#x27;PORT_SIZE&#x27;,\n",
              "                            &#x27;year&#x27;, &#x27;month&#x27;, &#x27;day&#x27;, &#x27;weekday&#x27;, &#x27;rounded_hour&#x27;,\n",
              "                            &#x27;month_sin&#x27;, &#x27;month_cos&#x27;, &#x27;day_sin&#x27;, &#x27;day_cos&#x27;,\n",
              "                            &#x27;weekday_sin&#x27;, &#x27;weekday_cos&#x27;, &#x27;rounded_hour_sin&#x27;,\n",
              "                            &#x27;rounded_hour_cos&#x27;, &#x27;mean_enc_ARI_CO&#x27;,\n",
              "                            &#x27;std_enc_ARI_CO&#x27;, &#x27;mean_enc_ARI_PO&#x27;,\n",
              "                            &#x27;std_enc_ARI_PO&#x27;, &#x27;mean_enc_year&#x27;, &#x27;std_enc_year&#x27;,\n",
              "                            &#x27;mean_enc_month&#x27;, ...],\n",
              "                   transformer=SimpleImputer())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical_imputer: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;ARI_PO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;ID&#x27;,\n",
              "                            &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;, &#x27;DRAUGHT&#x27;, &#x27;FLAG&#x27;,\n",
              "                            &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;, &#x27;PO_Y_M_D&#x27;],\n",
              "                   transformer=SimpleImputer(strategy=&#x27;most_frequent&#x27;))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\" ><label for=\"sk-estimator-id-49\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehot_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                            &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                   transformer=OneHotEncoder(cols=[&#x27;ARI_CO&#x27;,\n",
              "                                                   &#x27;SHIP_TYPE_CATEGORY&#x27;,\n",
              "                                                   &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                                                   &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;,\n",
              "                                                   &#x27;PORT_SIZE_Zone&#x27;],\n",
              "                                             handle_missing=&#x27;return_nan&#x27;,\n",
              "                                             use_cat_names=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                    &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(cols=[&#x27;ARI_CO&#x27;, &#x27;SHIP_TYPE_CATEGORY&#x27;, &#x27;BREADTH&#x27;, &#x27;DEPTH&#x27;,\n",
              "                    &#x27;DRAUGHT&#x27;, &#x27;country_cluster&#x27;, &#x27;PORT_SIZE_Zone&#x27;],\n",
              "              handle_missing=&#x27;return_nan&#x27;, use_cat_names=True)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">rest_encoding: TransformerWrapper</label><div class=\"sk-toggleable__content\"><pre>TransformerWrapper(include=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;],\n",
              "                   transformer=TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;,\n",
              "                                                   &#x27;PO_Y_M_D&#x27;]))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\" ><label for=\"sk-estimator-id-54\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder(cols=[&#x27;ARI_PO&#x27;, &#x27;ID&#x27;, &#x27;FLAG&#x27;, &#x27;PO_Y_M_D&#x27;])</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" ><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostRegressor object at 0x000001F8D5A571C0&gt;</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(memory=Memory(location=None),\n",
              "         steps=[('numerical_imputer',\n",
              "                 TransformerWrapper(include=['DIST', 'DEADWEIGHT', 'GT',\n",
              "                                             'LENGTH', 'ATA_LT', 'DUBAI',\n",
              "                                             'BRENT', 'WTI', 'BDI_ADJ',\n",
              "                                             'PORT_SIZE', 'year', 'month',\n",
              "                                             'day', 'weekday', 'rounded_hour',\n",
              "                                             'month_sin', 'month_cos',\n",
              "                                             'day_sin', 'day_cos',\n",
              "                                             'weekday_sin', 'weekday_cos',\n",
              "                                             'rounded_hour_sin',\n",
              "                                             'rounded_hour_cos',\n",
              "                                             'mean_enc_ARI...\n",
              "                                                                    'SHIP_TYPE_CATEGORY',\n",
              "                                                                    'BREADTH',\n",
              "                                                                    'DEPTH',\n",
              "                                                                    'DRAUGHT',\n",
              "                                                                    'country_cluster',\n",
              "                                                                    'PORT_SIZE_Zone'],\n",
              "                                                              handle_missing='return_nan',\n",
              "                                                              use_cat_names=True))),\n",
              "                ('rest_encoding',\n",
              "                 TransformerWrapper(include=['ARI_PO', 'ID', 'FLAG',\n",
              "                                             'PO_Y_M_D'],\n",
              "                                    transformer=TargetEncoder(cols=['ARI_PO',\n",
              "                                                                    'ID',\n",
              "                                                                    'FLAG',\n",
              "                                                                    'PO_Y_M_D']))),\n",
              "                ('actual_estimator',\n",
              "                 <catboost.core.CatBoostRegressor object at 0x000001F8D5A571C0>)])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stack_finalized = finalize_model(cat_t)\n",
        "stack_finalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>90.917114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>339.572906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>24.792130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>130.493073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>383.237549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>1316.202637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID      CI_HOUR\n",
              "0       TEST_000000    90.917114\n",
              "1       TEST_000001   339.572906\n",
              "2       TEST_000002     0.000000\n",
              "3       TEST_000003     0.000000\n",
              "4       TEST_000004    24.792130\n",
              "...             ...          ...\n",
              "244984  TEST_244984   130.493073\n",
              "244985  TEST_244985   383.237549\n",
              "244986  TEST_244986     0.000000\n",
              "244987  TEST_244987     0.000000\n",
              "244988  TEST_244988  1316.202637\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission = pd.read_csv('(f)bestaccuracy_nofit_scaled_sample_submission.csv')\n",
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('(f)bestaccuracy_nofit_scaled_sample_submission_2.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>98.878138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>206.534507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>67.811210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>29.505385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>205.425372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>954.309087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID     CI_HOUR\n",
              "0       TEST_000000   98.878138\n",
              "1       TEST_000001  206.534507\n",
              "2       TEST_000002    0.000000\n",
              "3       TEST_000003    0.000000\n",
              "4       TEST_000004   67.811210\n",
              "...             ...         ...\n",
              "244984  TEST_244984   29.505385\n",
              "244985  TEST_244985  205.425372\n",
              "244986  TEST_244986    0.000000\n",
              "244987  TEST_244987    0.000000\n",
              "244988  TEST_244988  954.309087\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission['CI_HOUR'] = stack_finalized.predict(test)\n",
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('pycat.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoML directory: AutoML_1\n",
            "The task is regression with evaluation metric user_defined_metric\n",
            "AutoML will use algorithms: ['LightGBM', 'Xgboost', 'CatBoost']\n",
            "AutoML will stack models\n",
            "AutoML will ensemble available models\n",
            "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
            "* Step adjust_validation will try to check up to 1 model\n",
            "1_DecisionTree user_defined_metric 79.057654 trained in 0.52 seconds\n",
            "Disable stacking for split validation\n",
            "Skip simple_algorithms because no parameters were generated.\n",
            "* Step default_algorithms will try to check up to 3 models\n",
            "There was an error during 2_Default_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 3_Default_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 4_Default_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "* Step not_so_random will try to check up to 27 models\n",
            "There was an error during 11_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 2_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 20_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 12_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 3_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 21_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 13_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 4_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 22_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 14_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 5_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 23_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 15_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 6_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 24_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 16_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 7_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 25_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 17_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 8_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 26_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 18_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 9_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 27_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 19_LightGBM training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 10_Xgboost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "There was an error during 28_CatBoost training.\n",
            "Please check AutoML_1\\errors.md for details.\n",
            "* Step golden_features will try to check up to 1 model\n",
            "None 10\n",
            "Add Golden Feature: month_sin_ratio_BDI_ADJ\n",
            "Add Golden Feature: month_sin_sum_DUBAI\n",
            "Add Golden Feature: day_sin_multiply_WTI\n",
            "Add Golden Feature: BRENT_ratio_day_sin\n",
            "Add Golden Feature: month_sin_multiply_DUBAI\n",
            "Add Golden Feature: day_sin_multiply_BRENT\n",
            "Add Golden Feature: month_sin_multiply_BRENT\n",
            "Add Golden Feature: day_sin_ratio_BRENT\n",
            "Add Golden Feature: day_sin_multiply_DUBAI\n",
            "Add Golden Feature: DUBAI_ratio_WTI\n",
            "Created 10 Golden Features in 6.82 seconds.\n",
            "1_DecisionTree_GoldenFeatures user_defined_metric 79.345917 trained in 8.02 seconds\n",
            "* Step kmeans_features will try to check up to 1 model\n",
            "1_DecisionTree_KMeansFeatures user_defined_metric 79.945139 trained in 2.93 seconds\n",
            "* Step insert_random_feature will try to check up to 1 model\n",
            "1_DecisionTree_RandomFeature user_defined_metric 79.057654 trained in 1.12 seconds\n",
            "Drop features ['BRENT', 'day_sin', 'random_feature']\n",
            "Skip features_selection because no parameters were generated.\n",
            "* Step hill_climbing_1 will try to check up to 3 models\n",
            "2_DecisionTree user_defined_metric 79.057654 trained in 0.59 seconds\n",
            "3_DecisionTree_GoldenFeatures user_defined_metric 79.345917 trained in 1.51 seconds\n",
            "4_DecisionTree user_defined_metric 79.945139 trained in 3.12 seconds\n",
            "* Step hill_climbing_2 will try to check up to 3 models\n",
            "5_DecisionTree user_defined_metric 79.634907 trained in 0.58 seconds\n",
            "6_DecisionTree user_defined_metric 79.634907 trained in 0.57 seconds\n",
            "7_DecisionTree_GoldenFeatures user_defined_metric 79.569052 trained in 1.16 seconds\n",
            "* Step ensemble will try to check up to 1 model\n",
            "Ensemble user_defined_metric 78.900765 trained in 0.33 seconds\n",
            "AutoML fit time: 36.89 seconds\n",
            "AutoML best model: Ensemble\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([63.11427878, 77.93100383, 63.11427878, ..., 59.73600808,\n",
              "       63.11427878, 63.11427878])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from supervised.automl import AutoML\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "'''\n",
        "automl = AutoML(\n",
        "    algorithms=[\"CatBoost\", \"Xgboost\", \"LightGBM\"],\n",
        "    model_time_limit=30*60,\n",
        "    start_random_models=10,\n",
        "    hill_climbing_steps=3,\n",
        "    top_models_to_improve=3,\n",
        "    golden_features=True,\n",
        "    features_selection=False,\n",
        "    stack_models=True,\n",
        "    train_ensemble=True,\n",
        "    explain_level=0,\n",
        "    validation_strategy={\n",
        "        \"validation_type\": \"kfold\",\n",
        "        \"k_folds\": 4,\n",
        "        \"shuffle\": False,\n",
        "        \"stratify\": True,\n",
        "    }\n",
        ")\n",
        "'''\n",
        "automl = AutoML(mode = \"Compete\",\n",
        "                algorithms = ['LightGBM', 'Xgboost', 'CatBoost'], golden_features=True,\n",
        "                ml_task = \"regression\", eval_metric=mean_absolute_error, random_state = 42, total_time_limit=100)\n",
        "\n",
        "automl.fit(train_x, train_y)\n",
        "\n",
        "predictions = automl.predict(test)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# h2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h2o in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (3.42.0.3)\n",
            "Requirement already satisfied: requests in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from h2o) (2.30.0)\n",
            "Requirement already satisfied: tabulate in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from requests->h2o) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from requests->h2o) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from requests->h2o) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages (from requests->h2o) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install h2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n"
          ]
        },
        {
          "ename": "H2OStartupError",
          "evalue": "Cannot find Java. Please install the latest JRE from\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mH2OConnectionError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\h2o.py:268\u001b[0m, in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     h2oconn \u001b[39m=\u001b[39m H2OConnection\u001b[39m.\u001b[39;49mopen(url\u001b[39m=\u001b[39;49murl, ip\u001b[39m=\u001b[39;49mip, port\u001b[39m=\u001b[39;49mport, name\u001b[39m=\u001b[39;49mname, https\u001b[39m=\u001b[39;49mhttps,\n\u001b[0;32m    269\u001b[0m                                  verify_ssl_certificates\u001b[39m=\u001b[39;49mverify_ssl_certificates, cacert\u001b[39m=\u001b[39;49mcacert,\n\u001b[0;32m    270\u001b[0m                                  auth\u001b[39m=\u001b[39;49mauth, proxy\u001b[39m=\u001b[39;49mproxy, cookies\u001b[39m=\u001b[39;49mcookies, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    271\u001b[0m                                  msgs\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mChecking whether there is an H2O instance running at \u001b[39;49m\u001b[39m{url}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    272\u001b[0m                                        \u001b[39m\"\u001b[39;49m\u001b[39mconnected.\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mnot found.\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    273\u001b[0m                                  strict_version_check\u001b[39m=\u001b[39;49msvc)\n\u001b[0;32m    274\u001b[0m \u001b[39mexcept\u001b[39;00m H2OConnectionError:\n\u001b[0;32m    275\u001b[0m     \u001b[39m# Backward compatibility: in init() port parameter really meant \"baseport\" when starting a local server...\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\connection.py:406\u001b[0m, in \u001b[0;36mH2OConnection.open\u001b[1;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, cacert, proxy, cookies, verbose, msgs, strict_version_check)\u001b[0m\n\u001b[0;32m    405\u001b[0m conn\u001b[39m.\u001b[39m_timeout \u001b[39m=\u001b[39m \u001b[39m3.0\u001b[39m\n\u001b[1;32m--> 406\u001b[0m conn\u001b[39m.\u001b[39m_cluster \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49m_test_connection(retries, messages\u001b[39m=\u001b[39;49mmsgs)\n\u001b[0;32m    407\u001b[0m \u001b[39m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# setting for now, for no good reason other than to ignore all those bugs :(\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\connection.py:713\u001b[0m, in \u001b[0;36mH2OConnection._test_connection\u001b[1;34m(self, max_retries, messages)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 713\u001b[0m     \u001b[39mraise\u001b[39;00m H2OConnectionError(\u001b[39m\"\u001b[39m\u001b[39mCould not establish link to the H2O cloud \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m after \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m retries\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    714\u001b[0m                              \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_url, max_retries, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(errors)))\n",
            "\u001b[1;31mH2OConnectionError\u001b[0m: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries\n[26:18.53] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025451CE2EE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n[26:22.83] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000254527547F0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n[26:27.10] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002545275F160>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n[26:31.42] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002545275FA60>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n[26:35.72] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002545542A400>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mH2OStartupError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mh2o\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mh2o\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautoml\u001b[39;00m \u001b[39mimport\u001b[39;00m H2OAutoML\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m h2o\u001b[39m.\u001b[39;49minit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Assuming your data is in a pandas DataFrame called 'df'\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m h2o_frame \u001b[39m=\u001b[39m h2o\u001b[39m.\u001b[39mH2OFrame(train)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\h2o.py:285\u001b[0m, in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m https:\n\u001b[0;32m    282\u001b[0m         \u001b[39mraise\u001b[39;00m H2OConnectionError(\u001b[39m'\u001b[39m\u001b[39mStarting local server is not available with https enabled. You may start local\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    283\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39m instance of H2O with https manually \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    284\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39m(https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#new-user-quick-start).\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 285\u001b[0m     hs \u001b[39m=\u001b[39m H2OLocalServer\u001b[39m.\u001b[39;49mstart(nthreads\u001b[39m=\u001b[39;49mnthreads, enable_assertions\u001b[39m=\u001b[39;49menable_assertions, max_mem_size\u001b[39m=\u001b[39;49mmmax,\n\u001b[0;32m    286\u001b[0m                               min_mem_size\u001b[39m=\u001b[39;49mmmin, ice_root\u001b[39m=\u001b[39;49mice_root, log_dir\u001b[39m=\u001b[39;49mlog_dir, log_level\u001b[39m=\u001b[39;49mlog_level,\n\u001b[0;32m    287\u001b[0m                               max_log_file_size\u001b[39m=\u001b[39;49mmax_log_file_size, port\u001b[39m=\u001b[39;49mport, name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    288\u001b[0m                               extra_classpath\u001b[39m=\u001b[39;49mextra_classpath, jvm_custom_args\u001b[39m=\u001b[39;49mjvm_custom_args,\n\u001b[0;32m    289\u001b[0m                               bind_to_localhost\u001b[39m=\u001b[39;49mbind_to_localhost)\n\u001b[0;32m    290\u001b[0m     h2oconn \u001b[39m=\u001b[39m H2OConnection\u001b[39m.\u001b[39mopen(server\u001b[39m=\u001b[39mhs, https\u001b[39m=\u001b[39mhttps, verify_ssl_certificates\u001b[39m=\u001b[39mverify_ssl_certificates,\n\u001b[0;32m    291\u001b[0m                                  cacert\u001b[39m=\u001b[39mcacert, auth\u001b[39m=\u001b[39mauth, proxy\u001b[39m=\u001b[39mproxy, cookies\u001b[39m=\u001b[39mcookies, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m                                  strict_version_check\u001b[39m=\u001b[39msvc)\n\u001b[0;32m    293\u001b[0m h2oconn\u001b[39m.\u001b[39mcluster\u001b[39m.\u001b[39mtimezone \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUTC\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\server.py:139\u001b[0m, in \u001b[0;36mH2OLocalServer.start\u001b[1;34m(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, log_dir, log_level, max_log_file_size, port, name, extra_classpath, verbose, jvm_custom_args, bind_to_localhost)\u001b[0m\n\u001b[0;32m    136\u001b[0m     hs\u001b[39m.\u001b[39m_tempdir \u001b[39m=\u001b[39m hs\u001b[39m.\u001b[39m_ice_root\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAttempting to start a local H2O server...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m hs\u001b[39m.\u001b[39;49m_launch_server(port\u001b[39m=\u001b[39;49mport, baseport\u001b[39m=\u001b[39;49mbaseport, nthreads\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(nthreads), ea\u001b[39m=\u001b[39;49menable_assertions,\n\u001b[0;32m    140\u001b[0m                   mmax\u001b[39m=\u001b[39;49mmax_mem_size, mmin\u001b[39m=\u001b[39;49mmin_mem_size, jvm_custom_args\u001b[39m=\u001b[39;49mjvm_custom_args,\n\u001b[0;32m    141\u001b[0m                   bind_to_localhost\u001b[39m=\u001b[39;49mbind_to_localhost, log_dir\u001b[39m=\u001b[39;49mlog_dir, log_level\u001b[39m=\u001b[39;49mlog_level, max_log_file_size\u001b[39m=\u001b[39;49mmax_log_file_size)\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  Server is running at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m://\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (hs\u001b[39m.\u001b[39mscheme, hs\u001b[39m.\u001b[39mip, hs\u001b[39m.\u001b[39mport))\n\u001b[0;32m    143\u001b[0m atexit\u001b[39m.\u001b[39mregister(\u001b[39mlambda\u001b[39;00m: hs\u001b[39m.\u001b[39mshutdown())\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\server.py:271\u001b[0m, in \u001b[0;36mH2OLocalServer._launch_server\u001b[1;34m(self, port, baseport, mmax, mmin, ea, nthreads, jvm_custom_args, bind_to_localhost, log_dir, log_level, max_log_file_size)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ip \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m127.0.0.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m \u001b[39m# Find Java and check version. (Note that subprocess.check_output returns the output as a bytes object)\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m java \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_java()\n\u001b[0;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_java(java, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose)\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\h2o\\backend\\server.py:443\u001b[0m, in \u001b[0;36mH2OLocalServer._find_java\u001b[1;34m()\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dirpath, java)\n\u001b[0;32m    442\u001b[0m \u001b[39m# not found...\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m \u001b[39mraise\u001b[39;00m H2OStartupError(\u001b[39m\"\u001b[39m\u001b[39mCannot find Java. Please install the latest JRE from\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mH2OStartupError\u001b[0m: Cannot find Java. Please install the latest JRE from\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements"
          ]
        }
      ],
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "h2o.init()\n",
        "# Assuming your data is in a pandas DataFrame called 'df'\n",
        "h2o_frame = h2o.H2OFrame(train)\n",
        "target = 'CI_HOUR'\n",
        "features = h2o_frame.columns\n",
        "features.remove(target)\n",
        "# Set up the AutoML object\n",
        "aml = H2OAutoML(max_runtime_secs=3600, # Adjust as needed\n",
        "                seed=313,\n",
        "                stopping_metric=\"MAE\", # Setting MAE as the stopping metric\n",
        "                sort_metric=\"MAE\")     # Setting MAE as the sorting metric for the leaderboard\n",
        "\n",
        "# Train the model\n",
        "aml.train(x=features, y=target, training_frame=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lb = aml.leaderboard\n",
        "print(lb.head(rows=lb.nrows))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = aml.predict(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mae = h2o.h2o.mae(preds, test[target])\n",
        "print(f\"Mean Absolute Error on Test Set: {mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h2o.cluster().shutdown()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "#import xgboost as xgb\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def objective(trial: Trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "            'iterations':trial.suggest_int(\"iterations\", 300, 1000),\n",
        "            'learning_rate' : trial.suggest_uniform('learning_rate',0.1, 1),\n",
        "            'depth': trial.suggest_int('depth',5, 16),\n",
        "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
        "            'reg_lambda': trial.suggest_uniform('reg_lambda',30,100),\n",
        "            'subsample': trial.suggest_uniform('subsample',0.3,1),\n",
        "            'random_strength': trial.suggest_uniform('random_strength',10,100),\n",
        "            'od_wait':trial.suggest_int('od_wait', 10, 150),\n",
        "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,20),\n",
        "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 1, 100),\n",
        "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0., 1.0),\n",
        "            'random_state' : 313,\n",
        "            'verbose' : 0,\n",
        "        }\n",
        "    #'task_type' : 'GPU',\n",
        "    #\"eval_metric\":'RMSE',\n",
        "    cat = CatBoostRegressor(**params)\n",
        "    cat.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val,y_val)],cat_features=cat_features,\n",
        "              verbose=False)\n",
        "    cat_pred = cat.predict(X_val)\n",
        "    score = mean_absolute_error(y_val, cat_pred)\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import bisect\n",
        "'''\n",
        "# Categorical 컬럼 인코딩\n",
        "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "encoders = {}\n",
        "\n",
        "for feature in tqdm(categorical_features, desc=\"Encoding features\"):\n",
        "    le = LabelEncoder()\n",
        "    train[feature] = le.fit_transform(train[feature].astype(str))\n",
        "    le_classes_set = set(le.classes_)\n",
        "    test[feature] = test[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
        "    le_classes = le.classes_.tolist()\n",
        "    bisect.insort_left(le_classes, '-1')\n",
        "    le.classes_ = np.array(le_classes)\n",
        "    test[feature] = le.transform(test[feature].astype(str))\n",
        "    encoders[feature] = le\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "\n",
        "# 결측치 처리\n",
        "train_x.fillna(train_x.mean(), inplace=True)\n",
        "test.fillna(train_x.mean(), inplace=True)\n",
        "'''\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'country_cluster', 'PORT_SIZE_Zone', 'BREADTH', 'DEPTH', 'DRAUGHT', 'PO_Y_M_D', 'FLAG']\n",
        "# 수치형 변수만 대상으로 결측치 대체\n",
        "numeric_cols = train_x.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 훈련 데이터의 평균 계산\n",
        "mean_values = train_x[numeric_cols].mean()\n",
        "\n",
        "# 결측치 대체\n",
        "train_x[numeric_cols] = train_x[numeric_cols].fillna(mean_values)\n",
        "test[numeric_cols] = test[numeric_cols].fillna(mean_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모든 카테고리형 변수를 문자열로 변환\n",
        "categorical_cols = train_x.select_dtypes(include=['object', 'category']).columns\n",
        "train_x[categorical_cols] = train_x[categorical_cols].astype(str)\n",
        "test[categorical_cols] = test[categorical_cols].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-13 16:57:11,821] A new study created in memory with name: no-name-4f7cc427-49e0-4f9c-b8ec-3af8b8181bd9\n",
            "[W 2023-10-13 16:57:11,824] Trial 0 failed with parameters: {'iterations': 416, 'learning_rate': 0.5950939294198955, 'depth': 15, 'min_data_in_leaf': 19, 'reg_lambda': 96.23724709797567, 'subsample': 0.6926005800768529, 'random_strength': 88.26287372368104, 'od_wait': 34, 'leaf_estimation_iterations': 10, 'bagging_temperature': 11.278863324734598, 'colsample_bylevel': 0.43633414956249306} because of the following error: ValueError(\"'ARI_CO' is not in list\").\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\ineeji\\AppData\\Local\\Temp\\ipykernel_4244\\2899142785.py\", line 12, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val.values), n_trials=20)\n",
            "  File \"C:\\Users\\ineeji\\AppData\\Local\\Temp\\ipykernel_4244\\1586171062.py\", line 33, in objective\n",
            "    cat.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val,y_val)],cat_features=cat_features,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 5734, in fit\n",
            "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 2341, in _fit\n",
            "    train_params = self._prepare_train_params(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 2222, in _prepare_train_params\n",
            "    train_pool = _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 1438, in _build_train_pool\n",
            "    train_pool = Pool(X, y, cat_features=cat_features, text_features=text_features, embedding_features=embedding_features, pairs=pairs, weight=sample_weight, group_id=group_id,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 792, in __init__\n",
            "    self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 1371, in _init\n",
            "    cat_features = _get_features_indices(cat_features, feature_names)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 269, in _get_features_indices\n",
            "    return [\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py\", line 270, in <listcomp>\n",
            "    feature_names.index(f) if isinstance(f, STRING_TYPES) else f\n",
            "ValueError: 'ARI_CO' is not in list\n",
            "[W 2023-10-13 16:57:11,827] Trial 0 failed with value None.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 1...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "'ARI_CO' is not in list",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m313\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: objective(trial, X_train, y_train, X_val, y_val\u001b[39m.\u001b[39;49mvalues), n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m     _optimize(\n\u001b[0;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    453\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m313\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val\u001b[39m.\u001b[39;49mvalues), n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\XGB_v3.ipynb Cell 40\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#'task_type' : 'GPU',\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#\"eval_metric\":'RMSE',\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m cat \u001b[39m=\u001b[39m CatBoostRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m cat\u001b[39m.\u001b[39;49mfit(X_train, y_train, eval_set\u001b[39m=\u001b[39;49m[(X_train,y_train),(X_val,y_val)],cat_features\u001b[39m=\u001b[39;49mcat_features,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m           verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m cat_pred \u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39mpredict(X_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/XGB_v3.ipynb#X43sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m score \u001b[39m=\u001b[39m mean_absolute_error(y_val, cat_pred)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:5734\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5731\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m   5732\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m-> 5734\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[0;32m   5735\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[0;32m   5736\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[0;32m   5737\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:2341\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2338\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, PATH_TYPES \u001b[39m+\u001b[39m (Pool,)):\n\u001b[0;32m   2339\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2341\u001b[0m train_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_train_params(\n\u001b[0;32m   2342\u001b[0m     X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, cat_features\u001b[39m=\u001b[39;49mcat_features, text_features\u001b[39m=\u001b[39;49mtext_features, embedding_features\u001b[39m=\u001b[39;49membedding_features,\n\u001b[0;32m   2343\u001b[0m     pairs\u001b[39m=\u001b[39;49mpairs, sample_weight\u001b[39m=\u001b[39;49msample_weight, group_id\u001b[39m=\u001b[39;49mgroup_id, group_weight\u001b[39m=\u001b[39;49mgroup_weight,\n\u001b[0;32m   2344\u001b[0m     subgroup_id\u001b[39m=\u001b[39;49msubgroup_id, pairs_weight\u001b[39m=\u001b[39;49mpairs_weight, baseline\u001b[39m=\u001b[39;49mbaseline, use_best_model\u001b[39m=\u001b[39;49muse_best_model,\n\u001b[0;32m   2345\u001b[0m     eval_set\u001b[39m=\u001b[39;49meval_set, verbose\u001b[39m=\u001b[39;49mverbose, logging_level\u001b[39m=\u001b[39;49mlogging_level, plot\u001b[39m=\u001b[39;49mplot, plot_file\u001b[39m=\u001b[39;49mplot_file,\n\u001b[0;32m   2346\u001b[0m     column_description\u001b[39m=\u001b[39;49mcolumn_description, verbose_eval\u001b[39m=\u001b[39;49mverbose_eval, metric_period\u001b[39m=\u001b[39;49mmetric_period,\n\u001b[0;32m   2347\u001b[0m     silent\u001b[39m=\u001b[39;49msilent, early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds, save_snapshot\u001b[39m=\u001b[39;49msave_snapshot,\n\u001b[0;32m   2348\u001b[0m     snapshot_file\u001b[39m=\u001b[39;49msnapshot_file, snapshot_interval\u001b[39m=\u001b[39;49msnapshot_interval, init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[0;32m   2349\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[0;32m   2350\u001b[0m )\n\u001b[0;32m   2351\u001b[0m params \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2352\u001b[0m train_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mtrain_pool\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:2222\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   2219\u001b[0m text_features \u001b[39m=\u001b[39m _process_feature_indices(text_features, X, params, \u001b[39m'\u001b[39m\u001b[39mtext_features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2220\u001b[0m embedding_features \u001b[39m=\u001b[39m _process_feature_indices(embedding_features, X, params, \u001b[39m'\u001b[39m\u001b[39membedding_features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 2222\u001b[0m train_pool \u001b[39m=\u001b[39m _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n\u001b[0;32m   2223\u001b[0m                                sample_weight, group_id, group_weight, subgroup_id, pairs_weight,\n\u001b[0;32m   2224\u001b[0m                                baseline, column_description)\n\u001b[0;32m   2225\u001b[0m \u001b[39mif\u001b[39;00m train_pool\u001b[39m.\u001b[39mis_empty_:\n\u001b[0;32m   2226\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mX is empty.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:1438\u001b[0m, in \u001b[0;36m_build_train_pool\u001b[1;34m(X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1437\u001b[0m         \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39my has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1438\u001b[0m     train_pool \u001b[39m=\u001b[39m Pool(X, y, cat_features\u001b[39m=\u001b[39;49mcat_features, text_features\u001b[39m=\u001b[39;49mtext_features, embedding_features\u001b[39m=\u001b[39;49membedding_features, pairs\u001b[39m=\u001b[39;49mpairs, weight\u001b[39m=\u001b[39;49msample_weight, group_id\u001b[39m=\u001b[39;49mgroup_id,\n\u001b[0;32m   1439\u001b[0m                       group_weight\u001b[39m=\u001b[39;49mgroup_weight, subgroup_id\u001b[39m=\u001b[39;49msubgroup_id, pairs_weight\u001b[39m=\u001b[39;49mpairs_weight, baseline\u001b[39m=\u001b[39;49mbaseline)\n\u001b[0;32m   1440\u001b[0m \u001b[39mreturn\u001b[39;00m train_pool\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:792\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[0;32m    787\u001b[0m             \u001b[39mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m    788\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpython objects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    790\u001b[0m             )\n\u001b[1;32m--> 792\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n\u001b[0;32m    793\u001b[0m                    group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[0;32m    794\u001b[0m \u001b[39msuper\u001b[39m(Pool, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:1371\u001b[0m, in \u001b[0;36mPool._init\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(feature_names, features_count)\n\u001b[0;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m cat_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m     cat_features \u001b[39m=\u001b[39m _get_features_indices(cat_features, feature_names)\n\u001b[0;32m   1372\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_string_feature_type(cat_features, \u001b[39m'\u001b[39m\u001b[39mcat_features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_string_feature_value(cat_features, features_count, \u001b[39m'\u001b[39m\u001b[39mcat_features\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:269\u001b[0m, in \u001b[0;36m_get_features_indices\u001b[1;34m(features, feature_names)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mfeature names should be a sequence, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mrepr\u001b[39m(features))\n\u001b[0;32m    268\u001b[0m \u001b[39mif\u001b[39;00m feature_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    270\u001b[0m         feature_names\u001b[39m.\u001b[39mindex(f) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, STRING_TYPES) \u001b[39melse\u001b[39;00m f\n\u001b[0;32m    271\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features\n\u001b[0;32m    272\u001b[0m     ]\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\catboost\\core.py:270\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mfeature names should be a sequence, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mrepr\u001b[39m(features))\n\u001b[0;32m    268\u001b[0m \u001b[39mif\u001b[39;00m feature_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 270\u001b[0m         feature_names\u001b[39m.\u001b[39;49mindex(f) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, STRING_TYPES) \u001b[39melse\u001b[39;00m f\n\u001b[0;32m    271\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features\n\u001b[0;32m    272\u001b[0m     ]\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features:\n",
            "\u001b[1;31mValueError\u001b[0m: 'ARI_CO' is not in list"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=1008)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "# 각 Fold에 대해서 Optuna로 하이퍼파라미터 튜닝\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index, :], train_y.iloc[val_index, :]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=313))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val.values), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best RMSE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = CatBoostRegressor(**best_params)\n",
        "    model.fit(X_train, y_train, cat_features=cat_features)\n",
        "    test_pred = model.predict(test)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "# 평균 앙상블\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "# 결과 출력\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")\n",
        "\n",
        "# 최종 예측 저장\n",
        "final_prediction_df = pd.DataFrame(final_prediction, columns=['MLM'])\n",
        "submission['CI_HOUR'] = final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>97.257813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>336.809458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>68.130348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>156.044942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>417.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>1051.383338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID      CI_HOUR\n",
              "0       TEST_000000    97.257813\n",
              "1       TEST_000001   336.809458\n",
              "2       TEST_000002     0.000000\n",
              "3       TEST_000003     0.000000\n",
              "4       TEST_000004    68.130348\n",
              "...             ...          ...\n",
              "244984  TEST_244984   156.044942\n",
              "244985  TEST_244985   417.542500\n",
              "244986  TEST_244986     0.000000\n",
              "244987  TEST_244987     0.000000\n",
              "244988  TEST_244988  1051.383338\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('CAT_v2_1008.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "#import xgboost as xgb\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "def objective(trial: Trial, X_train, y_train, X_val, y_val):\n",
        "    param = {\n",
        "        'objective': 'regression', # 회귀\n",
        "        'verbose': -1,\n",
        "        'metric': 'mae', \n",
        "        'max_depth': trial.suggest_int('max_depth',3, 15),\n",
        "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1e-2),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.4, 1),\n",
        "    }\n",
        "    model = lgb.LGBMRegressor(**param)\n",
        "    model.fit(X_train, y_train,\n",
        "            eval_set=[(X_train, y_train), (X_val,y_val)],\n",
        "            early_stopping_rounds=50,\n",
        "            verbose=100)\n",
        "    y_pred = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    return mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:09:03,533] A new study created in memory with name: no-name-70b86505-dc98-4849-a543-61ccdf0e998e\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 1...\n",
            "[100]\ttraining's l1: 80.5959\tvalid_1's l1: 79.696\n",
            "[200]\ttraining's l1: 80.475\tvalid_1's l1: 79.5766\n",
            "[300]\ttraining's l1: 80.3549\tvalid_1's l1: 79.4584\n",
            "[400]\ttraining's l1: 80.2241\tvalid_1's l1: 79.3298\n",
            "[500]\ttraining's l1: 80.0903\tvalid_1's l1: 79.1983\n",
            "[600]\ttraining's l1: 79.9576\tvalid_1's l1: 79.0679\n",
            "[700]\ttraining's l1: 79.8276\tvalid_1's l1: 78.9399\n",
            "[800]\ttraining's l1: 79.7019\tvalid_1's l1: 78.816\n",
            "[900]\ttraining's l1: 79.5771\tvalid_1's l1: 78.693\n",
            "[1000]\ttraining's l1: 79.4528\tvalid_1's l1: 78.5702\n",
            "[1100]\ttraining's l1: 79.3288\tvalid_1's l1: 78.4477\n",
            "[1200]\ttraining's l1: 79.2058\tvalid_1's l1: 78.326\n",
            "[1300]\ttraining's l1: 79.0838\tvalid_1's l1: 78.2052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:09:22,531] Trial 0 finished with value: 78.12197004448177 and parameters: {'max_depth': 5, 'learning_rate': 5.403219694006514e-05, 'n_estimators': 1369, 'min_child_samples': 80, 'subsample': 0.8174168223003468}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.7168\tvalid_1's l1: 79.8154\n",
            "[200]\ttraining's l1: 80.7156\tvalid_1's l1: 79.8143\n",
            "[300]\ttraining's l1: 80.7145\tvalid_1's l1: 79.8131\n",
            "[400]\ttraining's l1: 80.7133\tvalid_1's l1: 79.812\n",
            "[500]\ttraining's l1: 80.7121\tvalid_1's l1: 79.8108\n",
            "[600]\ttraining's l1: 80.711\tvalid_1's l1: 79.8097\n",
            "[700]\ttraining's l1: 80.7098\tvalid_1's l1: 79.8085\n",
            "[800]\ttraining's l1: 80.7086\tvalid_1's l1: 79.8074\n",
            "[900]\ttraining's l1: 80.7075\tvalid_1's l1: 79.8062\n",
            "[1000]\ttraining's l1: 80.7063\tvalid_1's l1: 79.8051\n",
            "[1100]\ttraining's l1: 80.7051\tvalid_1's l1: 79.8039\n",
            "[1200]\ttraining's l1: 80.704\tvalid_1's l1: 79.8028\n",
            "[1300]\ttraining's l1: 80.7028\tvalid_1's l1: 79.8016\n",
            "[1400]\ttraining's l1: 80.7017\tvalid_1's l1: 79.8005\n",
            "[1500]\ttraining's l1: 80.7005\tvalid_1's l1: 79.7993\n",
            "[1600]\ttraining's l1: 80.6993\tvalid_1's l1: 79.7982\n",
            "[1700]\ttraining's l1: 80.6982\tvalid_1's l1: 79.797\n",
            "[1800]\ttraining's l1: 80.697\tvalid_1's l1: 79.7959\n",
            "[1900]\ttraining's l1: 80.6958\tvalid_1's l1: 79.7947\n",
            "[2000]\ttraining's l1: 80.6947\tvalid_1's l1: 79.7936\n",
            "[2100]\ttraining's l1: 80.6935\tvalid_1's l1: 79.7924\n",
            "[2200]\ttraining's l1: 80.6923\tvalid_1's l1: 79.7913\n",
            "[2300]\ttraining's l1: 80.6912\tvalid_1's l1: 79.7901\n",
            "[2400]\ttraining's l1: 80.69\tvalid_1's l1: 79.789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:09:56,071] Trial 1 finished with value: 79.78867631271761 and parameters: {'max_depth': 6, 'learning_rate': 4.558117652667486e-07, 'n_estimators': 2426, 'min_child_samples': 96, 'subsample': 0.8925419381452181}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.6903\tvalid_1's l1: 79.7895\n",
            "[200]\ttraining's l1: 80.6628\tvalid_1's l1: 79.7624\n",
            "[300]\ttraining's l1: 80.6352\tvalid_1's l1: 79.7354\n",
            "[400]\ttraining's l1: 80.6077\tvalid_1's l1: 79.7085\n",
            "[500]\ttraining's l1: 80.5803\tvalid_1's l1: 79.6816\n",
            "[600]\ttraining's l1: 80.5529\tvalid_1's l1: 79.6547\n",
            "[700]\ttraining's l1: 80.5255\tvalid_1's l1: 79.6279\n",
            "[800]\ttraining's l1: 80.4982\tvalid_1's l1: 79.6012\n",
            "[900]\ttraining's l1: 80.471\tvalid_1's l1: 79.5745\n",
            "[1000]\ttraining's l1: 80.4437\tvalid_1's l1: 79.5479\n",
            "[1100]\ttraining's l1: 80.4166\tvalid_1's l1: 79.5214\n",
            "[1200]\ttraining's l1: 80.3896\tvalid_1's l1: 79.4949\n",
            "[1300]\ttraining's l1: 80.3625\tvalid_1's l1: 79.4685\n",
            "[1400]\ttraining's l1: 80.3355\tvalid_1's l1: 79.4421\n",
            "[1500]\ttraining's l1: 80.3086\tvalid_1's l1: 79.4158\n",
            "[1600]\ttraining's l1: 80.2817\tvalid_1's l1: 79.3895\n",
            "[1700]\ttraining's l1: 80.2548\tvalid_1's l1: 79.3632\n",
            "[1800]\ttraining's l1: 80.228\tvalid_1's l1: 79.337\n",
            "[1900]\ttraining's l1: 80.2013\tvalid_1's l1: 79.3108\n",
            "[2000]\ttraining's l1: 80.1745\tvalid_1's l1: 79.2847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:10:26,930] Trial 2 finished with value: 79.26329568703797 and parameters: {'max_depth': 7, 'learning_rate': 1.0138431078138509e-05, 'n_estimators': 2082, 'min_child_samples': 73, 'subsample': 0.5615618290533614}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.6873\tvalid_1's l1: 79.7862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:10:29,540] Trial 3 finished with value: 79.77436337383372 and parameters: {'max_depth': 10, 'learning_rate': 1.0435157103518567e-05, 'n_estimators': 139, 'min_child_samples': 79, 'subsample': 0.8980452795192285}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.5836\tvalid_1's l1: 79.6845\n",
            "[200]\ttraining's l1: 80.4505\tvalid_1's l1: 79.5535\n",
            "[300]\ttraining's l1: 80.3186\tvalid_1's l1: 79.424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:10:34,238] Trial 4 finished with value: 79.40078821208094 and parameters: {'max_depth': 7, 'learning_rate': 4.9246692819521035e-05, 'n_estimators': 318, 'min_child_samples': 40, 'subsample': 0.9405757371443862}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.7109\tvalid_1's l1: 79.8096\n",
            "[200]\ttraining's l1: 80.7039\tvalid_1's l1: 79.8026\n",
            "[300]\ttraining's l1: 80.6968\tvalid_1's l1: 79.7956\n",
            "[400]\ttraining's l1: 80.6898\tvalid_1's l1: 79.7886\n",
            "[500]\ttraining's l1: 80.6828\tvalid_1's l1: 79.7817\n",
            "[600]\ttraining's l1: 80.6757\tvalid_1's l1: 79.7747\n",
            "[700]\ttraining's l1: 80.6687\tvalid_1's l1: 79.7678\n",
            "[800]\ttraining's l1: 80.6617\tvalid_1's l1: 79.7608\n",
            "[900]\ttraining's l1: 80.6546\tvalid_1's l1: 79.7539\n",
            "[1000]\ttraining's l1: 80.6476\tvalid_1's l1: 79.7469\n",
            "[1100]\ttraining's l1: 80.6406\tvalid_1's l1: 79.74\n",
            "[1200]\ttraining's l1: 80.6336\tvalid_1's l1: 79.733\n",
            "[1300]\ttraining's l1: 80.6266\tvalid_1's l1: 79.7261\n",
            "[1400]\ttraining's l1: 80.6195\tvalid_1's l1: 79.7192\n",
            "[1500]\ttraining's l1: 80.6125\tvalid_1's l1: 79.7122\n",
            "[1600]\ttraining's l1: 80.6055\tvalid_1's l1: 79.7053\n",
            "[1700]\ttraining's l1: 80.5985\tvalid_1's l1: 79.6984\n",
            "[1800]\ttraining's l1: 80.5915\tvalid_1's l1: 79.6914\n",
            "[1900]\ttraining's l1: 80.5845\tvalid_1's l1: 79.6845\n",
            "[2000]\ttraining's l1: 80.5775\tvalid_1's l1: 79.6776\n",
            "[2100]\ttraining's l1: 80.5705\tvalid_1's l1: 79.6706\n",
            "[2200]\ttraining's l1: 80.5636\tvalid_1's l1: 79.6637\n",
            "[2300]\ttraining's l1: 80.5566\tvalid_1's l1: 79.6568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:11:20,287] Trial 5 finished with value: 79.65069644424287 and parameters: {'max_depth': 11, 'learning_rate': 2.416659513439968e-06, 'n_estimators': 2388, 'min_child_samples': 35, 'subsample': 0.6731768524043841}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.7059\tvalid_1's l1: 79.8046\n",
            "[200]\ttraining's l1: 80.6938\tvalid_1's l1: 79.7926\n",
            "[300]\ttraining's l1: 80.6817\tvalid_1's l1: 79.7807\n",
            "[400]\ttraining's l1: 80.6697\tvalid_1's l1: 79.7687\n",
            "[500]\ttraining's l1: 80.6576\tvalid_1's l1: 79.7568\n",
            "[600]\ttraining's l1: 80.6456\tvalid_1's l1: 79.7449\n",
            "[700]\ttraining's l1: 80.6335\tvalid_1's l1: 79.733\n",
            "[800]\ttraining's l1: 80.6215\tvalid_1's l1: 79.7211\n",
            "[900]\ttraining's l1: 80.6095\tvalid_1's l1: 79.7092\n",
            "[1000]\ttraining's l1: 80.5975\tvalid_1's l1: 79.6973\n",
            "[1100]\ttraining's l1: 80.5855\tvalid_1's l1: 79.6854\n",
            "[1200]\ttraining's l1: 80.5735\tvalid_1's l1: 79.6735\n",
            "[1300]\ttraining's l1: 80.5615\tvalid_1's l1: 79.6616\n",
            "[1400]\ttraining's l1: 80.5495\tvalid_1's l1: 79.6497\n",
            "[1500]\ttraining's l1: 80.5375\tvalid_1's l1: 79.6379\n",
            "[1600]\ttraining's l1: 80.5256\tvalid_1's l1: 79.626\n",
            "[1700]\ttraining's l1: 80.5136\tvalid_1's l1: 79.6142\n",
            "[1800]\ttraining's l1: 80.5017\tvalid_1's l1: 79.6023\n",
            "[1900]\ttraining's l1: 80.4897\tvalid_1's l1: 79.5905\n",
            "[2000]\ttraining's l1: 80.4778\tvalid_1's l1: 79.5787\n",
            "[2100]\ttraining's l1: 80.4659\tvalid_1's l1: 79.5669\n",
            "[2200]\ttraining's l1: 80.454\tvalid_1's l1: 79.5551\n",
            "[2300]\ttraining's l1: 80.4421\tvalid_1's l1: 79.5433\n",
            "[2400]\ttraining's l1: 80.4302\tvalid_1's l1: 79.5315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:12:04,708] Trial 6 finished with value: 79.52830157879258 and parameters: {'max_depth': 14, 'learning_rate': 4.140383253849187e-06, 'n_estimators': 2427, 'min_child_samples': 18, 'subsample': 0.7626295134510885}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.7174\tvalid_1's l1: 79.816\n",
            "[200]\ttraining's l1: 80.7168\tvalid_1's l1: 79.8154\n",
            "[300]\ttraining's l1: 80.7162\tvalid_1's l1: 79.8148\n",
            "[400]\ttraining's l1: 80.7156\tvalid_1's l1: 79.8142\n",
            "[500]\ttraining's l1: 80.715\tvalid_1's l1: 79.8136\n",
            "[600]\ttraining's l1: 80.7144\tvalid_1's l1: 79.813\n",
            "[700]\ttraining's l1: 80.7138\tvalid_1's l1: 79.8124\n",
            "[800]\ttraining's l1: 80.7132\tvalid_1's l1: 79.8118\n",
            "[900]\ttraining's l1: 80.7126\tvalid_1's l1: 79.8112\n",
            "[1000]\ttraining's l1: 80.712\tvalid_1's l1: 79.8106\n",
            "[1100]\ttraining's l1: 80.7114\tvalid_1's l1: 79.81\n",
            "[1200]\ttraining's l1: 80.7108\tvalid_1's l1: 79.8094\n",
            "[1300]\ttraining's l1: 80.7102\tvalid_1's l1: 79.8088\n",
            "[1400]\ttraining's l1: 80.7096\tvalid_1's l1: 79.8082\n",
            "[1500]\ttraining's l1: 80.709\tvalid_1's l1: 79.8077\n",
            "[1600]\ttraining's l1: 80.7084\tvalid_1's l1: 79.8071\n",
            "[1700]\ttraining's l1: 80.7078\tvalid_1's l1: 79.8065\n",
            "[1800]\ttraining's l1: 80.7072\tvalid_1's l1: 79.8059\n",
            "[1900]\ttraining's l1: 80.7066\tvalid_1's l1: 79.8053\n",
            "[2000]\ttraining's l1: 80.706\tvalid_1's l1: 79.8047\n",
            "[2100]\ttraining's l1: 80.7054\tvalid_1's l1: 79.8041\n",
            "[2200]\ttraining's l1: 80.7048\tvalid_1's l1: 79.8035\n",
            "[2300]\ttraining's l1: 80.7042\tvalid_1's l1: 79.8029\n",
            "[2400]\ttraining's l1: 80.7036\tvalid_1's l1: 79.8023\n",
            "[2500]\ttraining's l1: 80.703\tvalid_1's l1: 79.8017\n",
            "[2600]\ttraining's l1: 80.7024\tvalid_1's l1: 79.8011\n",
            "[2700]\ttraining's l1: 80.7018\tvalid_1's l1: 79.8005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:12:51,062] Trial 7 finished with value: 79.80004193107165 and parameters: {'max_depth': 12, 'learning_rate': 2.0547199328558857e-07, 'n_estimators': 2783, 'min_child_samples': 47, 'subsample': 0.9202656079409988}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.7177\tvalid_1's l1: 79.8163\n",
            "[200]\ttraining's l1: 80.7175\tvalid_1's l1: 79.8161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:12:53,565] Trial 8 finished with value: 79.81599042717905 and parameters: {'max_depth': 3, 'learning_rate': 1.2756234750552218e-07, 'n_estimators': 237, 'min_child_samples': 69, 'subsample': 0.6897393150325494}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.7179\tvalid_1's l1: 79.8165\n",
            "[200]\ttraining's l1: 80.7179\tvalid_1's l1: 79.8165\n",
            "[300]\ttraining's l1: 80.7178\tvalid_1's l1: 79.8164\n",
            "[400]\ttraining's l1: 80.7178\tvalid_1's l1: 79.8163\n",
            "[500]\ttraining's l1: 80.7177\tvalid_1's l1: 79.8163\n",
            "[600]\ttraining's l1: 80.7177\tvalid_1's l1: 79.8162\n",
            "[700]\ttraining's l1: 80.7176\tvalid_1's l1: 79.8162\n",
            "[800]\ttraining's l1: 80.7176\tvalid_1's l1: 79.8161\n",
            "[900]\ttraining's l1: 80.7175\tvalid_1's l1: 79.8161\n",
            "[1000]\ttraining's l1: 80.7174\tvalid_1's l1: 79.816\n",
            "[1100]\ttraining's l1: 80.7174\tvalid_1's l1: 79.816\n",
            "[1200]\ttraining's l1: 80.7173\tvalid_1's l1: 79.8159\n",
            "[1300]\ttraining's l1: 80.7173\tvalid_1's l1: 79.8159\n",
            "[1400]\ttraining's l1: 80.7172\tvalid_1's l1: 79.8158\n",
            "[1500]\ttraining's l1: 80.7172\tvalid_1's l1: 79.8158\n",
            "[1600]\ttraining's l1: 80.7171\tvalid_1's l1: 79.8157\n",
            "[1700]\ttraining's l1: 80.7171\tvalid_1's l1: 79.8157\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:13:22,920] Trial 9 finished with value: 79.81564142156058 and parameters: {'max_depth': 9, 'learning_rate': 1.8194678087880884e-08, 'n_estimators': 1728, 'min_child_samples': 36, 'subsample': 0.6341771899009377}. Best is trial 0 with value: 78.12197004448177.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 76.6377\tvalid_1's l1: 75.7914\n",
            "[200]\ttraining's l1: 73.4446\tvalid_1's l1: 72.645\n",
            "[300]\ttraining's l1: 71.0882\tvalid_1's l1: 70.3045\n",
            "[400]\ttraining's l1: 69.3205\tvalid_1's l1: 68.5506\n",
            "[500]\ttraining's l1: 67.8565\tvalid_1's l1: 67.0948\n",
            "[600]\ttraining's l1: 66.5482\tvalid_1's l1: 65.7866\n",
            "[700]\ttraining's l1: 65.4613\tvalid_1's l1: 64.6802\n",
            "[800]\ttraining's l1: 64.5421\tvalid_1's l1: 63.7426\n",
            "[900]\ttraining's l1: 63.8332\tvalid_1's l1: 63.024\n",
            "[1000]\ttraining's l1: 63.2635\tvalid_1's l1: 62.4498\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:13:38,548] Trial 10 finished with value: 62.2397848723055 and parameters: {'max_depth': 3, 'learning_rate': 0.0023074972749146296, 'n_estimators': 1039, 'min_child_samples': 99, 'subsample': 0.4330776943512126}. Best is trial 10 with value: 62.2397848723055.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 76.2415\tvalid_1's l1: 75.4021\n",
            "[200]\ttraining's l1: 72.8843\tvalid_1's l1: 72.0884\n",
            "[300]\ttraining's l1: 70.4738\tvalid_1's l1: 69.6948\n",
            "[400]\ttraining's l1: 68.6932\tvalid_1's l1: 67.9284\n",
            "[500]\ttraining's l1: 67.1342\tvalid_1's l1: 66.3741\n",
            "[600]\ttraining's l1: 65.8338\tvalid_1's l1: 65.0602\n",
            "[700]\ttraining's l1: 64.7652\tvalid_1's l1: 63.9721\n",
            "[800]\ttraining's l1: 63.9321\tvalid_1's l1: 63.1258\n",
            "[900]\ttraining's l1: 63.2878\tvalid_1's l1: 62.4755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:13:51,190] Trial 11 finished with value: 62.00121921921346 and parameters: {'max_depth': 3, 'learning_rate': 0.002552138181406136, 'n_estimators': 982, 'min_child_samples': 97, 'subsample': 0.4060724173367089}. Best is trial 11 with value: 62.00121921921346.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 69.9095\tvalid_1's l1: 69.1357\n",
            "[200]\ttraining's l1: 65.1716\tvalid_1's l1: 64.3861\n",
            "[300]\ttraining's l1: 62.7844\tvalid_1's l1: 61.9655\n",
            "[400]\ttraining's l1: 61.6823\tvalid_1's l1: 60.8507\n",
            "[500]\ttraining's l1: 61.5907\tvalid_1's l1: 60.749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:13:57,887] Trial 12 finished with value: 60.738012046791326 and parameters: {'max_depth': 3, 'learning_rate': 0.008384679572702159, 'n_estimators': 1024, 'min_child_samples': 99, 'subsample': 0.4051837370550797}. Best is trial 12 with value: 60.738012046791326.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 69.2365\tvalid_1's l1: 68.4671\n",
            "[200]\ttraining's l1: 64.2256\tvalid_1's l1: 63.452\n",
            "[300]\ttraining's l1: 61.2359\tvalid_1's l1: 60.3793\n",
            "[400]\ttraining's l1: 59.852\tvalid_1's l1: 58.9894\n",
            "[500]\ttraining's l1: 59.4367\tvalid_1's l1: 58.5793\n",
            "[600]\ttraining's l1: 59.3225\tvalid_1's l1: 58.4984\n",
            "[700]\ttraining's l1: 59.2939\tvalid_1's l1: 58.5073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:14:10,173] Trial 13 finished with value: 58.47762988206401 and parameters: {'max_depth': 4, 'learning_rate': 0.007714473041412747, 'n_estimators': 915, 'min_child_samples': 90, 'subsample': 0.40208137391685717}. Best is trial 13 with value: 58.47762988206401.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 67.737\tvalid_1's l1: 66.9493\n",
            "[200]\ttraining's l1: 62.2454\tvalid_1's l1: 61.4922\n",
            "[300]\ttraining's l1: 59.4064\tvalid_1's l1: 58.6323\n",
            "[400]\ttraining's l1: 58.0608\tvalid_1's l1: 57.3544\n",
            "[500]\ttraining's l1: 57.6522\tvalid_1's l1: 57.0084\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:14:20,746] Trial 14 finished with value: 57.00167178462569 and parameters: {'max_depth': 5, 'learning_rate': 0.008645764928797792, 'n_estimators': 770, 'min_child_samples': 60, 'subsample': 0.46901968548177947}. Best is trial 14 with value: 57.00167178462569.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 78.5154\tvalid_1's l1: 77.6407\n",
            "[200]\ttraining's l1: 76.5574\tvalid_1's l1: 75.6952\n",
            "[300]\ttraining's l1: 74.8337\tvalid_1's l1: 73.987\n",
            "[400]\ttraining's l1: 73.3131\tvalid_1's l1: 72.4852\n",
            "[500]\ttraining's l1: 71.9629\tvalid_1's l1: 71.1515\n",
            "[600]\ttraining's l1: 70.738\tvalid_1's l1: 69.944\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:14:33,965] Trial 15 finished with value: 69.86402767030759 and parameters: {'max_depth': 5, 'learning_rate': 0.000959363518233158, 'n_estimators': 607, 'min_child_samples': 59, 'subsample': 0.4837536242763549}. Best is trial 14 with value: 57.00167178462569.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 65.1132\tvalid_1's l1: 64.3214\n",
            "[200]\ttraining's l1: 59.8145\tvalid_1's l1: 59.1595\n",
            "[300]\ttraining's l1: 57.6169\tvalid_1's l1: 57.0672\n",
            "[400]\ttraining's l1: 56.5031\tvalid_1's l1: 56.0899\n",
            "[500]\ttraining's l1: 55.8773\tvalid_1's l1: 55.5725\n",
            "[600]\ttraining's l1: 55.7072\tvalid_1's l1: 55.5233\n",
            "[700]\ttraining's l1: 55.5838\tvalid_1's l1: 55.4914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:14:50,665] Trial 16 finished with value: 55.411562686166654 and parameters: {'max_depth': 8, 'learning_rate': 0.00991389611665848, 'n_estimators': 761, 'min_child_samples': 59, 'subsample': 0.49675921046799526}. Best is trial 16 with value: 55.411562686166654.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 79.9434\tvalid_1's l1: 79.0487\n",
            "[200]\ttraining's l1: 79.1965\tvalid_1's l1: 78.3074\n",
            "[300]\ttraining's l1: 78.4713\tvalid_1's l1: 77.5853\n",
            "[400]\ttraining's l1: 77.7728\tvalid_1's l1: 76.8892\n",
            "[500]\ttraining's l1: 77.1038\tvalid_1's l1: 76.2214\n",
            "[600]\ttraining's l1: 76.462\tvalid_1's l1: 75.5806\n",
            "[700]\ttraining's l1: 75.8452\tvalid_1's l1: 74.9656\n",
            "[800]\ttraining's l1: 75.2537\tvalid_1's l1: 74.3761\n",
            "[900]\ttraining's l1: 74.6858\tvalid_1's l1: 73.8102\n",
            "[1000]\ttraining's l1: 74.1407\tvalid_1's l1: 73.2677\n",
            "[1100]\ttraining's l1: 73.618\tvalid_1's l1: 72.7489\n",
            "[1200]\ttraining's l1: 73.1165\tvalid_1's l1: 72.2502\n",
            "[1300]\ttraining's l1: 72.6344\tvalid_1's l1: 71.7717\n",
            "[1400]\ttraining's l1: 72.1698\tvalid_1's l1: 71.3108\n",
            "[1500]\ttraining's l1: 71.7214\tvalid_1's l1: 70.8655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:15:24,990] Trial 17 finished with value: 70.64055751679001 and parameters: {'max_depth': 8, 'learning_rate': 0.0002796477629319034, 'n_estimators': 1552, 'min_child_samples': 61, 'subsample': 0.5266646476176604}. Best is trial 16 with value: 55.411562686166654.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 79.894\tvalid_1's l1: 78.9998\n",
            "[200]\ttraining's l1: 79.1072\tvalid_1's l1: 78.2203\n",
            "[300]\ttraining's l1: 78.3489\tvalid_1's l1: 77.4659\n",
            "[400]\ttraining's l1: 77.6269\tvalid_1's l1: 76.7455\n",
            "[500]\ttraining's l1: 76.9357\tvalid_1's l1: 76.0577\n",
            "[600]\ttraining's l1: 76.2735\tvalid_1's l1: 75.3987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:15:38,447] Trial 18 finished with value: 75.17434975713115 and parameters: {'max_depth': 15, 'learning_rate': 0.0002888338850834119, 'n_estimators': 635, 'min_child_samples': 5, 'subsample': 0.46685923939675045}. Best is trial 16 with value: 55.411562686166654.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 78.4881\tvalid_1's l1: 77.6006\n",
            "[200]\ttraining's l1: 76.4904\tvalid_1's l1: 75.6066\n",
            "[300]\ttraining's l1: 74.723\tvalid_1's l1: 73.8447\n",
            "[400]\ttraining's l1: 73.1601\tvalid_1's l1: 72.2919\n",
            "[500]\ttraining's l1: 71.7701\tvalid_1's l1: 70.9124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:15:48,892] Trial 19 finished with value: 69.77908244252899 and parameters: {'max_depth': 8, 'learning_rate': 0.0008313601447265986, 'n_estimators': 592, 'min_child_samples': 51, 'subsample': 0.5501208266207983}. Best is trial 16 with value: 55.411562686166654.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best MAE for fold 1: 55.411562686166654\n",
            "Best hyperparameters for fold 1: {'max_depth': 8, 'learning_rate': 0.00991389611665848, 'n_estimators': 761, 'min_child_samples': 59, 'subsample': 0.49675921046799526}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:15:59,722] A new study created in memory with name: no-name-b04f319d-caf0-4ea3-b611-f456e2902be3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 2...\n",
            "[100]\ttraining's l1: 80.2186\tvalid_1's l1: 81.7329\n",
            "[200]\ttraining's l1: 80.0822\tvalid_1's l1: 81.5945\n",
            "[300]\ttraining's l1: 79.9469\tvalid_1's l1: 81.4574\n",
            "[400]\ttraining's l1: 79.8146\tvalid_1's l1: 81.3238\n",
            "[500]\ttraining's l1: 79.6834\tvalid_1's l1: 81.1913\n",
            "[600]\ttraining's l1: 79.5532\tvalid_1's l1: 81.0599\n",
            "[700]\ttraining's l1: 79.424\tvalid_1's l1: 80.9294\n",
            "[800]\ttraining's l1: 79.2959\tvalid_1's l1: 80.7999\n",
            "[900]\ttraining's l1: 79.1689\tvalid_1's l1: 80.6715\n",
            "[1000]\ttraining's l1: 79.0436\tvalid_1's l1: 80.5448\n",
            "[1100]\ttraining's l1: 78.9191\tvalid_1's l1: 80.4189\n",
            "[1200]\ttraining's l1: 78.7951\tvalid_1's l1: 80.2936\n",
            "[1300]\ttraining's l1: 78.6721\tvalid_1's l1: 80.1691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:16:18,508] Trial 0 finished with value: 80.08405635148821 and parameters: {'max_depth': 5, 'learning_rate': 5.403219694006514e-05, 'n_estimators': 1369, 'min_child_samples': 80, 'subsample': 0.8174168223003468}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.355\tvalid_1's l1: 81.8712\n",
            "[200]\ttraining's l1: 80.3537\tvalid_1's l1: 81.87\n",
            "[300]\ttraining's l1: 80.3525\tvalid_1's l1: 81.8687\n",
            "[400]\ttraining's l1: 80.3512\tvalid_1's l1: 81.8675\n",
            "[500]\ttraining's l1: 80.35\tvalid_1's l1: 81.8662\n",
            "[600]\ttraining's l1: 80.3487\tvalid_1's l1: 81.8649\n",
            "[700]\ttraining's l1: 80.3475\tvalid_1's l1: 81.8637\n",
            "[800]\ttraining's l1: 80.3462\tvalid_1's l1: 81.8624\n",
            "[900]\ttraining's l1: 80.345\tvalid_1's l1: 81.8611\n",
            "[1000]\ttraining's l1: 80.3437\tvalid_1's l1: 81.8599\n",
            "[1100]\ttraining's l1: 80.3425\tvalid_1's l1: 81.8586\n",
            "[1200]\ttraining's l1: 80.3412\tvalid_1's l1: 81.8573\n",
            "[1300]\ttraining's l1: 80.34\tvalid_1's l1: 81.8561\n",
            "[1400]\ttraining's l1: 80.3387\tvalid_1's l1: 81.8548\n",
            "[1500]\ttraining's l1: 80.3375\tvalid_1's l1: 81.8535\n",
            "[1600]\ttraining's l1: 80.3362\tvalid_1's l1: 81.8523\n",
            "[1700]\ttraining's l1: 80.335\tvalid_1's l1: 81.851\n",
            "[1800]\ttraining's l1: 80.3337\tvalid_1's l1: 81.8498\n",
            "[1900]\ttraining's l1: 80.3325\tvalid_1's l1: 81.8485\n",
            "[2000]\ttraining's l1: 80.3313\tvalid_1's l1: 81.8472\n",
            "[2100]\ttraining's l1: 80.33\tvalid_1's l1: 81.846\n",
            "[2200]\ttraining's l1: 80.3288\tvalid_1's l1: 81.8447\n",
            "[2300]\ttraining's l1: 80.3275\tvalid_1's l1: 81.8435\n",
            "[2400]\ttraining's l1: 80.3263\tvalid_1's l1: 81.8422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:16:52,957] Trial 1 finished with value: 81.84186394824765 and parameters: {'max_depth': 6, 'learning_rate': 4.558117652667486e-07, 'n_estimators': 2426, 'min_child_samples': 96, 'subsample': 0.8925419381452181}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.3264\tvalid_1's l1: 81.8425\n",
            "[200]\ttraining's l1: 80.2967\tvalid_1's l1: 81.8125\n",
            "[300]\ttraining's l1: 80.2671\tvalid_1's l1: 81.7825\n",
            "[400]\ttraining's l1: 80.2375\tvalid_1's l1: 81.7526\n",
            "[500]\ttraining's l1: 80.2079\tvalid_1's l1: 81.7228\n",
            "[600]\ttraining's l1: 80.1784\tvalid_1's l1: 81.693\n",
            "[700]\ttraining's l1: 80.149\tvalid_1's l1: 81.6633\n",
            "[800]\ttraining's l1: 80.1196\tvalid_1's l1: 81.6336\n",
            "[900]\ttraining's l1: 80.0902\tvalid_1's l1: 81.604\n",
            "[1000]\ttraining's l1: 80.061\tvalid_1's l1: 81.5745\n",
            "[1100]\ttraining's l1: 80.0317\tvalid_1's l1: 81.5449\n",
            "[1200]\ttraining's l1: 80.0026\tvalid_1's l1: 81.5155\n",
            "[1300]\ttraining's l1: 79.9734\tvalid_1's l1: 81.4861\n",
            "[1400]\ttraining's l1: 79.9446\tvalid_1's l1: 81.4569\n",
            "[1500]\ttraining's l1: 79.9159\tvalid_1's l1: 81.428\n",
            "[1600]\ttraining's l1: 79.8872\tvalid_1's l1: 81.3991\n",
            "[1700]\ttraining's l1: 79.8586\tvalid_1's l1: 81.3702\n",
            "[1800]\ttraining's l1: 79.8301\tvalid_1's l1: 81.3414\n",
            "[1900]\ttraining's l1: 79.8015\tvalid_1's l1: 81.3126\n",
            "[2000]\ttraining's l1: 79.773\tvalid_1's l1: 81.2839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:17:24,815] Trial 2 finished with value: 81.26031977243258 and parameters: {'max_depth': 7, 'learning_rate': 1.0138431078138509e-05, 'n_estimators': 2082, 'min_child_samples': 73, 'subsample': 0.5615618290533614}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.3256\tvalid_1's l1: 81.8416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:17:27,454] Trial 3 finished with value: 81.82953826397086 and parameters: {'max_depth': 10, 'learning_rate': 1.0435157103518567e-05, 'n_estimators': 139, 'min_child_samples': 79, 'subsample': 0.8980452795192285}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.2117\tvalid_1's l1: 81.7264\n",
            "[200]\ttraining's l1: 80.0684\tvalid_1's l1: 81.5816\n",
            "[300]\ttraining's l1: 79.9267\tvalid_1's l1: 81.4383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:17:32,665] Trial 4 finished with value: 81.41292914659994 and parameters: {'max_depth': 7, 'learning_rate': 4.9246692819521035e-05, 'n_estimators': 318, 'min_child_samples': 40, 'subsample': 0.9405757371443862}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.3491\tvalid_1's l1: 81.8654\n",
            "[200]\ttraining's l1: 80.3421\tvalid_1's l1: 81.8583\n",
            "[300]\ttraining's l1: 80.335\tvalid_1's l1: 81.8511\n",
            "[400]\ttraining's l1: 80.328\tvalid_1's l1: 81.844\n",
            "[500]\ttraining's l1: 80.3209\tvalid_1's l1: 81.8369\n",
            "[600]\ttraining's l1: 80.3139\tvalid_1's l1: 81.8298\n",
            "[700]\ttraining's l1: 80.3068\tvalid_1's l1: 81.8227\n",
            "[800]\ttraining's l1: 80.2998\tvalid_1's l1: 81.8156\n",
            "[900]\ttraining's l1: 80.2927\tvalid_1's l1: 81.8085\n",
            "[1000]\ttraining's l1: 80.2857\tvalid_1's l1: 81.8014\n",
            "[1100]\ttraining's l1: 80.2787\tvalid_1's l1: 81.7943\n",
            "[1200]\ttraining's l1: 80.2716\tvalid_1's l1: 81.7872\n",
            "[1300]\ttraining's l1: 80.2646\tvalid_1's l1: 81.7801\n",
            "[1400]\ttraining's l1: 80.2576\tvalid_1's l1: 81.773\n",
            "[1500]\ttraining's l1: 80.2506\tvalid_1's l1: 81.7659\n",
            "[1600]\ttraining's l1: 80.2435\tvalid_1's l1: 81.7588\n",
            "[1700]\ttraining's l1: 80.2365\tvalid_1's l1: 81.7518\n",
            "[1800]\ttraining's l1: 80.2295\tvalid_1's l1: 81.7447\n",
            "[1900]\ttraining's l1: 80.2225\tvalid_1's l1: 81.7376\n",
            "[2000]\ttraining's l1: 80.2155\tvalid_1's l1: 81.7305\n",
            "[2100]\ttraining's l1: 80.2085\tvalid_1's l1: 81.7235\n",
            "[2200]\ttraining's l1: 80.2015\tvalid_1's l1: 81.7164\n",
            "[2300]\ttraining's l1: 80.1945\tvalid_1's l1: 81.7094\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:18:12,014] Trial 5 finished with value: 81.70313824468062 and parameters: {'max_depth': 11, 'learning_rate': 2.416659513439968e-06, 'n_estimators': 2388, 'min_child_samples': 35, 'subsample': 0.6731768524043841}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.3441\tvalid_1's l1: 81.8603\n",
            "[200]\ttraining's l1: 80.332\tvalid_1's l1: 81.8481\n",
            "[300]\ttraining's l1: 80.3199\tvalid_1's l1: 81.8359\n",
            "[400]\ttraining's l1: 80.3079\tvalid_1's l1: 81.8237\n",
            "[500]\ttraining's l1: 80.2958\tvalid_1's l1: 81.8116\n",
            "[600]\ttraining's l1: 80.2837\tvalid_1's l1: 81.7994\n",
            "[700]\ttraining's l1: 80.2717\tvalid_1's l1: 81.7872\n",
            "[800]\ttraining's l1: 80.2596\tvalid_1's l1: 81.7751\n",
            "[900]\ttraining's l1: 80.2476\tvalid_1's l1: 81.763\n",
            "[1000]\ttraining's l1: 80.2356\tvalid_1's l1: 81.7508\n",
            "[1100]\ttraining's l1: 80.2236\tvalid_1's l1: 81.7387\n",
            "[1200]\ttraining's l1: 80.2116\tvalid_1's l1: 81.7266\n",
            "[1300]\ttraining's l1: 80.1996\tvalid_1's l1: 81.7145\n",
            "[1400]\ttraining's l1: 80.1876\tvalid_1's l1: 81.7024\n",
            "[1500]\ttraining's l1: 80.1756\tvalid_1's l1: 81.6903\n",
            "[1600]\ttraining's l1: 80.1636\tvalid_1's l1: 81.6782\n",
            "[1700]\ttraining's l1: 80.1517\tvalid_1's l1: 81.6662\n",
            "[1800]\ttraining's l1: 80.1397\tvalid_1's l1: 81.6541\n",
            "[1900]\ttraining's l1: 80.1278\tvalid_1's l1: 81.6421\n",
            "[2000]\ttraining's l1: 80.1158\tvalid_1's l1: 81.63\n",
            "[2100]\ttraining's l1: 80.1039\tvalid_1's l1: 81.618\n",
            "[2200]\ttraining's l1: 80.092\tvalid_1's l1: 81.6059\n",
            "[2300]\ttraining's l1: 80.0801\tvalid_1's l1: 81.5939\n",
            "[2400]\ttraining's l1: 80.0682\tvalid_1's l1: 81.5819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:18:50,223] Trial 6 finished with value: 81.5786589678979 and parameters: {'max_depth': 14, 'learning_rate': 4.140383253849187e-06, 'n_estimators': 2427, 'min_child_samples': 18, 'subsample': 0.7626295134510885}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.3556\tvalid_1's l1: 81.8719\n",
            "[200]\ttraining's l1: 80.355\tvalid_1's l1: 81.8713\n",
            "[300]\ttraining's l1: 80.3544\tvalid_1's l1: 81.8707\n",
            "[400]\ttraining's l1: 80.3538\tvalid_1's l1: 81.8701\n",
            "[500]\ttraining's l1: 80.3532\tvalid_1's l1: 81.8695\n",
            "[600]\ttraining's l1: 80.3526\tvalid_1's l1: 81.8689\n",
            "[700]\ttraining's l1: 80.352\tvalid_1's l1: 81.8683\n",
            "[800]\ttraining's l1: 80.3514\tvalid_1's l1: 81.8677\n",
            "[900]\ttraining's l1: 80.3508\tvalid_1's l1: 81.8671\n",
            "[1000]\ttraining's l1: 80.3502\tvalid_1's l1: 81.8665\n",
            "[1100]\ttraining's l1: 80.3496\tvalid_1's l1: 81.8659\n",
            "[1200]\ttraining's l1: 80.349\tvalid_1's l1: 81.8653\n",
            "[1300]\ttraining's l1: 80.3484\tvalid_1's l1: 81.8647\n",
            "[1400]\ttraining's l1: 80.3478\tvalid_1's l1: 81.864\n",
            "[1500]\ttraining's l1: 80.3472\tvalid_1's l1: 81.8634\n",
            "[1600]\ttraining's l1: 80.3466\tvalid_1's l1: 81.8628\n",
            "[1700]\ttraining's l1: 80.346\tvalid_1's l1: 81.8622\n",
            "[1800]\ttraining's l1: 80.3454\tvalid_1's l1: 81.8616\n",
            "[1900]\ttraining's l1: 80.3448\tvalid_1's l1: 81.861\n",
            "[2000]\ttraining's l1: 80.3442\tvalid_1's l1: 81.8604\n",
            "[2100]\ttraining's l1: 80.3436\tvalid_1's l1: 81.8598\n",
            "[2200]\ttraining's l1: 80.343\tvalid_1's l1: 81.8592\n",
            "[2300]\ttraining's l1: 80.3424\tvalid_1's l1: 81.8586\n",
            "[2400]\ttraining's l1: 80.3418\tvalid_1's l1: 81.858\n",
            "[2500]\ttraining's l1: 80.3412\tvalid_1's l1: 81.8574\n",
            "[2600]\ttraining's l1: 80.3406\tvalid_1's l1: 81.8568\n",
            "[2700]\ttraining's l1: 80.34\tvalid_1's l1: 81.8562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:19:38,847] Trial 7 finished with value: 81.8556896342887 and parameters: {'max_depth': 12, 'learning_rate': 2.0547199328558857e-07, 'n_estimators': 2783, 'min_child_samples': 47, 'subsample': 0.9202656079409988}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.3559\tvalid_1's l1: 81.8722\n",
            "[200]\ttraining's l1: 80.3557\tvalid_1's l1: 81.872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:19:41,477] Trial 8 finished with value: 81.87186244803561 and parameters: {'max_depth': 3, 'learning_rate': 1.2756234750552218e-07, 'n_estimators': 237, 'min_child_samples': 69, 'subsample': 0.6897393150325494}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 80.3562\tvalid_1's l1: 81.8725\n",
            "[200]\ttraining's l1: 80.3561\tvalid_1's l1: 81.8724\n",
            "[300]\ttraining's l1: 80.3561\tvalid_1's l1: 81.8723\n",
            "[400]\ttraining's l1: 80.356\tvalid_1's l1: 81.8723\n",
            "[500]\ttraining's l1: 80.3559\tvalid_1's l1: 81.8722\n",
            "[600]\ttraining's l1: 80.3559\tvalid_1's l1: 81.8722\n",
            "[700]\ttraining's l1: 80.3558\tvalid_1's l1: 81.8721\n",
            "[800]\ttraining's l1: 80.3558\tvalid_1's l1: 81.8721\n",
            "[900]\ttraining's l1: 80.3557\tvalid_1's l1: 81.872\n",
            "[1000]\ttraining's l1: 80.3557\tvalid_1's l1: 81.872\n",
            "[1100]\ttraining's l1: 80.3556\tvalid_1's l1: 81.8719\n",
            "[1200]\ttraining's l1: 80.3556\tvalid_1's l1: 81.8719\n",
            "[1300]\ttraining's l1: 80.3555\tvalid_1's l1: 81.8718\n",
            "[1400]\ttraining's l1: 80.3555\tvalid_1's l1: 81.8718\n",
            "[1500]\ttraining's l1: 80.3554\tvalid_1's l1: 81.8717\n",
            "[1600]\ttraining's l1: 80.3554\tvalid_1's l1: 81.8716\n",
            "[1700]\ttraining's l1: 80.3553\tvalid_1's l1: 81.8716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:20:16,819] Trial 9 finished with value: 81.87157514725558 and parameters: {'max_depth': 9, 'learning_rate': 1.8194678087880884e-08, 'n_estimators': 1728, 'min_child_samples': 36, 'subsample': 0.6341771899009377}. Best is trial 0 with value: 80.08405635148821.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 76.1994\tvalid_1's l1: 77.6754\n",
            "[200]\ttraining's l1: 73.0778\tvalid_1's l1: 74.5244\n",
            "[300]\ttraining's l1: 70.7567\tvalid_1's l1: 72.1795\n",
            "[400]\ttraining's l1: 68.9962\tvalid_1's l1: 70.4026\n",
            "[500]\ttraining's l1: 67.5395\tvalid_1's l1: 68.9403\n",
            "[600]\ttraining's l1: 66.2233\tvalid_1's l1: 67.6249\n",
            "[700]\ttraining's l1: 65.1266\tvalid_1's l1: 66.5123\n",
            "[800]\ttraining's l1: 64.2164\tvalid_1's l1: 65.5938\n",
            "[900]\ttraining's l1: 63.5034\tvalid_1's l1: 64.8679\n",
            "[1000]\ttraining's l1: 62.9338\tvalid_1's l1: 64.2823\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:20:28,811] Trial 10 finished with value: 64.0725071809224 and parameters: {'max_depth': 3, 'learning_rate': 0.0023074972749146296, 'n_estimators': 1039, 'min_child_samples': 99, 'subsample': 0.4330776943512126}. Best is trial 10 with value: 64.0725071809224.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 75.7987\tvalid_1's l1: 77.2688\n",
            "[200]\ttraining's l1: 72.5245\tvalid_1's l1: 73.9648\n",
            "[300]\ttraining's l1: 70.1496\tvalid_1's l1: 71.5664\n",
            "[400]\ttraining's l1: 68.3654\tvalid_1's l1: 69.7668\n",
            "[500]\ttraining's l1: 66.7928\tvalid_1's l1: 68.1945\n",
            "[600]\ttraining's l1: 65.499\tvalid_1's l1: 66.8901\n",
            "[700]\ttraining's l1: 64.4371\tvalid_1's l1: 65.8158\n",
            "[800]\ttraining's l1: 63.5902\tvalid_1's l1: 64.957\n",
            "[900]\ttraining's l1: 62.9611\tvalid_1's l1: 64.3093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:20:38,331] Trial 11 finished with value: 63.83984355763432 and parameters: {'max_depth': 3, 'learning_rate': 0.002552138181406136, 'n_estimators': 982, 'min_child_samples': 97, 'subsample': 0.4060724173367089}. Best is trial 11 with value: 63.83984355763432.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 69.5628\tvalid_1's l1: 70.9735\n",
            "[200]\ttraining's l1: 64.8566\tvalid_1's l1: 66.2357\n",
            "[300]\ttraining's l1: 62.4787\tvalid_1's l1: 63.8155\n",
            "[400]\ttraining's l1: 61.3394\tvalid_1's l1: 62.6276\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:20:43,489] Trial 12 finished with value: 62.502140748426896 and parameters: {'max_depth': 3, 'learning_rate': 0.008384679572702159, 'n_estimators': 1024, 'min_child_samples': 99, 'subsample': 0.4051837370550797}. Best is trial 12 with value: 62.502140748426896.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 68.922\tvalid_1's l1: 70.2917\n",
            "[200]\ttraining's l1: 63.8721\tvalid_1's l1: 65.2024\n",
            "[300]\ttraining's l1: 60.9136\tvalid_1's l1: 62.2023\n",
            "[400]\ttraining's l1: 59.4925\tvalid_1's l1: 60.7793\n",
            "[500]\ttraining's l1: 58.993\tvalid_1's l1: 60.2626\n",
            "[600]\ttraining's l1: 58.9531\tvalid_1's l1: 60.2277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:20:50,713] Trial 13 finished with value: 60.19878410447363 and parameters: {'max_depth': 4, 'learning_rate': 0.007714473041412747, 'n_estimators': 915, 'min_child_samples': 90, 'subsample': 0.40208137391685717}. Best is trial 13 with value: 60.19878410447363.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 67.3743\tvalid_1's l1: 68.7488\n",
            "[200]\ttraining's l1: 61.8671\tvalid_1's l1: 63.1864\n",
            "[300]\ttraining's l1: 59.0624\tvalid_1's l1: 60.4326\n",
            "[400]\ttraining's l1: 57.717\tvalid_1's l1: 59.1276\n",
            "[500]\ttraining's l1: 57.2714\tvalid_1's l1: 58.7089\n",
            "[600]\ttraining's l1: 57.2033\tvalid_1's l1: 58.6624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-19 18:21:00,334] Trial 14 finished with value: 58.65799622951135 and parameters: {'max_depth': 5, 'learning_rate': 0.008645764928797792, 'n_estimators': 770, 'min_child_samples': 60, 'subsample': 0.46901968548177947}. Best is trial 14 with value: 58.65799622951135.\n",
            "[W 2023-10-19 18:21:02,181] Trial 15 failed with parameters: {'max_depth': 5, 'learning_rate': 0.000959363518233158, 'n_estimators': 607, 'min_child_samples': 59, 'subsample': 0.4837536242763549} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\ineeji\\AppData\\Local\\Temp\\ipykernel_47296\\3448921983.py\", line 12, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=20)\n",
            "  File \"C:\\Users\\ineeji\\AppData\\Local\\Temp\\ipykernel_47296\\664658558.py\", line 25, in objective\n",
            "    model.fit(X_train, y_train,\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\sklearn.py\", line 895, in fit\n",
            "    f\"match the input. Model n_features_ is {self._n_features} and \"\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
            "    params = self._process_params(stage=\"fit\")\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\engine.py\", line 292, in train\n",
            "    except callback.EarlyStopException as earlyStopException:\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\basic.py\", line 3021, in update\n",
            "    _safe_call(_LIB.LGBM_DatasetAddFeaturesFrom(self._handle, other._handle))\n",
            "KeyboardInterrupt\n",
            "[W 2023-10-19 18:21:02,189] Trial 15 failed with value None.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100]\ttraining's l1: 78.1032\tvalid_1's l1: 79.5933\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\Special_VAL.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m1234\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: objective(trial, X_train, y_train, X_val, y_val), n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m     _optimize(\n\u001b[0;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    453\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\Special_VAL.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m1234\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val), n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "\u001b[1;32mc:\\Users\\ineeji\\Desktop\\HD\\Special_VAL.ipynb Cell 40\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m param \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# 회귀\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msubsample\u001b[39m\u001b[39m'\u001b[39m: trial\u001b[39m.\u001b[39msuggest_loguniform(\u001b[39m'\u001b[39m\u001b[39msubsample\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.4\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mLGBMRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparam)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         eval_set\u001b[39m=\u001b[39;49m[(X_train, y_train), (X_val,y_val)],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ineeji/Desktop/HD/Special_VAL.ipynb#X61sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m mae \u001b[39m=\u001b[39m mean_absolute_error(y_val, y_pred)\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\sklearn.py:895\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y,\n\u001b[0;32m    889\u001b[0m         sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, init_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    890\u001b[0m         eval_set\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_names\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    891\u001b[0m         eval_init_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_metric\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    892\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m'\u001b[39m, feature_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, categorical_feature\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    893\u001b[0m         callbacks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, init_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    894\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 895\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight, init_score\u001b[39m=\u001b[39;49minit_score,\n\u001b[0;32m    896\u001b[0m                 eval_set\u001b[39m=\u001b[39;49meval_set, eval_names\u001b[39m=\u001b[39;49meval_names, eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n\u001b[0;32m    897\u001b[0m                 eval_init_score\u001b[39m=\u001b[39;49meval_init_score, eval_metric\u001b[39m=\u001b[39;49meval_metric,\n\u001b[0;32m    898\u001b[0m                 early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds, verbose\u001b[39m=\u001b[39;49mverbose, feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[0;32m    899\u001b[0m                 categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature, callbacks\u001b[39m=\u001b[39;49mcallbacks, init_model\u001b[39m=\u001b[39;49minit_model)\n\u001b[0;32m    900\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[39m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[39m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m    749\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    750\u001b[0m     train_set\u001b[39m=\u001b[39;49mtrain_set,\n\u001b[0;32m    751\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators,\n\u001b[0;32m    752\u001b[0m     valid_sets\u001b[39m=\u001b[39;49mvalid_sets,\n\u001b[0;32m    753\u001b[0m     valid_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[0;32m    754\u001b[0m     fobj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fobj,\n\u001b[0;32m    755\u001b[0m     feval\u001b[39m=\u001b[39;49meval_metrics_callable,\n\u001b[0;32m    756\u001b[0m     init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[0;32m    757\u001b[0m     feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[0;32m    758\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[0;32m    759\u001b[0m )\n\u001b[0;32m    761\u001b[0m \u001b[39mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evals_result \u001b[39m=\u001b[39m evals_result\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[0;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=10, shuffle=True, random_state=3732)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "# 각 Fold에 대해서 Optuna로 하이퍼파라미터 튜닝\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index, :], train_y.iloc[val_index, :]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=1234))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best MAE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = lgb.LGBMRegressor(**best_params)\n",
        "    model.fit(X_train, y_train)\n",
        "    test_pred = model.predict(test)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "# 평균 앙상블\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "# 결과 출력\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")\n",
        "\n",
        "# 최종 예측 저장\n",
        "final_prediction_df = pd.DataFrame(final_prediction, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = final_prediction"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
