{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kvY2HmDexFj3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets, ensemble\n",
        "from catboost import CatBoostRegressor\n",
        "from tqdm import tqdm\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "import itertools\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>LENGTH</th>\n",
              "      <th>SHIPMANAGER</th>\n",
              "      <th>FLAG</th>\n",
              "      <th>U_WIND</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>30.881018</td>\n",
              "      <td>2018-12-17 21:29</td>\n",
              "      <td>Z618338</td>\n",
              "      <td>30.0</td>\n",
              "      <td>24</td>\n",
              "      <td>24300</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>CQSB78</td>\n",
              "      <td>Panama</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>3.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>IN</td>\n",
              "      <td>UJM2</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2014-09-23 6:59</td>\n",
              "      <td>X886125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>13</td>\n",
              "      <td>35900</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>SPNO34</td>\n",
              "      <td>Marshall Islands</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-02-03 22:00</td>\n",
              "      <td>T674582</td>\n",
              "      <td>50.0</td>\n",
              "      <td>12</td>\n",
              "      <td>146000</td>\n",
              "      <td>...</td>\n",
              "      <td>370.0</td>\n",
              "      <td>FNPK22</td>\n",
              "      <td>Malta</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>ZAG4</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-01-17 4:02</td>\n",
              "      <td>Y847238</td>\n",
              "      <td>20.0</td>\n",
              "      <td>18</td>\n",
              "      <td>6910</td>\n",
              "      <td>...</td>\n",
              "      <td>120.0</td>\n",
              "      <td>PBZV77</td>\n",
              "      <td>Bahamas</td>\n",
              "      <td>-3.18</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.629350</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>27.037650</td>\n",
              "      <td>2020-01-26 7:51</td>\n",
              "      <td>A872328</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10</td>\n",
              "      <td>116000</td>\n",
              "      <td>...</td>\n",
              "      <td>300.0</td>\n",
              "      <td>GUCE76</td>\n",
              "      <td>Liberia</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>25.6</td>\n",
              "      <td>2.495953</td>\n",
              "      <td>15</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>253.554444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391934</th>\n",
              "      <td>TRAIN_391934</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2017-06-06 5:02</td>\n",
              "      <td>Y375615</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>6820</td>\n",
              "      <td>...</td>\n",
              "      <td>110.0</td>\n",
              "      <td>KEJZ24</td>\n",
              "      <td>China, People's Republic Of</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391935</th>\n",
              "      <td>TRAIN_391935</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>5.884603</td>\n",
              "      <td>2019-10-16 0:36</td>\n",
              "      <td>K635567</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12</td>\n",
              "      <td>3160</td>\n",
              "      <td>...</td>\n",
              "      <td>80.0</td>\n",
              "      <td>JLTM64</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>27.3</td>\n",
              "      <td>1.253491</td>\n",
              "      <td>8</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>144.061389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391936</th>\n",
              "      <td>TRAIN_391936</td>\n",
              "      <td>US</td>\n",
              "      <td>QGN3</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>70.660241</td>\n",
              "      <td>2021-03-23 22:35</td>\n",
              "      <td>J284147</td>\n",
              "      <td>30.0</td>\n",
              "      <td>8</td>\n",
              "      <td>60300</td>\n",
              "      <td>...</td>\n",
              "      <td>200.0</td>\n",
              "      <td>YERJ68</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>-3.44</td>\n",
              "      <td>7.99</td>\n",
              "      <td>21.1</td>\n",
              "      <td>4.766257</td>\n",
              "      <td>18</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>41.482222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391937</th>\n",
              "      <td>TRAIN_391937</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>9.448179</td>\n",
              "      <td>2015-01-08 7:15</td>\n",
              "      <td>J644215</td>\n",
              "      <td>30.0</td>\n",
              "      <td>29</td>\n",
              "      <td>23800</td>\n",
              "      <td>...</td>\n",
              "      <td>170.0</td>\n",
              "      <td>HCZK58</td>\n",
              "      <td>Comoros</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>7.485278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391938</th>\n",
              "      <td>TRAIN_391938</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-06-08 23:30</td>\n",
              "      <td>D123358</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>50600</td>\n",
              "      <td>...</td>\n",
              "      <td>260.0</td>\n",
              "      <td>GRJG55</td>\n",
              "      <td>Hong Kong, China</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>391939 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     SG   GIW5          Container  30.881018   \n",
              "1       TRAIN_000001     IN   UJM2               Bulk   0.000000   \n",
              "2       TRAIN_000002     CN   EUC8          Container   0.000000   \n",
              "3       TRAIN_000003     JP   ZAG4          Container   0.000000   \n",
              "4       TRAIN_000004     SG   GIW5          Container  27.037650   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "391934  TRAIN_391934     JP   QYY1          Container   0.000000   \n",
              "391935  TRAIN_391935     SG   GIW5               Bulk   5.884603   \n",
              "391936  TRAIN_391936     US   QGN3               Bulk  70.660241   \n",
              "391937  TRAIN_391937     TW   JWI3          Container   9.448179   \n",
              "391938  TRAIN_391938     TW   JWI3          Container   0.000000   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  LENGTH  \\\n",
              "0       2018-12-17 21:29  Z618338     30.0     24       24300  ...   180.0   \n",
              "1        2014-09-23 6:59  X886125     30.0     13       35900  ...   180.0   \n",
              "2       2015-02-03 22:00  T674582     50.0     12      146000  ...   370.0   \n",
              "3        2020-01-17 4:02  Y847238     20.0     18        6910  ...   120.0   \n",
              "4        2020-01-26 7:51  A872328     50.0     10      116000  ...   300.0   \n",
              "...                  ...      ...      ...    ...         ...  ...     ...   \n",
              "391934   2017-06-06 5:02  Y375615     20.0     27        6820  ...   110.0   \n",
              "391935   2019-10-16 0:36  K635567     10.0     12        3160  ...    80.0   \n",
              "391936  2021-03-23 22:35  J284147     30.0      8       60300  ...   200.0   \n",
              "391937   2015-01-08 7:15  J644215     30.0     29       23800  ...   170.0   \n",
              "391938  2015-06-08 23:30  D123358     30.0     15       50600  ...   260.0   \n",
              "\n",
              "        SHIPMANAGER                         FLAG  U_WIND V_WIND  \\\n",
              "0            CQSB78                       Panama     NaN    NaN   \n",
              "1            SPNO34             Marshall Islands     NaN    NaN   \n",
              "2            FNPK22                        Malta     NaN    NaN   \n",
              "3            PBZV77                      Bahamas   -3.18  -1.61   \n",
              "4            GUCE76                      Liberia   -0.33  -3.28   \n",
              "...             ...                          ...     ...    ...   \n",
              "391934       KEJZ24  China, People's Republic Of     NaN    NaN   \n",
              "391935       JLTM64                      Vietnam   -0.66   0.97   \n",
              "391936       YERJ68                    Singapore   -3.44   7.99   \n",
              "391937       HCZK58                      Comoros     NaN    NaN   \n",
              "391938       GRJG55             Hong Kong, China     NaN    NaN   \n",
              "\n",
              "       AIR_TEMPERATURE        BN  ATA_LT  PORT_SIZE     CI_HOUR  \n",
              "0                  NaN       NaN       5   0.002615    3.450000  \n",
              "1                  NaN       NaN      12   0.000217    0.000000  \n",
              "2                  NaN       NaN       6   0.001614    0.000000  \n",
              "3                  6.7  2.629350      13   0.000356    0.000000  \n",
              "4                 25.6  2.495953      15   0.002615  253.554444  \n",
              "...                ...       ...     ...        ...         ...  \n",
              "391934             NaN       NaN      14   0.000552    0.000000  \n",
              "391935            27.3  1.253491       8   0.002615  144.061389  \n",
              "391936            21.1  4.766257      18   0.000155   41.482222  \n",
              "391937             NaN       NaN      15   0.000990    7.485278  \n",
              "391938             NaN       NaN       7   0.000990    0.000000  \n",
              "\n",
              "[391939 rows x 23 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>SHIPMANAGER</th>\n",
              "      <th>FLAG</th>\n",
              "      <th>U_WIND</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>종가</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>30.881018</td>\n",
              "      <td>2018-12-17 21:29</td>\n",
              "      <td>Z618338</td>\n",
              "      <td>30.0</td>\n",
              "      <td>24</td>\n",
              "      <td>24300</td>\n",
              "      <td>...</td>\n",
              "      <td>CQSB78</td>\n",
              "      <td>Panama</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>3.450000</td>\n",
              "      <td>1129.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000001</td>\n",
              "      <td>IN</td>\n",
              "      <td>UJM2</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2014-09-23 6:59</td>\n",
              "      <td>X886125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>13</td>\n",
              "      <td>35900</td>\n",
              "      <td>...</td>\n",
              "      <td>SPNO34</td>\n",
              "      <td>Marshall Islands</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1040.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000002</td>\n",
              "      <td>CN</td>\n",
              "      <td>EUC8</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-02-03 22:00</td>\n",
              "      <td>T674582</td>\n",
              "      <td>50.0</td>\n",
              "      <td>12</td>\n",
              "      <td>146000</td>\n",
              "      <td>...</td>\n",
              "      <td>FNPK22</td>\n",
              "      <td>Malta</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1093.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000003</td>\n",
              "      <td>JP</td>\n",
              "      <td>ZAG4</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2020-01-17 4:02</td>\n",
              "      <td>Y847238</td>\n",
              "      <td>20.0</td>\n",
              "      <td>18</td>\n",
              "      <td>6910</td>\n",
              "      <td>...</td>\n",
              "      <td>PBZV77</td>\n",
              "      <td>Bahamas</td>\n",
              "      <td>-3.18</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.629350</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1160.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>27.037650</td>\n",
              "      <td>2020-01-26 7:51</td>\n",
              "      <td>A872328</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10</td>\n",
              "      <td>116000</td>\n",
              "      <td>...</td>\n",
              "      <td>GUCE76</td>\n",
              "      <td>Liberia</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>25.6</td>\n",
              "      <td>2.495953</td>\n",
              "      <td>15</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>253.554444</td>\n",
              "      <td>1169.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391934</th>\n",
              "      <td>TRAIN_391934</td>\n",
              "      <td>JP</td>\n",
              "      <td>QYY1</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2017-06-06 5:02</td>\n",
              "      <td>Y375615</td>\n",
              "      <td>20.0</td>\n",
              "      <td>27</td>\n",
              "      <td>6820</td>\n",
              "      <td>...</td>\n",
              "      <td>KEJZ24</td>\n",
              "      <td>China, People's Republic Of</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1118.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391935</th>\n",
              "      <td>TRAIN_391935</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>5.884603</td>\n",
              "      <td>2019-10-16 0:36</td>\n",
              "      <td>K635567</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12</td>\n",
              "      <td>3160</td>\n",
              "      <td>...</td>\n",
              "      <td>JLTM64</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>27.3</td>\n",
              "      <td>1.253491</td>\n",
              "      <td>8</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>144.061389</td>\n",
              "      <td>1185.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391936</th>\n",
              "      <td>TRAIN_391936</td>\n",
              "      <td>US</td>\n",
              "      <td>QGN3</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>70.660241</td>\n",
              "      <td>2021-03-23 22:35</td>\n",
              "      <td>J284147</td>\n",
              "      <td>30.0</td>\n",
              "      <td>8</td>\n",
              "      <td>60300</td>\n",
              "      <td>...</td>\n",
              "      <td>YERJ68</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>-3.44</td>\n",
              "      <td>7.99</td>\n",
              "      <td>21.1</td>\n",
              "      <td>4.766257</td>\n",
              "      <td>18</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>41.482222</td>\n",
              "      <td>1132.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391937</th>\n",
              "      <td>TRAIN_391937</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>9.448179</td>\n",
              "      <td>2015-01-08 7:15</td>\n",
              "      <td>J644215</td>\n",
              "      <td>30.0</td>\n",
              "      <td>29</td>\n",
              "      <td>23800</td>\n",
              "      <td>...</td>\n",
              "      <td>HCZK58</td>\n",
              "      <td>Comoros</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>7.485278</td>\n",
              "      <td>1095.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391938</th>\n",
              "      <td>TRAIN_391938</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2015-06-08 23:30</td>\n",
              "      <td>D123358</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15</td>\n",
              "      <td>50600</td>\n",
              "      <td>...</td>\n",
              "      <td>GRJG55</td>\n",
              "      <td>Hong Kong, China</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1118.87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>391939 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY       DIST  \\\n",
              "0       TRAIN_000000     SG   GIW5          Container  30.881018   \n",
              "1       TRAIN_000001     IN   UJM2               Bulk   0.000000   \n",
              "2       TRAIN_000002     CN   EUC8          Container   0.000000   \n",
              "3       TRAIN_000003     JP   ZAG4          Container   0.000000   \n",
              "4       TRAIN_000004     SG   GIW5          Container  27.037650   \n",
              "...              ...    ...    ...                ...        ...   \n",
              "391934  TRAIN_391934     JP   QYY1          Container   0.000000   \n",
              "391935  TRAIN_391935     SG   GIW5               Bulk   5.884603   \n",
              "391936  TRAIN_391936     US   QGN3               Bulk  70.660241   \n",
              "391937  TRAIN_391937     TW   JWI3          Container   9.448179   \n",
              "391938  TRAIN_391938     TW   JWI3          Container   0.000000   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  \\\n",
              "0       2018-12-17 21:29  Z618338     30.0     24       24300  ...   \n",
              "1        2014-09-23 6:59  X886125     30.0     13       35900  ...   \n",
              "2       2015-02-03 22:00  T674582     50.0     12      146000  ...   \n",
              "3        2020-01-17 4:02  Y847238     20.0     18        6910  ...   \n",
              "4        2020-01-26 7:51  A872328     50.0     10      116000  ...   \n",
              "...                  ...      ...      ...    ...         ...  ...   \n",
              "391934   2017-06-06 5:02  Y375615     20.0     27        6820  ...   \n",
              "391935   2019-10-16 0:36  K635567     10.0     12        3160  ...   \n",
              "391936  2021-03-23 22:35  J284147     30.0      8       60300  ...   \n",
              "391937   2015-01-08 7:15  J644215     30.0     29       23800  ...   \n",
              "391938  2015-06-08 23:30  D123358     30.0     15       50600  ...   \n",
              "\n",
              "        SHIPMANAGER                         FLAG  U_WIND  V_WIND  \\\n",
              "0            CQSB78                       Panama     NaN     NaN   \n",
              "1            SPNO34             Marshall Islands     NaN     NaN   \n",
              "2            FNPK22                        Malta     NaN     NaN   \n",
              "3            PBZV77                      Bahamas   -3.18   -1.61   \n",
              "4            GUCE76                      Liberia   -0.33   -3.28   \n",
              "...             ...                          ...     ...     ...   \n",
              "391934       KEJZ24  China, People's Republic Of     NaN     NaN   \n",
              "391935       JLTM64                      Vietnam   -0.66    0.97   \n",
              "391936       YERJ68                    Singapore   -3.44    7.99   \n",
              "391937       HCZK58                      Comoros     NaN     NaN   \n",
              "391938       GRJG55             Hong Kong, China     NaN     NaN   \n",
              "\n",
              "       AIR_TEMPERATURE        BN  ATA_LT  PORT_SIZE     CI_HOUR       종가  \n",
              "0                  NaN       NaN       5   0.002615    3.450000  1129.55  \n",
              "1                  NaN       NaN      12   0.000217    0.000000  1040.15  \n",
              "2                  NaN       NaN       6   0.001614    0.000000  1093.04  \n",
              "3                  6.7  2.629350      13   0.000356    0.000000  1160.44  \n",
              "4                 25.6  2.495953      15   0.002615  253.554444  1169.20  \n",
              "...                ...       ...     ...        ...         ...      ...  \n",
              "391934             NaN       NaN      14   0.000552    0.000000  1118.08  \n",
              "391935            27.3  1.253491       8   0.002615  144.061389  1185.51  \n",
              "391936            21.1  4.766257      18   0.000155   41.482222  1132.90  \n",
              "391937             NaN       NaN      15   0.000990    7.485278  1095.51  \n",
              "391938             NaN       NaN       7   0.000990    0.000000  1118.87  \n",
              "\n",
              "[391939 rows x 24 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Load the USD data\n",
        "usd_data = pd.read_csv(\"USD.csv\")\n",
        "\n",
        "# 2. Convert the '날짜' column in USD data to datetime format\n",
        "#usd_data['날짜'] = pd.to_datetime(usd_data['날짜'], format='%Y- %m- %d').dt.date\n",
        "usd_data['날짜'] = pd.to_datetime(usd_data['날짜'], format='%Y-%m-%d').dt.date\n",
        "\n",
        "# Convert the 'ATA' column in train and test data to only the date format\n",
        "train['ATA_date'] = pd.to_datetime(train['ATA']).dt.date\n",
        "test['ATA_date'] = pd.to_datetime(test['ATA']).dt.date\n",
        "\n",
        "# 3. Merge the USD data with train and test data on the date columns\n",
        "train_merged = pd.merge(train, usd_data, left_on='ATA_date', right_on='날짜', how='left')\n",
        "test_merged = pd.merge(test, usd_data, left_on='ATA_date', right_on='날짜', how='left')\n",
        "\n",
        "# Drop the '날짜' and 'ATA_date' columns from merged datasets as they are redundant\n",
        "train_merged.drop(['날짜', 'ATA_date'], axis=1, inplace=True)\n",
        "test_merged.drop(['날짜', 'ATA_date'], axis=1, inplace=True)\n",
        "train_merged.drop(['거래량', '변동 %'],axis=1,inplace=True)\n",
        "test_merged.drop(['거래량', '변동 %'],axis=1,inplace=True)\n",
        "train_merged.drop(['시가', '고가', '저가'],axis=1,inplace=True)\n",
        "test_merged.drop(['시가', '고가', '저가'],axis=1,inplace=True)\n",
        "train_merged['종가'] = train_merged['종가'].str.replace(',', '').astype(float)\n",
        "test_merged['종가'] = test_merged['종가'].str.replace(',', '').astype(float)\n",
        "train = train_merged.copy()\n",
        "test = test_merged.copy()\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0포함 o x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ATA</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>SHIPMANAGER</th>\n",
              "      <th>FLAG</th>\n",
              "      <th>U_WIND</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>종가</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_000000</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>30.881018</td>\n",
              "      <td>2018-12-17 21:29</td>\n",
              "      <td>Z618338</td>\n",
              "      <td>30.0</td>\n",
              "      <td>24</td>\n",
              "      <td>24300</td>\n",
              "      <td>...</td>\n",
              "      <td>CQSB78</td>\n",
              "      <td>Panama</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>3.450000</td>\n",
              "      <td>1129.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_000004</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Container</td>\n",
              "      <td>27.037650</td>\n",
              "      <td>2020-01-26 7:51</td>\n",
              "      <td>A872328</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10</td>\n",
              "      <td>116000</td>\n",
              "      <td>...</td>\n",
              "      <td>GUCE76</td>\n",
              "      <td>Liberia</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>25.6</td>\n",
              "      <td>2.495953</td>\n",
              "      <td>15</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>253.554444</td>\n",
              "      <td>1169.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_000005</td>\n",
              "      <td>AU</td>\n",
              "      <td>WHH4</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>49.953585</td>\n",
              "      <td>2021-03-05 18:36</td>\n",
              "      <td>S731836</td>\n",
              "      <td>40.0</td>\n",
              "      <td>7</td>\n",
              "      <td>183000</td>\n",
              "      <td>...</td>\n",
              "      <td>HZUO14</td>\n",
              "      <td>Japan</td>\n",
              "      <td>6.10</td>\n",
              "      <td>-2.84</td>\n",
              "      <td>28.1</td>\n",
              "      <td>4.016217</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>68.391389</td>\n",
              "      <td>1128.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_000006</td>\n",
              "      <td>ID</td>\n",
              "      <td>REJ1</td>\n",
              "      <td>Container</td>\n",
              "      <td>42.276281</td>\n",
              "      <td>2016-12-11 3:00</td>\n",
              "      <td>A735263</td>\n",
              "      <td>20.0</td>\n",
              "      <td>30</td>\n",
              "      <td>6800</td>\n",
              "      <td>...</td>\n",
              "      <td>HCOS27</td>\n",
              "      <td>Indonesia</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>31.700556</td>\n",
              "      <td>1172.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_000009</td>\n",
              "      <td>CN</td>\n",
              "      <td>NGG6</td>\n",
              "      <td>Container</td>\n",
              "      <td>101.521598</td>\n",
              "      <td>2018-11-30 19:29</td>\n",
              "      <td>S458225</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>124000</td>\n",
              "      <td>...</td>\n",
              "      <td>YLMR26</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>58.193056</td>\n",
              "      <td>1120.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234641</th>\n",
              "      <td>TRAIN_391932</td>\n",
              "      <td>CN</td>\n",
              "      <td>WAF5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>21.866691</td>\n",
              "      <td>2017-06-01 15:27</td>\n",
              "      <td>V427658</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>169000</td>\n",
              "      <td>...</td>\n",
              "      <td>TNIW71</td>\n",
              "      <td>Marshall Islands</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>23</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>89.884167</td>\n",
              "      <td>1122.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234642</th>\n",
              "      <td>TRAIN_391933</td>\n",
              "      <td>CN</td>\n",
              "      <td>NCU8</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>9.647703</td>\n",
              "      <td>2020-03-02 13:37</td>\n",
              "      <td>H321245</td>\n",
              "      <td>30.0</td>\n",
              "      <td>26</td>\n",
              "      <td>69000</td>\n",
              "      <td>...</td>\n",
              "      <td>PPMJ71</td>\n",
              "      <td>Panama</td>\n",
              "      <td>3.64</td>\n",
              "      <td>3.44</td>\n",
              "      <td>21.4</td>\n",
              "      <td>3.298553</td>\n",
              "      <td>21</td>\n",
              "      <td>0.000939</td>\n",
              "      <td>1095.597222</td>\n",
              "      <td>1189.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234643</th>\n",
              "      <td>TRAIN_391935</td>\n",
              "      <td>SG</td>\n",
              "      <td>GIW5</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>5.884603</td>\n",
              "      <td>2019-10-16 0:36</td>\n",
              "      <td>K635567</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12</td>\n",
              "      <td>3160</td>\n",
              "      <td>...</td>\n",
              "      <td>JLTM64</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>27.3</td>\n",
              "      <td>1.253491</td>\n",
              "      <td>8</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>144.061389</td>\n",
              "      <td>1185.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234644</th>\n",
              "      <td>TRAIN_391936</td>\n",
              "      <td>US</td>\n",
              "      <td>QGN3</td>\n",
              "      <td>Bulk</td>\n",
              "      <td>70.660241</td>\n",
              "      <td>2021-03-23 22:35</td>\n",
              "      <td>J284147</td>\n",
              "      <td>30.0</td>\n",
              "      <td>8</td>\n",
              "      <td>60300</td>\n",
              "      <td>...</td>\n",
              "      <td>YERJ68</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>-3.44</td>\n",
              "      <td>7.99</td>\n",
              "      <td>21.1</td>\n",
              "      <td>4.766257</td>\n",
              "      <td>18</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>41.482222</td>\n",
              "      <td>1132.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234645</th>\n",
              "      <td>TRAIN_391937</td>\n",
              "      <td>TW</td>\n",
              "      <td>JWI3</td>\n",
              "      <td>Container</td>\n",
              "      <td>9.448179</td>\n",
              "      <td>2015-01-08 7:15</td>\n",
              "      <td>J644215</td>\n",
              "      <td>30.0</td>\n",
              "      <td>29</td>\n",
              "      <td>23800</td>\n",
              "      <td>...</td>\n",
              "      <td>HCZK58</td>\n",
              "      <td>Comoros</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>7.485278</td>\n",
              "      <td>1095.51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>234646 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           SAMPLE_ID ARI_CO ARI_PO SHIP_TYPE_CATEGORY        DIST  \\\n",
              "0       TRAIN_000000     SG   GIW5          Container   30.881018   \n",
              "1       TRAIN_000004     SG   GIW5          Container   27.037650   \n",
              "2       TRAIN_000005     AU   WHH4               Bulk   49.953585   \n",
              "3       TRAIN_000006     ID   REJ1          Container   42.276281   \n",
              "4       TRAIN_000009     CN   NGG6          Container  101.521598   \n",
              "...              ...    ...    ...                ...         ...   \n",
              "234641  TRAIN_391932     CN   WAF5               Bulk   21.866691   \n",
              "234642  TRAIN_391933     CN   NCU8               Bulk    9.647703   \n",
              "234643  TRAIN_391935     SG   GIW5               Bulk    5.884603   \n",
              "234644  TRAIN_391936     US   QGN3               Bulk   70.660241   \n",
              "234645  TRAIN_391937     TW   JWI3          Container    9.448179   \n",
              "\n",
              "                     ATA       ID  BREADTH  BUILT  DEADWEIGHT  ...  \\\n",
              "0       2018-12-17 21:29  Z618338     30.0     24       24300  ...   \n",
              "1        2020-01-26 7:51  A872328     50.0     10      116000  ...   \n",
              "2       2021-03-05 18:36  S731836     40.0      7      183000  ...   \n",
              "3        2016-12-11 3:00  A735263     20.0     30        6800  ...   \n",
              "4       2018-11-30 19:29  S458225     50.0      7      124000  ...   \n",
              "...                  ...      ...      ...    ...         ...  ...   \n",
              "234641  2017-06-01 15:27  V427658     40.0     13      169000  ...   \n",
              "234642  2020-03-02 13:37  H321245     30.0     26       69000  ...   \n",
              "234643   2019-10-16 0:36  K635567     10.0     12        3160  ...   \n",
              "234644  2021-03-23 22:35  J284147     30.0      8       60300  ...   \n",
              "234645   2015-01-08 7:15  J644215     30.0     29       23800  ...   \n",
              "\n",
              "        SHIPMANAGER              FLAG  U_WIND  V_WIND AIR_TEMPERATURE  \\\n",
              "0            CQSB78            Panama     NaN     NaN             NaN   \n",
              "1            GUCE76           Liberia   -0.33   -3.28            25.6   \n",
              "2            HZUO14             Japan    6.10   -2.84            28.1   \n",
              "3            HCOS27         Indonesia     NaN     NaN             NaN   \n",
              "4            YLMR26    United Kingdom     NaN     NaN             NaN   \n",
              "...             ...               ...     ...     ...             ...   \n",
              "234641       TNIW71  Marshall Islands     NaN     NaN             NaN   \n",
              "234642       PPMJ71            Panama    3.64    3.44            21.4   \n",
              "234643       JLTM64           Vietnam   -0.66    0.97            27.3   \n",
              "234644       YERJ68         Singapore   -3.44    7.99            21.1   \n",
              "234645       HCZK58           Comoros     NaN     NaN             NaN   \n",
              "\n",
              "              BN  ATA_LT  PORT_SIZE      CI_HOUR       종가  \n",
              "0            NaN       5   0.002615     3.450000  1129.55  \n",
              "1       2.495953      15   0.002615   253.554444  1169.20  \n",
              "2       4.016217       5   0.000103    68.391389  1128.00  \n",
              "3            NaN      10   0.000041    31.700556  1172.89  \n",
              "4            NaN       3   0.001743    58.193056  1120.44  \n",
              "...          ...     ...        ...          ...      ...  \n",
              "234641       NaN      23   0.000618    89.884167  1122.13  \n",
              "234642  3.298553      21   0.000939  1095.597222  1189.61  \n",
              "234643  1.253491       8   0.002615   144.061389  1185.51  \n",
              "234644  4.766257      18   0.000155    41.482222  1132.90  \n",
              "234645       NaN      15   0.000990     7.485278  1095.51  \n",
              "\n",
              "[234646 rows x 24 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = train[train['DIST'] != 0]\n",
        "train = train.reset_index(drop = True)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.drop(['SAMPLE_ID'],axis=1,inplace=True)\n",
        "test.drop(['SAMPLE_ID'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# datetime 컬럼 처리\n",
        "train['ATA'] = pd.to_datetime(train['ATA'])\n",
        "test['ATA'] = pd.to_datetime(test['ATA'])\n",
        "\n",
        "# datetime을 여러 파생 변수로 변환\n",
        "for df in [train, test]:\n",
        "    df['year'] = df['ATA'].dt.year\n",
        "    df['month'] = df['ATA'].dt.month\n",
        "    df['day'] = df['ATA'].dt.day\n",
        "    #df['hour'] = df['ATA'].dt.hour\n",
        "    #df['minute'] = df['ATA'].dt.minute\n",
        "    df['weekday'] = df['ATA'].dt.weekday\n",
        "\n",
        "# datetime 컬럼 제거\n",
        "train.drop(columns='ATA', inplace=True)\n",
        "test.drop(columns='ATA', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding features:   0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding features: 100%|██████████| 6/6 [00:11<00:00,  1.89s/it]\n"
          ]
        }
      ],
      "source": [
        "# Categorical 컬럼 인코딩\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "\n",
        "\n",
        "for feature in tqdm(categorical_features, desc=\"Encoding features\"):\n",
        "    encoder = LabelEncoder()\n",
        "    train[feature] = encoder.fit_transform(train[feature])\n",
        "    for label in np.unique(test[feature]):\n",
        "        if label not in encoder.classes_:\n",
        "            encoder.classes_ = np.append(encoder.classes_, label)\n",
        "    test[feature] = encoder.transform(test[feature])\n",
        "\n",
        "# 결측치 처리\n",
        "train.fillna(train.mean(), inplace=True)\n",
        "test.fillna(train.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "label = 'CI_HOUR'\n",
        "eval_metric = 'mean_absolute_error'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231018_173455\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (234646 samples, 46.93 MB).\n",
            "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231018_173455\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.6\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.19041\n",
            "Disk Space Avail:   23.33 GB / 498.62 GB (4.7%)\n",
            "Train Data Rows:    234646\n",
            "Train Data Columns: 27\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.002777778, 103.45012, 210.79577)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    2288.33 MB\n",
            "\tTrain Data (Original)  Memory Usage: 45.05 MB (2.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 16 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 16 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.9s = Fit runtime\n",
            "\t27 features in original data used to generate 27 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 45.05 MB (2.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.02s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "\tWarning: Potentially not enough memory to safely train model. Estimated to require 0.091 GB out of 2.289 GB available memory (19.931%)... (20.000% of avail memory is the max safe size)\n",
            "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=0.32 to avoid the warning)\n",
            "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
            "\t-104.4646\t = Validation score   (-mean_absolute_error)\n",
            "\t0.21s\t = Training   runtime\n",
            "\t1040.5s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "\t-104.3617\t = Validation score   (-mean_absolute_error)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t904.16s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "\t-90.4036\t = Validation score   (-mean_absolute_error)\n",
            "\t114.09s\t = Training   runtime\n",
            "\t7.08s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "\t-89.9545\t = Validation score   (-mean_absolute_error)\n",
            "\t31.42s\t = Training   runtime\n",
            "\t6.33s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-89.4795\t = Validation score   (-mean_absolute_error)\n",
            "\t1.41s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "\t-90.5419\t = Validation score   (-mean_absolute_error)\n",
            "\t167.52s\t = Training   runtime\n",
            "\t7.44s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "\t-89.555\t = Validation score   (-mean_absolute_error)\n",
            "\t38.99s\t = Training   runtime\n",
            "\t6.38s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "\tWarning: Exception caused LightGBMLarge_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\tThe model requires minimum cpu 1, but you only specified 0\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 571, in _fit_folds\n",
            "    fold_fitting_strategy: FoldFittingStrategy = fold_fitting_strategy_cls(**fold_fitting_strategy_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 476, in __init__\n",
            "    self.resources, self.batches, self.num_parallel_jobs = self._get_resource_suggestions(\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 684, in _get_resource_suggestions\n",
            "    resources_info = resources_calculator.get_resources_per_job(**get_resources_per_job_args)\n",
            "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\ray\\resources_calculator.py\", line 56, in get_resources_per_job\n",
            "    assert cpu_per_job >= minimum_cpu_per_job, f\"The model requires minimum cpu {minimum_cpu_per_job}, but you only specified {cpu_per_job}\"\n",
            "AssertionError: The model requires minimum cpu 1, but you only specified 0\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-89.5105\t = Validation score   (-mean_absolute_error)\n",
            "\t0.97s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 2332.87s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231018_173455\\\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_test</th>\n",
              "      <th>score_val</th>\n",
              "      <th>pred_time_test</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_test_marginal</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNeighborsDist_BAG_L1</td>\n",
              "      <td>-0.003345</td>\n",
              "      <td>-104.361655</td>\n",
              "      <td>900.693840</td>\n",
              "      <td>904.163923</td>\n",
              "      <td>0.099090</td>\n",
              "      <td>900.693840</td>\n",
              "      <td>904.163923</td>\n",
              "      <td>0.099090</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RandomForestMSE_BAG_L1</td>\n",
              "      <td>-46.410799</td>\n",
              "      <td>-90.403649</td>\n",
              "      <td>1.916742</td>\n",
              "      <td>7.082437</td>\n",
              "      <td>114.085697</td>\n",
              "      <td>1.916742</td>\n",
              "      <td>7.082437</td>\n",
              "      <td>114.085697</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>-46.423226</td>\n",
              "      <td>-89.479478</td>\n",
              "      <td>904.158989</td>\n",
              "      <td>917.576113</td>\n",
              "      <td>147.015628</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>1.406278</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ExtraTreesMSE_BAG_L2</td>\n",
              "      <td>-50.161845</td>\n",
              "      <td>-89.555028</td>\n",
              "      <td>1807.495845</td>\n",
              "      <td>1964.450499</td>\n",
              "      <td>184.816986</td>\n",
              "      <td>1.406278</td>\n",
              "      <td>6.381801</td>\n",
              "      <td>38.993442</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WeightedEnsemble_L3</td>\n",
              "      <td>-51.891205</td>\n",
              "      <td>-89.510544</td>\n",
              "      <td>1809.282470</td>\n",
              "      <td>1971.891262</td>\n",
              "      <td>353.305130</td>\n",
              "      <td>0.010009</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>0.966879</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ExtraTreesMSE_BAG_L1</td>\n",
              "      <td>-52.427497</td>\n",
              "      <td>-89.954507</td>\n",
              "      <td>1.542402</td>\n",
              "      <td>6.326750</td>\n",
              "      <td>31.424563</td>\n",
              "      <td>1.542402</td>\n",
              "      <td>6.326750</td>\n",
              "      <td>31.424563</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RandomForestMSE_BAG_L2</td>\n",
              "      <td>-62.415460</td>\n",
              "      <td>-90.541928</td>\n",
              "      <td>1807.866182</td>\n",
              "      <td>1965.506459</td>\n",
              "      <td>313.344810</td>\n",
              "      <td>1.776615</td>\n",
              "      <td>7.437761</td>\n",
              "      <td>167.521265</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>KNeighborsUnif_BAG_L1</td>\n",
              "      <td>-84.415406</td>\n",
              "      <td>-104.464650</td>\n",
              "      <td>901.936583</td>\n",
              "      <td>1040.495588</td>\n",
              "      <td>0.214195</td>\n",
              "      <td>901.936583</td>\n",
              "      <td>1040.495588</td>\n",
              "      <td>0.214195</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    model  score_test   score_val  pred_time_test  \\\n",
              "0   KNeighborsDist_BAG_L1   -0.003345 -104.361655      900.693840   \n",
              "1  RandomForestMSE_BAG_L1  -46.410799  -90.403649        1.916742   \n",
              "2     WeightedEnsemble_L2  -46.423226  -89.479478      904.158989   \n",
              "3    ExtraTreesMSE_BAG_L2  -50.161845  -89.555028     1807.495845   \n",
              "4     WeightedEnsemble_L3  -51.891205  -89.510544     1809.282470   \n",
              "5    ExtraTreesMSE_BAG_L1  -52.427497  -89.954507        1.542402   \n",
              "6  RandomForestMSE_BAG_L2  -62.415460  -90.541928     1807.866182   \n",
              "7   KNeighborsUnif_BAG_L1  -84.415406 -104.464650      901.936583   \n",
              "\n",
              "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
              "0     904.163923    0.099090               900.693840              904.163923   \n",
              "1       7.082437  114.085697                 1.916742                7.082437   \n",
              "2     917.576113  147.015628                 0.006005                0.003003   \n",
              "3    1964.450499  184.816986                 1.406278                6.381801   \n",
              "4    1971.891262  353.305130                 0.010009                0.003002   \n",
              "5       6.326750   31.424563                 1.542402                6.326750   \n",
              "6    1965.506459  313.344810                 1.776615                7.437761   \n",
              "7    1040.495588    0.214195               901.936583             1040.495588   \n",
              "\n",
              "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
              "0           0.099090            1       True          2  \n",
              "1         114.085697            1       True          3  \n",
              "2           1.406278            2       True          5  \n",
              "3          38.993442            2       True          7  \n",
              "4           0.966879            3       True          8  \n",
              "5          31.424563            1       True          4  \n",
              "6         167.521265            2       True          6  \n",
              "7           0.214195            1       True          1  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_data = TabularDataset(train)\n",
        "test_data = TabularDataset(test)\n",
        "\n",
        "#predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='medium_quality',  ag_args_fit={'num_gpus': 0}, included_model_types = ['RF','GBM','XGB'])\n",
        "predictor = TabularPredictor(label='CI_HOUR',  eval_metric='mean_absolute_error').fit(train, presets='best_quality',  ag_args_fit={'num_gpus': 0})\n",
        "\n",
        "y_pred_best = predictor.predict(test_data)\n",
        "y_pred_best = pd.DataFrame(y_pred_best, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = y_pred_best['CI_HOUR']\n",
        "predictor.leaderboard(train, silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\tempfile.py:77\u001b[0m, in \u001b[0;36m_exists\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     _os\u001b[39m.\u001b[39;49mlstat(fn)\n\u001b[0;32m     78\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다: '\\\\\\\\.\\\\pipe\\\\pyc-17304-2462-z0c_1vag'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\USER\\Desktop\\새 폴더 (8)\\NEW_AG_1.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/NEW_AG_1.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictor\u001b[39m.\u001b[39mfeature_importance(train, silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:2425\u001b[0m, in \u001b[0;36mTabularPredictor.feature_importance\u001b[1;34m(self, data, model, features, feature_stage, subsample_size, time_limit, num_shuffle_sets, include_confidence_band, confidence_level, silent)\u001b[0m\n\u001b[0;32m   2422\u001b[0m \u001b[39mif\u001b[39;00m num_shuffle_sets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2423\u001b[0m     num_shuffle_sets \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39mif\u001b[39;00m time_limit \u001b[39melse\u001b[39;00m \u001b[39m5\u001b[39m\n\u001b[1;32m-> 2425\u001b[0m fi_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mget_feature_importance(\n\u001b[0;32m   2426\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   2427\u001b[0m     X\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m   2428\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2429\u001b[0m     feature_stage\u001b[39m=\u001b[39;49mfeature_stage,\n\u001b[0;32m   2430\u001b[0m     subsample_size\u001b[39m=\u001b[39;49msubsample_size,\n\u001b[0;32m   2431\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m   2432\u001b[0m     num_shuffle_sets\u001b[39m=\u001b[39;49mnum_shuffle_sets,\n\u001b[0;32m   2433\u001b[0m     silent\u001b[39m=\u001b[39;49msilent,\n\u001b[0;32m   2434\u001b[0m )\n\u001b[0;32m   2436\u001b[0m \u001b[39mif\u001b[39;00m include_confidence_band:\n\u001b[0;32m   2437\u001b[0m     \u001b[39mif\u001b[39;00m confidence_level \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39mor\u001b[39;00m confidence_level \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:870\u001b[0m, in \u001b[0;36mAbstractTabularLearner.get_feature_importance\u001b[1;34m(self, model, X, y, features, feature_stage, subsample_size, silent, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m         X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39munused_features)\n\u001b[0;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m feature_stage \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moriginal\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 870\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39;49m_get_feature_importance_raw(\n\u001b[0;32m    871\u001b[0m             model\u001b[39m=\u001b[39;49mmodel, X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, features\u001b[39m=\u001b[39;49mfeatures, subsample_size\u001b[39m=\u001b[39;49msubsample_size, transform_func\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_features, silent\u001b[39m=\u001b[39;49msilent, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    873\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_features(X)\n\u001b[0;32m    874\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2574\u001b[0m, in \u001b[0;36mAbstractTrainer._get_feature_importance_raw\u001b[1;34m(self, X, y, model, eval_metric, **kwargs)\u001b[0m\n\u001b[0;32m   2572\u001b[0m model: AbstractModel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_model(model)\n\u001b[0;32m   2573\u001b[0m predict_func_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m-> 2574\u001b[0m \u001b[39mreturn\u001b[39;00m compute_permutation_feature_importance(\n\u001b[0;32m   2575\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   2576\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   2577\u001b[0m     predict_func\u001b[39m=\u001b[39;49mpredict_func,\n\u001b[0;32m   2578\u001b[0m     predict_func_kwargs\u001b[39m=\u001b[39;49mpredict_func_kwargs,\n\u001b[0;32m   2579\u001b[0m     eval_metric\u001b[39m=\u001b[39;49meval_metric,\n\u001b[0;32m   2580\u001b[0m     quantile_levels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantile_levels,\n\u001b[0;32m   2581\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2582\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\utils\\utils.py:867\u001b[0m, in \u001b[0;36mcompute_permutation_feature_importance\u001b[1;34m(X, y, predict_func, eval_metric, features, subsample_size, num_shuffle_sets, predict_func_kwargs, transform_func, transform_func_kwargs, time_limit, silent, log_prefix, importance_as_list, random_state, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     X_raw_transformed \u001b[39m=\u001b[39m X_raw \u001b[39mif\u001b[39;00m transform_func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m transform_func(X_raw, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtransform_func_kwargs)\n\u001b[1;32m--> 867\u001b[0m y_pred \u001b[39m=\u001b[39m predict_func(X_raw_transformed, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpredict_func_kwargs)\n\u001b[0;32m    869\u001b[0m row_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    870\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m parallel_computed_features:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:749\u001b[0m, in \u001b[0;36mAbstractTrainer.predict\u001b[1;34m(self, X, model)\u001b[0m\n\u001b[0;32m    747\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_best()\n\u001b[0;32m    748\u001b[0m cascade \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(model, \u001b[39mlist\u001b[39m)\n\u001b[1;32m--> 749\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_model(X, model, cascade\u001b[39m=\u001b[39;49mcascade)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2388\u001b[0m, in \u001b[0;36mAbstractTrainer._predict_model\u001b[1;34m(self, X, model, model_pred_proba_dict, cascade)\u001b[0m\n\u001b[0;32m   2387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict_model\u001b[39m(\u001b[39mself\u001b[39m, X, model, model_pred_proba_dict\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cascade\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m-> 2388\u001b[0m     y_pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_proba_model(X\u001b[39m=\u001b[39;49mX, model\u001b[39m=\u001b[39;49mmodel, model_pred_proba_dict\u001b[39m=\u001b[39;49mmodel_pred_proba_dict, cascade\u001b[39m=\u001b[39;49mcascade)\n\u001b[0;32m   2389\u001b[0m     \u001b[39mreturn\u001b[39;00m get_pred_from_proba(y_pred_proba\u001b[39m=\u001b[39my_pred_proba, problem_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_type)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2392\u001b[0m, in \u001b[0;36mAbstractTrainer._predict_proba_model\u001b[1;34m(self, X, model, model_pred_proba_dict, cascade)\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict_proba_model\u001b[39m(\u001b[39mself\u001b[39m, X, model, model_pred_proba_dict\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cascade\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m-> 2392\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_pred_proba_from_model(model\u001b[39m=\u001b[39;49mmodel, X\u001b[39m=\u001b[39;49mX, model_pred_proba_dict\u001b[39m=\u001b[39;49mmodel_pred_proba_dict, cascade\u001b[39m=\u001b[39;49mcascade)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:769\u001b[0m, in \u001b[0;36mAbstractTrainer.get_pred_proba_from_model\u001b[1;34m(self, model, X, model_pred_proba_dict, cascade)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    768\u001b[0m     models \u001b[39m=\u001b[39m [model]\n\u001b[1;32m--> 769\u001b[0m model_pred_proba_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_model_pred_proba_dict(X\u001b[39m=\u001b[39;49mX, models\u001b[39m=\u001b[39;49mmodels, model_pred_proba_dict\u001b[39m=\u001b[39;49mmodel_pred_proba_dict, cascade\u001b[39m=\u001b[39;49mcascade)\n\u001b[0;32m    770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    771\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mname\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1018\u001b[0m, in \u001b[0;36mAbstractTrainer.get_model_pred_proba_dict\u001b[1;34m(self, X, models, model_pred_proba_dict, model_pred_time_dict, record_pred_time, use_val_cache, cascade, cascade_threshold)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1017\u001b[0m         preprocess_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(infer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, model_pred_proba_dict\u001b[39m=\u001b[39mmodel_pred_proba_dict)\n\u001b[1;32m-> 1018\u001b[0m     model_pred_proba_dict[model_name] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpreprocess_kwargs)\n\u001b[0;32m   1019\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1020\u001b[0m     model_pred_proba_dict[model_name] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(X)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:346\u001b[0m, in \u001b[0;36mBaggedEnsembleModel.predict_proba\u001b[1;34m(self, X, normalize, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_child(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels[\u001b[39m0\u001b[39m])\n\u001b[0;32m    345\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(X, model\u001b[39m=\u001b[39mmodel, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 346\u001b[0m pred_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_proba(X\u001b[39m=\u001b[39;49mX, preprocess_nonadaptive\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, normalize\u001b[39m=\u001b[39;49mnormalize)\n\u001b[0;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m    348\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_child(model)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:931\u001b[0m, in \u001b[0;36mAbstractModel.predict_proba\u001b[1;34m(self, X, normalize, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[39mif\u001b[39;00m normalize \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     normalize \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_pred_probas\n\u001b[1;32m--> 931\u001b[0m y_pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_proba(X\u001b[39m=\u001b[39;49mX, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    932\u001b[0m \u001b[39mif\u001b[39;00m normalize:\n\u001b[0;32m    933\u001b[0m     y_pred_proba \u001b[39m=\u001b[39m normalize_pred_probas(y_pred_proba, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_type)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:946\u001b[0m, in \u001b[0;36mAbstractModel._predict_proba\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    945\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_type \u001b[39min\u001b[39;00m [REGRESSION, QUANTILE]:\n\u001b[1;32m--> 946\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    947\u001b[0m     \u001b[39mreturn\u001b[39;00m y_pred\n\u001b[0;32m    949\u001b[0m y_pred_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict_proba(X)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neighbors\\_regression.py:232\u001b[0m, in \u001b[0;36mKNeighborsRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    230\u001b[0m     neigh_dist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     neigh_dist, neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X)\n\u001b[0;32m    234\u001b[0m weights \u001b[39m=\u001b[39m _get_weights(neigh_dist, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n\u001b[0;32m    236\u001b[0m _y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\neighbors\\_base.py:796\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         kwds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_params_\n\u001b[1;32m--> 796\u001b[0m     chunked_results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m    797\u001b[0m         pairwise_distances_chunked(\n\u001b[0;32m    798\u001b[0m             X,\n\u001b[0;32m    799\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[0;32m    800\u001b[0m             reduce_func\u001b[39m=\u001b[39;49mreduce_func,\n\u001b[0;32m    801\u001b[0m             metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[0;32m    802\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    803\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[0;32m    804\u001b[0m         )\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    807\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mball_tree\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkd_tree\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    808\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X):\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1850\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1848\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1849\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[1;32m-> 1850\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m   1851\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[0;32m   1852\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   1854\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[0;32m   1855\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[0;32m   1856\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2022\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2019\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m   2020\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m-> 2022\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1568\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1566\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n\u001b[0;32m   1567\u001b[0m ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1568\u001b[0m Parallel(backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreading\u001b[39;49m\u001b[39m\"\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49mn_jobs)(\n\u001b[0;32m   1569\u001b[0m     fd(func, ret, s, X, Y[s], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m gen_even_slices(_num_samples(Y), effective_n_jobs(n_jobs))\n\u001b[0;32m   1571\u001b[0m )\n\u001b[0;32m   1573\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   1574\u001b[0m     \u001b[39m# zeroing diagonal for euclidean norm.\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m     \u001b[39m# TODO: do it also for other norms.\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m     np\u001b[39m.\u001b[39mfill_diagonal(ret, \u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\_parallel_backends.py:252\u001b[0m, in \u001b[0;36mPoolManagerMixin.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    251\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_pool()\u001b[39m.\u001b[39mapply_async(\n\u001b[0;32m    253\u001b[0m         SafeFunction(func), callback\u001b[39m=\u001b[39mcallback)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\joblib\\_parallel_backends.py:407\u001b[0m, in \u001b[0;36mThreadingBackend._get_pool\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39m\"\"\"Lazily initialize the thread pool\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \n\u001b[0;32m    403\u001b[0m \u001b[39mThe actual pool of worker threads is only initialized at the first\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39mcall to apply_async.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m ThreadPool(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_jobs)\n\u001b[0;32m    408\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\pool.py:925\u001b[0m, in \u001b[0;36mThreadPool.__init__\u001b[1;34m(self, processes, initializer, initargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, processes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, initializer\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, initargs\u001b[39m=\u001b[39m()):\n\u001b[1;32m--> 925\u001b[0m     Pool\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, processes, initializer, initargs)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\pool.py:196\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_taskqueue \u001b[39m=\u001b[39m queue\u001b[39m.\u001b[39mSimpleQueue()\n\u001b[0;32m    193\u001b[0m \u001b[39m# The _change_notifier queue exist to wake up self._handle_workers()\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m# when the cache (self._cache) is empty or when there is a change in\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# the _state variable of the thread that runs _handle_workers.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_change_notifier \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ctx\u001b[39m.\u001b[39;49mSimpleQueue()\n\u001b[0;32m    197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache \u001b[39m=\u001b[39m _PoolCache(notifier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_change_notifier)\n\u001b[0;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maxtasksperchild \u001b[39m=\u001b[39m maxtasksperchild\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\context.py:113\u001b[0m, in \u001b[0;36mBaseContext.SimpleQueue\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m'''Returns a queue object'''\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mqueues\u001b[39;00m \u001b[39mimport\u001b[39;00m SimpleQueue\n\u001b[1;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m SimpleQueue(ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_context())\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\queues.py:335\u001b[0m, in \u001b[0;36mSimpleQueue.__init__\u001b[1;34m(self, ctx)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m, ctx):\n\u001b[1;32m--> 335\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mPipe(duplex\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    336\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mLock()\n\u001b[0;32m    337\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mpoll\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py:539\u001b[0m, in \u001b[0;36mPipe\u001b[1;34m(duplex)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mPipe\u001b[39m(duplex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    536\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[39m    Returns pair of connection objects at either end of a pipe\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m     address \u001b[39m=\u001b[39m arbitrary_address(\u001b[39m'\u001b[39;49m\u001b[39mAF_PIPE\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    540\u001b[0m     \u001b[39mif\u001b[39;00m duplex:\n\u001b[0;32m    541\u001b[0m         openmode \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39mPIPE_ACCESS_DUPLEX\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\multiprocessing\\connection.py:78\u001b[0m, in \u001b[0;36marbitrary_address\u001b[1;34m(family)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m tempfile\u001b[39m.\u001b[39mmktemp(prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlistener-\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39mutil\u001b[39m.\u001b[39mget_temp_dir())\n\u001b[0;32m     77\u001b[0m \u001b[39melif\u001b[39;00m family \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAF_PIPE\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m tempfile\u001b[39m.\u001b[39;49mmktemp(prefix\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mpipe\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mpyc-\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m-\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m\n\u001b[0;32m     79\u001b[0m                            (os\u001b[39m.\u001b[39;49mgetpid(), \u001b[39mnext\u001b[39;49m(_mmap_counter)), \u001b[39mdir\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39munrecognized family\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\tempfile.py:399\u001b[0m, in \u001b[0;36mmktemp\u001b[1;34m(suffix, prefix, dir)\u001b[0m\n\u001b[0;32m    397\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(names)\n\u001b[0;32m    398\u001b[0m     file \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m suffix)\n\u001b[1;32m--> 399\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _exists(file):\n\u001b[0;32m    400\u001b[0m         \u001b[39mreturn\u001b[39;00m file\n\u001b[0;32m    402\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m(_errno\u001b[39m.\u001b[39mEEXIST,\n\u001b[0;32m    403\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mNo usable temporary filename found\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\tempfile.py:77\u001b[0m, in \u001b[0;36m_exists\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_exists\u001b[39m(fn):\n\u001b[0;32m     76\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m         _os\u001b[39m.\u001b[39;49mlstat(fn)\n\u001b[0;32m     78\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "predictor.feature_importance(train, silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 220491 entries, 0 to 220490\n",
            "Data columns (total 27 columns):\n",
            " #   Column              Non-Null Count   Dtype  \n",
            "---  ------              --------------   -----  \n",
            " 0   ARI_CO              220491 non-null  int32  \n",
            " 1   ARI_PO              220491 non-null  int32  \n",
            " 2   SHIP_TYPE_CATEGORY  220491 non-null  int32  \n",
            " 3   DIST                220491 non-null  float64\n",
            " 4   ID                  220491 non-null  int32  \n",
            " 5   BREADTH             220491 non-null  float64\n",
            " 6   BUILT               220491 non-null  int64  \n",
            " 7   DEADWEIGHT          220491 non-null  int64  \n",
            " 8   DEPTH               220491 non-null  float64\n",
            " 9   DRAUGHT             220491 non-null  float64\n",
            " 10  GT                  220491 non-null  int64  \n",
            " 11  LENGTH              220491 non-null  float64\n",
            " 12  SHIPMANAGER         220491 non-null  int32  \n",
            " 13  FLAG                220491 non-null  int32  \n",
            " 14  U_WIND              220491 non-null  float64\n",
            " 15  V_WIND              220491 non-null  float64\n",
            " 16  AIR_TEMPERATURE     220491 non-null  float64\n",
            " 17  BN                  220491 non-null  float64\n",
            " 18  ATA_LT              220491 non-null  int64  \n",
            " 19  PORT_SIZE           220491 non-null  float64\n",
            " 20  종가                  220491 non-null  float64\n",
            " 21  year                220491 non-null  int64  \n",
            " 22  month               220491 non-null  int64  \n",
            " 23  day                 220491 non-null  int64  \n",
            " 24  hour                220491 non-null  int64  \n",
            " 25  minute              220491 non-null  int64  \n",
            " 26  weekday             220491 non-null  int64  \n",
            "dtypes: float64(11), int32(6), int64(10)\n",
            "memory usage: 42.1 MB\n"
          ]
        }
      ],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_145255\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_145255\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   192.00 GB / 511.09 GB (37.6%)\n",
            "Train Data Rows:    28005\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2151.602778, 0.014166667, 93.25425, 194.93553)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    15792.04 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.48 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t24 features in original data used to generate 24 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 4.37 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-95.2649\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.61s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-95.9026\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.54s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-79.0133\t = Validation score   (-mean_absolute_error)\n",
            "\t3.45s\t = Training   runtime\n",
            "\t0.96s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-77.233\t = Validation score   (-mean_absolute_error)\n",
            "\t3.85s\t = Training   runtime\n",
            "\t0.47s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-77.7992\t = Validation score   (-mean_absolute_error)\n",
            "\t20.73s\t = Training   runtime\n",
            "\t1.36s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-77.8432\t = Validation score   (-mean_absolute_error)\n",
            "\t46.18s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-77.8952\t = Validation score   (-mean_absolute_error)\n",
            "\t4.53s\t = Training   runtime\n",
            "\t1.19s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-82.9011\t = Validation score   (-mean_absolute_error)\n",
            "\t43.7s\t = Training   runtime\n",
            "\t0.49s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=45892, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=45892, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-19 23:55:25,712\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-19 23:55:25,717\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-19 23:55:25,719\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-19 23:55:25,727\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-19 23:55:25,730\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-19 23:55:25,736\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-19 23:55:25,740\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-69.6922\t = Validation score   (-mean_absolute_error)\n",
            "\t115.7s\t = Training   runtime\n",
            "\t0.23s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-76.1108\t = Validation score   (-mean_absolute_error)\n",
            "\t4.8s\t = Training   runtime\n",
            "\t0.83s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-68.4257\t = Validation score   (-mean_absolute_error)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-73.3107\t = Validation score   (-mean_absolute_error)\n",
            "\t3.81s\t = Training   runtime\n",
            "\t0.24s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-73.3455\t = Validation score   (-mean_absolute_error)\n",
            "\t3.79s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-78.5546\t = Validation score   (-mean_absolute_error)\n",
            "\t62.57s\t = Training   runtime\n",
            "\t1.53s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-73.3027\t = Validation score   (-mean_absolute_error)\n",
            "\t15.87s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-78.4442\t = Validation score   (-mean_absolute_error)\n",
            "\t7.95s\t = Training   runtime\n",
            "\t1.35s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-72.1192\t = Validation score   (-mean_absolute_error)\n",
            "\t42.26s\t = Training   runtime\n",
            "\t0.52s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=6864, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=6864, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 00:00:08,647\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:00:08,647\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:00:08,655\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:00:08,655\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:00:08,663\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:00:08,663\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:00:08,663\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-64.2201\t = Validation score   (-mean_absolute_error)\n",
            "\t48.72s\t = Training   runtime\n",
            "\t0.3s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-74.4969\t = Validation score   (-mean_absolute_error)\n",
            "\t5.1s\t = Training   runtime\n",
            "\t0.34s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-64.1995\t = Validation score   (-mean_absolute_error)\n",
            "\t0.49s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 492.34s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_145255\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2018\n",
            "2018\n",
            "2018\n",
            "2018\n",
            "2018\n",
            "28005\n",
            "28005\n",
            "28005\n",
            "28005\n",
            "28005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_150303\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_150303\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   185.80 GB / 511.09 GB (36.4%)\n",
            "Train Data Rows:    26852\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2152.193889, 0.016388889, 104.38929, 213.1725)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    17208.6 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.3 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t24 features in original data used to generate 24 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 4.19 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.16s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-108.331\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-109.0112\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.44s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-93.4316\t = Validation score   (-mean_absolute_error)\n",
            "\t5.77s\t = Training   runtime\n",
            "\t1.07s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-91.366\t = Validation score   (-mean_absolute_error)\n",
            "\t4.85s\t = Training   runtime\n",
            "\t0.66s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-95.6465\t = Validation score   (-mean_absolute_error)\n",
            "\t27.28s\t = Training   runtime\n",
            "\t1.33s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-91.7887\t = Validation score   (-mean_absolute_error)\n",
            "\t79.9s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-95.3415\t = Validation score   (-mean_absolute_error)\n",
            "\t4.55s\t = Training   runtime\n",
            "\t1.21s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-90.8443\t = Validation score   (-mean_absolute_error)\n",
            "\t41.0s\t = Training   runtime\n",
            "\t0.5s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=49052, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=49052, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 00:06:08,799\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:06:08,808\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:06:08,810\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:06:08,817\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:06:08,822\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:06:08,826\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:06:08,826\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-80.105\t = Validation score   (-mean_absolute_error)\n",
            "\t98.07s\t = Training   runtime\n",
            "\t0.19s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-90.8051\t = Validation score   (-mean_absolute_error)\n",
            "\t10.93s\t = Training   runtime\n",
            "\t1.41s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-79.1824\t = Validation score   (-mean_absolute_error)\n",
            "\t0.53s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-87.6032\t = Validation score   (-mean_absolute_error)\n",
            "\t3.97s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-87.3601\t = Validation score   (-mean_absolute_error)\n",
            "\t3.81s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-93.8008\t = Validation score   (-mean_absolute_error)\n",
            "\t60.36s\t = Training   runtime\n",
            "\t1.48s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-86.6692\t = Validation score   (-mean_absolute_error)\n",
            "\t15.93s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-93.2121\t = Validation score   (-mean_absolute_error)\n",
            "\t7.15s\t = Training   runtime\n",
            "\t1.33s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-84.23\t = Validation score   (-mean_absolute_error)\n",
            "\t40.66s\t = Training   runtime\n",
            "\t0.49s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=31716, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=31716, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 00:10:36,090\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:10:36,090\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:10:36,097\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:10:36,097\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:10:36,123\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:10:36,126\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:10:36,126\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-76.2315\t = Validation score   (-mean_absolute_error)\n",
            "\t41.76s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-88.3789\t = Validation score   (-mean_absolute_error)\n",
            "\t6.43s\t = Training   runtime\n",
            "\t0.34s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-76.1392\t = Validation score   (-mean_absolute_error)\n",
            "\t0.48s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 506.55s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_150303\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020\n",
            "2020\n",
            "2020\n",
            "2020\n",
            "2020\n",
            "26852\n",
            "26852\n",
            "26852\n",
            "26852\n",
            "26852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_151332\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_151332\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   183.36 GB / 511.09 GB (35.9%)\n",
            "Train Data Rows:    38810\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2157.861389, 0.100833333, 106.47821, 211.1771)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    16577.27 MB\n",
            "\tTrain Data (Original)  Memory Usage: 6.21 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t24 features in original data used to generate 24 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 6.05 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-106.6974\t = Validation score   (-mean_absolute_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.91s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-107.4822\t = Validation score   (-mean_absolute_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.9s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-92.393\t = Validation score   (-mean_absolute_error)\n",
            "\t9.53s\t = Training   runtime\n",
            "\t2.28s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-90.6193\t = Validation score   (-mean_absolute_error)\n",
            "\t6.55s\t = Training   runtime\n",
            "\t1.24s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-95.187\t = Validation score   (-mean_absolute_error)\n",
            "\t37.01s\t = Training   runtime\n",
            "\t1.97s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-91.0489\t = Validation score   (-mean_absolute_error)\n",
            "\t75.9s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-95.5624\t = Validation score   (-mean_absolute_error)\n",
            "\t6.14s\t = Training   runtime\n",
            "\t1.75s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-91.2492\t = Validation score   (-mean_absolute_error)\n",
            "\t56.07s\t = Training   runtime\n",
            "\t0.61s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=46556, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=46556, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 00:17:07,745\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:17:07,749\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:17:07,752\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:17:07,758\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:17:07,764\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:17:07,766\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:17:07,771\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-80.4151\t = Validation score   (-mean_absolute_error)\n",
            "\t99.26s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-90.1905\t = Validation score   (-mean_absolute_error)\n",
            "\t11.64s\t = Training   runtime\n",
            "\t2.17s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-79.2907\t = Validation score   (-mean_absolute_error)\n",
            "\t0.73s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-86.8729\t = Validation score   (-mean_absolute_error)\n",
            "\t5.01s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-86.7491\t = Validation score   (-mean_absolute_error)\n",
            "\t3.92s\t = Training   runtime\n",
            "\t0.23s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-93.5183\t = Validation score   (-mean_absolute_error)\n",
            "\t85.99s\t = Training   runtime\n",
            "\t2.18s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-86.5729\t = Validation score   (-mean_absolute_error)\n",
            "\t20.13s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-93.0649\t = Validation score   (-mean_absolute_error)\n",
            "\t9.56s\t = Training   runtime\n",
            "\t1.89s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-83.4387\t = Validation score   (-mean_absolute_error)\n",
            "\t57.37s\t = Training   runtime\n",
            "\t0.65s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=47448, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=47448, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 00:22:28,878\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:22:28,878\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:22:28,885\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:22:28,885\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:22:28,893\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:22:28,902\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:22:28,902\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-75.5668\t = Validation score   (-mean_absolute_error)\n",
            "\t57.55s\t = Training   runtime\n",
            "\t0.36s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-87.6405\t = Validation score   (-mean_absolute_error)\n",
            "\t5.65s\t = Training   runtime\n",
            "\t0.38s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-75.5343\t = Validation score   (-mean_absolute_error)\n",
            "\t0.66s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 605.64s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_151332\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021\n",
            "2021\n",
            "2021\n",
            "2021\n",
            "2021\n",
            "38810\n",
            "38810\n",
            "38810\n",
            "38810\n",
            "38810\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_152555\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_152555\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   179.81 GB / 511.09 GB (35.2%)\n",
            "Train Data Rows:    21761\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2153.450556, 0.002777778, 101.69171, 213.44924)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    16193.77 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.48 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 5): ['U_WIND', 'V_WIND', 'AIR_TEMPERATURE', 'BN', 'year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) :  7 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) :  7 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t20 features in original data used to generate 20 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.7 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-101.6366\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-100.4522\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-89.3809\t = Validation score   (-mean_absolute_error)\n",
            "\t5.11s\t = Training   runtime\n",
            "\t0.86s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-86.9086\t = Validation score   (-mean_absolute_error)\n",
            "\t3.82s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-89.8424\t = Validation score   (-mean_absolute_error)\n",
            "\t14.77s\t = Training   runtime\n",
            "\t1.13s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-87.1682\t = Validation score   (-mean_absolute_error)\n",
            "\t52.83s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-90.108\t = Validation score   (-mean_absolute_error)\n",
            "\t3.55s\t = Training   runtime\n",
            "\t1.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-89.5167\t = Validation score   (-mean_absolute_error)\n",
            "\t33.43s\t = Training   runtime\n",
            "\t0.37s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=21508, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=21508, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 00:28:08,165\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 00:28:08,181\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:28:08,181\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:28:08,187\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:28:08,187\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:28:08,197\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:28:08,199\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-76.1146\t = Validation score   (-mean_absolute_error)\n",
            "\t72.11s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-85.5578\t = Validation score   (-mean_absolute_error)\n",
            "\t5.26s\t = Training   runtime\n",
            "\t0.64s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-75.2063\t = Validation score   (-mean_absolute_error)\n",
            "\t0.53s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-83.4339\t = Validation score   (-mean_absolute_error)\n",
            "\t3.57s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-83.4861\t = Validation score   (-mean_absolute_error)\n",
            "\t3.31s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-90.1053\t = Validation score   (-mean_absolute_error)\n",
            "\t39.58s\t = Training   runtime\n",
            "\t1.21s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-82.8762\t = Validation score   (-mean_absolute_error)\n",
            "\t13.22s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-89.7087\t = Validation score   (-mean_absolute_error)\n",
            "\t5.37s\t = Training   runtime\n",
            "\t1.13s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-81.8943\t = Validation score   (-mean_absolute_error)\n",
            "\t34.15s\t = Training   runtime\n",
            "\t0.41s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=3104, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=3104, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 00:31:30,197\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:31:30,200\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:31:30,205\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:31:30,205\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:31:30,213\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:31:30,216\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:31:30,221\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-71.8005\t = Validation score   (-mean_absolute_error)\n",
            "\t39.98s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-85.1468\t = Validation score   (-mean_absolute_error)\n",
            "\t5.5s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-71.7496\t = Validation score   (-mean_absolute_error)\n",
            "\t0.46s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 385.96s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_152555\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2016\n",
            "2016\n",
            "2016\n",
            "2016\n",
            "2016\n",
            "21761\n",
            "21761\n",
            "21761\n",
            "21761\n",
            "21761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_153413\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_153413\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   177.59 GB / 511.09 GB (34.7%)\n",
            "Train Data Rows:    25354\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2140.12, 0.023611111, 94.57865, 195.93535)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    16398.07 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.06 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 5): ['U_WIND', 'V_WIND', 'AIR_TEMPERATURE', 'BN', 'year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) :  7 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) :  7 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t20 features in original data used to generate 20 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.14 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-92.8873\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.38s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-93.1218\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.37s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-79.815\t = Validation score   (-mean_absolute_error)\n",
            "\t4.72s\t = Training   runtime\n",
            "\t0.71s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-77.6223\t = Validation score   (-mean_absolute_error)\n",
            "\t3.67s\t = Training   runtime\n",
            "\t0.35s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-79.9584\t = Validation score   (-mean_absolute_error)\n",
            "\t16.99s\t = Training   runtime\n",
            "\t1.25s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-78.6025\t = Validation score   (-mean_absolute_error)\n",
            "\t34.56s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-79.4831\t = Validation score   (-mean_absolute_error)\n",
            "\t3.78s\t = Training   runtime\n",
            "\t1.18s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-80.4119\t = Validation score   (-mean_absolute_error)\n",
            "\t39.41s\t = Training   runtime\n",
            "\t0.48s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=49044, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=49044, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 00:36:18,412\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:36:18,420\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:36:18,429\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:36:18,432\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:36:18,438\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:36:18,444\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:36:18,447\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-67.9438\t = Validation score   (-mean_absolute_error)\n",
            "\t79.58s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-76.5533\t = Validation score   (-mean_absolute_error)\n",
            "\t4.02s\t = Training   runtime\n",
            "\t0.48s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-67.2603\t = Validation score   (-mean_absolute_error)\n",
            "\t0.65s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-74.3188\t = Validation score   (-mean_absolute_error)\n",
            "\t3.32s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-74.8793\t = Validation score   (-mean_absolute_error)\n",
            "\t3.29s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-80.4882\t = Validation score   (-mean_absolute_error)\n",
            "\t46.8s\t = Training   runtime\n",
            "\t1.42s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-74.1608\t = Validation score   (-mean_absolute_error)\n",
            "\t12.15s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-79.9373\t = Validation score   (-mean_absolute_error)\n",
            "\t6.67s\t = Training   runtime\n",
            "\t1.28s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-72.9407\t = Validation score   (-mean_absolute_error)\n",
            "\t39.11s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=48000, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=48000, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 00:39:57,603\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:39:57,612\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:39:57,613\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:39:57,619\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:39:57,619\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:39:57,627\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:39:57,627\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-64.023\t = Validation score   (-mean_absolute_error)\n",
            "\t46.8s\t = Training   runtime\n",
            "\t0.3s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-75.8712\t = Validation score   (-mean_absolute_error)\n",
            "\t4.86s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-63.9596\t = Validation score   (-mean_absolute_error)\n",
            "\t0.49s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 401.06s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_153413\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2017\n",
            "2017\n",
            "2017\n",
            "2017\n",
            "2017\n",
            "25354\n",
            "25354\n",
            "25354\n",
            "25354\n",
            "25354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_154240\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_154240\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   175.20 GB / 511.09 GB (34.3%)\n",
            "Train Data Rows:    14344\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2159.130556, 0.0225, 115.43077, 226.22288)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    16226.52 MB\n",
            "\tTrain Data (Original)  Memory Usage: 2.3 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 5): ['U_WIND', 'V_WIND', 'AIR_TEMPERATURE', 'BN', 'year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) :  7 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) :  7 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t20 features in original data used to generate 20 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.78 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.13s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-119.5704\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-118.9532\t = Validation score   (-mean_absolute_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-104.5407\t = Validation score   (-mean_absolute_error)\n",
            "\t3.31s\t = Training   runtime\n",
            "\t0.33s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-103.3983\t = Validation score   (-mean_absolute_error)\n",
            "\t3.31s\t = Training   runtime\n",
            "\t0.21s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-105.4353\t = Validation score   (-mean_absolute_error)\n",
            "\t9.54s\t = Training   runtime\n",
            "\t0.66s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-103.6875\t = Validation score   (-mean_absolute_error)\n",
            "\t34.7s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-107.1539\t = Validation score   (-mean_absolute_error)\n",
            "\t2.1s\t = Training   runtime\n",
            "\t0.65s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-104.0608\t = Validation score   (-mean_absolute_error)\n",
            "\t22.79s\t = Training   runtime\n",
            "\t0.31s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=31976, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=31976, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 00:44:13,595\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:44:13,595\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:44:13,603\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:44:13,610\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:44:13,618\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:44:13,618\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:44:13,629\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-89.4875\t = Validation score   (-mean_absolute_error)\n",
            "\t50.37s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-102.4102\t = Validation score   (-mean_absolute_error)\n",
            "\t4.04s\t = Training   runtime\n",
            "\t0.32s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-88.5616\t = Validation score   (-mean_absolute_error)\n",
            "\t0.46s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-99.8791\t = Validation score   (-mean_absolute_error)\n",
            "\t3.32s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-100.1748\t = Validation score   (-mean_absolute_error)\n",
            "\t3.32s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-108.3047\t = Validation score   (-mean_absolute_error)\n",
            "\t25.56s\t = Training   runtime\n",
            "\t0.75s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-99.2294\t = Validation score   (-mean_absolute_error)\n",
            "\t12.24s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-107.4876\t = Validation score   (-mean_absolute_error)\n",
            "\t3.65s\t = Training   runtime\n",
            "\t0.68s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-98.5801\t = Validation score   (-mean_absolute_error)\n",
            "\t24.14s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=46036, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=46036, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 00:46:43,769\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:46:43,775\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:46:43,777\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:46:43,777\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:46:43,777\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:46:43,785\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:46:43,785\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-84.9477\t = Validation score   (-mean_absolute_error)\n",
            "\t28.04s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-102.6109\t = Validation score   (-mean_absolute_error)\n",
            "\t5.4s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-84.8057\t = Validation score   (-mean_absolute_error)\n",
            "\t0.44s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 282.12s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_154240\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2015\n",
            "2015\n",
            "2015\n",
            "2015\n",
            "2015\n",
            "14344\n",
            "14344\n",
            "14344\n",
            "14344\n",
            "14344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_154857\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_154857\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   173.73 GB / 511.09 GB (34.0%)\n",
            "Train Data Rows:    25478\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2158.901944, 0.027777778, 93.04907, 203.53228)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    16210.02 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t24 features in original data used to generate 24 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.97 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-98.9476\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.4s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-99.4221\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.39s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-83.9179\t = Validation score   (-mean_absolute_error)\n",
            "\t6.24s\t = Training   runtime\n",
            "\t1.23s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-81.0527\t = Validation score   (-mean_absolute_error)\n",
            "\t4.55s\t = Training   runtime\n",
            "\t0.54s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-83.2937\t = Validation score   (-mean_absolute_error)\n",
            "\t22.11s\t = Training   runtime\n",
            "\t1.34s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-81.6147\t = Validation score   (-mean_absolute_error)\n",
            "\t83.42s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-83.6888\t = Validation score   (-mean_absolute_error)\n",
            "\t4.32s\t = Training   runtime\n",
            "\t1.39s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-84.0291\t = Validation score   (-mean_absolute_error)\n",
            "\t40.91s\t = Training   runtime\n",
            "\t0.52s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=45140, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=45140, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 00:52:02,014\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:52:02,120\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:52:02,126\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:52:02,129\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:52:02,133\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:52:02,138\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:52:02,159\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-73.1716\t = Validation score   (-mean_absolute_error)\n",
            "\t108.84s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-80.091\t = Validation score   (-mean_absolute_error)\n",
            "\t6.96s\t = Training   runtime\n",
            "\t1.24s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-71.4611\t = Validation score   (-mean_absolute_error)\n",
            "\t0.55s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-76.1507\t = Validation score   (-mean_absolute_error)\n",
            "\t4.69s\t = Training   runtime\n",
            "\t0.33s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-75.8777\t = Validation score   (-mean_absolute_error)\n",
            "\t3.72s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-81.9236\t = Validation score   (-mean_absolute_error)\n",
            "\t53.82s\t = Training   runtime\n",
            "\t1.41s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-75.827\t = Validation score   (-mean_absolute_error)\n",
            "\t16.66s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-81.2688\t = Validation score   (-mean_absolute_error)\n",
            "\t6.3s\t = Training   runtime\n",
            "\t1.31s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-73.9029\t = Validation score   (-mean_absolute_error)\n",
            "\t38.53s\t = Training   runtime\n",
            "\t0.41s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=3428, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=3428, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 00:56:26,879\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:56:26,887\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:56:26,887\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:56:26,887\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:56:26,895\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:56:26,895\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 00:56:26,903\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-66.4373\t = Validation score   (-mean_absolute_error)\n",
            "\t47.87s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-77.0165\t = Validation score   (-mean_absolute_error)\n",
            "\t5.38s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-66.3363\t = Validation score   (-mean_absolute_error)\n",
            "\t0.52s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 507.87s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_154857\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2019\n",
            "2019\n",
            "2019\n",
            "2019\n",
            "2019\n",
            "25478\n",
            "25478\n",
            "25478\n",
            "25478\n",
            "25478\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_155923\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_155923\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   171.25 GB / 511.09 GB (33.5%)\n",
            "Train Data Rows:    46210\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2158.4675, 0.380833333, 112.42193, 223.30602)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    15877.47 MB\n",
            "\tTrain Data (Original)  Memory Usage: 7.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.2s = Fit runtime\n",
            "\t24 features in original data used to generate 24 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 7.21 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-114.4044\t = Validation score   (-mean_absolute_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t1.23s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-114.5776\t = Validation score   (-mean_absolute_error)\n",
            "\t0.04s\t = Training   runtime\n",
            "\t1.21s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-100.0071\t = Validation score   (-mean_absolute_error)\n",
            "\t12.87s\t = Training   runtime\n",
            "\t3.6s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-97.8415\t = Validation score   (-mean_absolute_error)\n",
            "\t8.3s\t = Training   runtime\n",
            "\t2.11s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-104.0373\t = Validation score   (-mean_absolute_error)\n",
            "\t46.04s\t = Training   runtime\n",
            "\t2.23s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-98.4077\t = Validation score   (-mean_absolute_error)\n",
            "\t112.65s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-103.3458\t = Validation score   (-mean_absolute_error)\n",
            "\t6.84s\t = Training   runtime\n",
            "\t1.97s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-99.1722\t = Validation score   (-mean_absolute_error)\n",
            "\t64.06s\t = Training   runtime\n",
            "\t0.75s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=39668, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=39668, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 01:04:00,772\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:04:00,780\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 01:04:00,788\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:04:00,796\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:04:00,798\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:04:00,798\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:04:00,805\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-85.9988\t = Validation score   (-mean_absolute_error)\n",
            "\t135.4s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-96.9564\t = Validation score   (-mean_absolute_error)\n",
            "\t21.51s\t = Training   runtime\n",
            "\t3.87s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-84.947\t = Validation score   (-mean_absolute_error)\n",
            "\t0.74s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-93.6037\t = Validation score   (-mean_absolute_error)\n",
            "\t4.29s\t = Training   runtime\n",
            "\t0.45s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-93.4434\t = Validation score   (-mean_absolute_error)\n",
            "\t3.39s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-100.6729\t = Validation score   (-mean_absolute_error)\n",
            "\t88.12s\t = Training   runtime\n",
            "\t2.35s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-93.0351\t = Validation score   (-mean_absolute_error)\n",
            "\t26.09s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-100.0481\t = Validation score   (-mean_absolute_error)\n",
            "\t10.52s\t = Training   runtime\n",
            "\t2.13s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-90.1806\t = Validation score   (-mean_absolute_error)\n",
            "\t59.33s\t = Training   runtime\n",
            "\t0.7s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4380, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4380, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 01:10:16,793\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:10:16,793\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:10:16,809\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:10:16,809\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:10:16,809\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:10:16,809\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:10:16,824\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-81.5608\t = Validation score   (-mean_absolute_error)\n",
            "\t62.22s\t = Training   runtime\n",
            "\t0.39s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-93.9713\t = Validation score   (-mean_absolute_error)\n",
            "\t5.11s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-81.4614\t = Validation score   (-mean_absolute_error)\n",
            "\t0.71s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 725.89s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_155923\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022\n",
            "2022\n",
            "2022\n",
            "2022\n",
            "2022\n",
            "46210\n",
            "46210\n",
            "46210\n",
            "46210\n",
            "46210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_161351\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_161351\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   168.74 GB / 511.09 GB (33.0%)\n",
            "Train Data Rows:    5694\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (1186.935278, 0.635833333, 76.99137, 121.86614)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    16864.8 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.91 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 11 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])       : 12 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['month']\n",
            "\t0.1s = Fit runtime\n",
            "\t24 features in original data used to generate 24 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.87 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-75.4068\t = Validation score   (-mean_absolute_error)\n",
            "\t0.0s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-74.9069\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-63.8786\t = Validation score   (-mean_absolute_error)\n",
            "\t2.59s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-63.0023\t = Validation score   (-mean_absolute_error)\n",
            "\t2.58s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-66.0167\t = Validation score   (-mean_absolute_error)\n",
            "\t4.21s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-63.8435\t = Validation score   (-mean_absolute_error)\n",
            "\t14.08s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-65.3941\t = Validation score   (-mean_absolute_error)\n",
            "\t1.05s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-64.4725\t = Validation score   (-mean_absolute_error)\n",
            "\t10.22s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=23648, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=23648, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 01:14:41,956\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:14:41,963\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:14:41,969\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:14:41,973\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:14:41,977\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:14:41,980\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:14:41,984\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-57.5179\t = Validation score   (-mean_absolute_error)\n",
            "\t17.61s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-63.3954\t = Validation score   (-mean_absolute_error)\n",
            "\t7.36s\t = Training   runtime\n",
            "\t0.19s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-56.6128\t = Validation score   (-mean_absolute_error)\n",
            "\t0.33s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-62.3645\t = Validation score   (-mean_absolute_error)\n",
            "\t2.73s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-63.0519\t = Validation score   (-mean_absolute_error)\n",
            "\t3.07s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-66.7888\t = Validation score   (-mean_absolute_error)\n",
            "\t8.78s\t = Training   runtime\n",
            "\t0.33s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-61.9553\t = Validation score   (-mean_absolute_error)\n",
            "\t8.86s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-65.9918\t = Validation score   (-mean_absolute_error)\n",
            "\t1.44s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-63.0464\t = Validation score   (-mean_absolute_error)\n",
            "\t10.68s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=42104, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=42104, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 01:16:03,903\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:16:03,907\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:16:03,910\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:16:03,914\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:16:03,916\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:16:03,916\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:16:03,916\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-56.0906\t = Validation score   (-mean_absolute_error)\n",
            "\t13.1s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-64.5833\t = Validation score   (-mean_absolute_error)\n",
            "\t5.47s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-55.9118\t = Validation score   (-mean_absolute_error)\n",
            "\t0.27s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 155.45s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_161351\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023\n",
            "2023\n",
            "2023\n",
            "2023\n",
            "2023\n",
            "5694\n",
            "5694\n",
            "5694\n",
            "5694\n",
            "5694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231019_161745\\\"\n",
            "Presets specified: ['best_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels\\ag-20231019_161745\\\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.8.16\n",
            "Operating System:   Windows\n",
            "Platform Machine:   AMD64\n",
            "Platform Version:   10.0.22621\n",
            "Disk Space Avail:   168.12 GB / 511.09 GB (32.9%)\n",
            "Train Data Rows:    2138\n",
            "Train Data Columns: 25\n",
            "Label Column: CI_HOUR\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2040.676667, 0.0725, 213.46147, 324.2631)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    17034.92 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 5): ['U_WIND', 'V_WIND', 'AIR_TEMPERATURE', 'BN', 'year']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) :  7 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) :  7 | ['DIST', 'BREADTH', 'DEPTH', 'DRAUGHT', 'LENGTH', ...]\n",
            "\t\t('int', [])   : 13 | ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'BUILT', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t20 features in original data used to generate 20 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.27 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.09s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-195.947\t = Validation score   (-mean_absolute_error)\n",
            "\t0.0s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-197.2383\t = Validation score   (-mean_absolute_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-175.6988\t = Validation score   (-mean_absolute_error)\n",
            "\t2.3s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-174.5902\t = Validation score   (-mean_absolute_error)\n",
            "\t2.15s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-182.4643\t = Validation score   (-mean_absolute_error)\n",
            "\t1.22s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-173.2851\t = Validation score   (-mean_absolute_error)\n",
            "\t10.77s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-179.9488\t = Validation score   (-mean_absolute_error)\n",
            "\t0.48s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-184.7125\t = Validation score   (-mean_absolute_error)\n",
            "\t6.14s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=10996, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=10996, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "2023-10-20 01:18:22,328\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:18:22,334\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:18:22,337\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:18:22,345\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:18:22,348\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:18:22,351\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:18:22,355\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-166.5604\t = Validation score   (-mean_absolute_error)\n",
            "\t7.98s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-181.6831\t = Validation score   (-mean_absolute_error)\n",
            "\t3.42s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-163.5786\t = Validation score   (-mean_absolute_error)\n",
            "\t0.35s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 9 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-175.5375\t = Validation score   (-mean_absolute_error)\n",
            "\t2.66s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-177.9948\t = Validation score   (-mean_absolute_error)\n",
            "\t2.88s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-183.4402\t = Validation score   (-mean_absolute_error)\n",
            "\t2.87s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-172.7389\t = Validation score   (-mean_absolute_error)\n",
            "\t6.3s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\t-182.0091\t = Validation score   (-mean_absolute_error)\n",
            "\t0.69s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-179.9762\t = Validation score   (-mean_absolute_error)\n",
            "\t6.72s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=43156, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1733, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1684, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py\", line 169, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 266, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py\", line 592, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 581, in after_all_folds_scheduled\n",
            "    raise processed_exception\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 541, in after_all_folds_scheduled\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\ray\\_private\\worker.py\", line 2380, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=43156, ip=127.0.0.1)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 857, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 861, in ray._raylet.execute_task\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 829, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"c:\\Users\\ineeji\\anaconda3\\envs\\v1\\lib\\site-packages\\autogluon\\tabular\\models\\xgboost\\xgboost_model.py\", line 143, in _fit\n",
            "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
            "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "2023-10-20 01:19:15,808\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:19:15,817\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:19:15,819\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:19:15,819\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:19:15,819\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:19:15,832\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2023-10-20 01:19:15,832\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-160.0965\t = Validation score   (-mean_absolute_error)\n",
            "\t9.9s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ...\n",
            "Specified total num_gpus: 1, but only 0 are available. Will use 0 instead\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
            "\t-181.0058\t = Validation score   (-mean_absolute_error)\n",
            "\t4.43s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\t-159.0826\t = Validation score   (-mean_absolute_error)\n",
            "\t0.28s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 109.22s ... Best model: \"WeightedEnsemble_L3\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231019_161745\\\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2014\n",
            "2014\n",
            "2014\n",
            "2014\n",
            "2014\n",
            "2138\n",
            "2138\n",
            "2138\n",
            "2138\n",
            "2138\n",
            "          SAMPLE_ID    CI_HOUR\n",
            "0       TEST_000000  30.150824\n",
            "1       TEST_000001  28.454451\n",
            "2       TEST_000002  26.296280\n",
            "3       TEST_000003  86.548218\n",
            "4       TEST_000004   0.329801\n",
            "...             ...        ...\n",
            "220486  TEST_220486  92.346962\n",
            "220487  TEST_220487  60.992531\n",
            "220488  TEST_220488  26.162733\n",
            "220489  TEST_220489   5.040003\n",
            "220490  TEST_220490  34.408936\n",
            "\n",
            "[220491 rows x 2 columns]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>30.150824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>28.454451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>26.296280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>86.548218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>0.329801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220486</th>\n",
              "      <td>TEST_220486</td>\n",
              "      <td>92.346962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220487</th>\n",
              "      <td>TEST_220487</td>\n",
              "      <td>60.992531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220488</th>\n",
              "      <td>TEST_220488</td>\n",
              "      <td>26.162733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220489</th>\n",
              "      <td>TEST_220489</td>\n",
              "      <td>5.040003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220490</th>\n",
              "      <td>TEST_220490</td>\n",
              "      <td>34.408936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220491 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID    CI_HOUR\n",
              "0       TEST_000000  30.150824\n",
              "1       TEST_000001  28.454451\n",
              "2       TEST_000002  26.296280\n",
              "3       TEST_000003  86.548218\n",
              "4       TEST_000004   0.329801\n",
              "...             ...        ...\n",
              "220486  TEST_220486  92.346962\n",
              "220487  TEST_220487  60.992531\n",
              "220488  TEST_220488  26.162733\n",
              "220489  TEST_220489   5.040003\n",
              "220490  TEST_220490  34.408936\n",
              "\n",
              "[220491 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "# 각 ARI_CO 별로 데이터 분할 후 학습 및 예측\n",
        "unique_ari_co = train['year'].unique()\n",
        "\n",
        "for ari_co in unique_ari_co:\n",
        "    # ARI_CO 별 데이터 분할\n",
        "    train_subset = train[train['year'] == ari_co].copy()\n",
        "    test_subset = test[test['year'] == ari_co].copy()\n",
        "    #train_subset.drop(['year'],axis=1,inplace=True)\n",
        "    #test_subset.drop(['year'],axis=1,inplace=True)    \n",
        "\n",
        "    # 데이터셋 변환\n",
        "    train_data = TabularDataset(train_subset)\n",
        "    test_data = TabularDataset(test_subset)\n",
        "    \n",
        "    # 모델 학습\n",
        "    predictor = TabularPredictor(label='CI_HOUR', eval_metric='mean_absolute_error', path = None).fit(\n",
        "        train_data, \n",
        "        presets='best_quality',\n",
        "        #ag_args_fit={'num_gpus': 0},\n",
        "        #ag_args_fit={'num_cpus': 1, 'num_gpus': 0}\n",
        "        #ag_args_fit={'num_gpus': 0, 'num_cpus': 1}\n",
        "        num_gpus=1\n",
        "        #included_model_types=['CAT']\n",
        "    )\n",
        "    \n",
        "    # 예측 및 결과 저장\n",
        "    y_pred = predictor.predict(test_data)\n",
        "    submission.loc[test_subset.index, 'CI_HOUR'] = y_pred.values\n",
        "    print(ari_co)\n",
        "    print(ari_co)\n",
        "    print(ari_co)\n",
        "    print(ari_co)\n",
        "    print(ari_co)\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    print(len(train_subset))\n",
        "    predictor.leaderboard(train, silent=True)    \n",
        "\n",
        "# 최종 결과 확인\n",
        "print(submission)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>DEPTH</th>\n",
              "      <th>DRAUGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>CI_HOUR</th>\n",
              "      <th>종가</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>30.881018</td>\n",
              "      <td>21975</td>\n",
              "      <td>30.0</td>\n",
              "      <td>24</td>\n",
              "      <td>24300</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>5</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>3.450000</td>\n",
              "      <td>1129.55</td>\n",
              "      <td>2018</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17</td>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>27.037650</td>\n",
              "      <td>807</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10</td>\n",
              "      <td>116000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>25.600000</td>\n",
              "      <td>2.495953</td>\n",
              "      <td>15</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>253.554444</td>\n",
              "      <td>1169.20</td>\n",
              "      <td>2020</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>91</td>\n",
              "      <td>0</td>\n",
              "      <td>49.953585</td>\n",
              "      <td>16077</td>\n",
              "      <td>40.0</td>\n",
              "      <td>7</td>\n",
              "      <td>183000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>28.100000</td>\n",
              "      <td>4.016217</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>68.391389</td>\n",
              "      <td>1128.00</td>\n",
              "      <td>2021</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>67</td>\n",
              "      <td>2</td>\n",
              "      <td>42.276281</td>\n",
              "      <td>650</td>\n",
              "      <td>20.0</td>\n",
              "      <td>30</td>\n",
              "      <td>6800</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>10</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>31.700556</td>\n",
              "      <td>1172.89</td>\n",
              "      <td>2016</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>101.521598</td>\n",
              "      <td>15818</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "      <td>124000</td>\n",
              "      <td>30.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>3</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>58.193056</td>\n",
              "      <td>1120.44</td>\n",
              "      <td>2018</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234641</th>\n",
              "      <td>4</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>21.866691</td>\n",
              "      <td>18428</td>\n",
              "      <td>40.0</td>\n",
              "      <td>13</td>\n",
              "      <td>169000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>23</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>89.884167</td>\n",
              "      <td>1122.13</td>\n",
              "      <td>2017</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234642</th>\n",
              "      <td>4</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>9.647703</td>\n",
              "      <td>6146</td>\n",
              "      <td>30.0</td>\n",
              "      <td>26</td>\n",
              "      <td>69000</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>21.400000</td>\n",
              "      <td>3.298553</td>\n",
              "      <td>21</td>\n",
              "      <td>0.000939</td>\n",
              "      <td>1095.597222</td>\n",
              "      <td>1189.61</td>\n",
              "      <td>2020</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234643</th>\n",
              "      <td>17</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>5.884603</td>\n",
              "      <td>9087</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12</td>\n",
              "      <td>3160</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>27.300000</td>\n",
              "      <td>1.253491</td>\n",
              "      <td>8</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>144.061389</td>\n",
              "      <td>1185.51</td>\n",
              "      <td>2019</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234644</th>\n",
              "      <td>21</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>70.660241</td>\n",
              "      <td>7861</td>\n",
              "      <td>30.0</td>\n",
              "      <td>8</td>\n",
              "      <td>60300</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>21.100000</td>\n",
              "      <td>4.766257</td>\n",
              "      <td>18</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>41.482222</td>\n",
              "      <td>1132.90</td>\n",
              "      <td>2021</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234645</th>\n",
              "      <td>19</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>9.448179</td>\n",
              "      <td>8238</td>\n",
              "      <td>30.0</td>\n",
              "      <td>29</td>\n",
              "      <td>23800</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>15</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>7.485278</td>\n",
              "      <td>1095.51</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>234646 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        ARI_CO  ARI_PO  SHIP_TYPE_CATEGORY        DIST     ID  BREADTH  BUILT  \\\n",
              "0           17      21                   2   30.881018  21975     30.0     24   \n",
              "1           17      21                   2   27.037650    807     50.0     10   \n",
              "2            0      91                   0   49.953585  16077     40.0      7   \n",
              "3            6      67                   2   42.276281    650     20.0     30   \n",
              "4            4      45                   2  101.521598  15818     50.0      7   \n",
              "...        ...     ...                 ...         ...    ...      ...    ...   \n",
              "234641       4      89                   0   21.866691  18428     40.0     13   \n",
              "234642       4      44                   0    9.647703   6146     30.0     26   \n",
              "234643      17      21                   0    5.884603   9087     10.0     12   \n",
              "234644      21      61                   0   70.660241   7861     30.0      8   \n",
              "234645      19      35                   2    9.448179   8238     30.0     29   \n",
              "\n",
              "        DEADWEIGHT  DEPTH  DRAUGHT  ...  AIR_TEMPERATURE        BN  ATA_LT  \\\n",
              "0            24300   10.0     10.0  ...        19.295802  2.658560       5   \n",
              "1           116000   20.0     10.0  ...        25.600000  2.495953      15   \n",
              "2           183000   20.0     20.0  ...        28.100000  4.016217       5   \n",
              "3             6800   10.0     10.0  ...        19.295802  2.658560      10   \n",
              "4           124000   30.0     20.0  ...        19.295802  2.658560       3   \n",
              "...            ...    ...      ...  ...              ...       ...     ...   \n",
              "234641      169000   20.0     20.0  ...        19.295802  2.658560      23   \n",
              "234642       69000   20.0     10.0  ...        21.400000  3.298553      21   \n",
              "234643        3160   10.0     10.0  ...        27.300000  1.253491       8   \n",
              "234644       60300   20.0     10.0  ...        21.100000  4.766257      18   \n",
              "234645       23800   10.0     10.0  ...        19.295802  2.658560      15   \n",
              "\n",
              "        PORT_SIZE      CI_HOUR       종가  year  month  day  weekday  \n",
              "0        0.002615     3.450000  1129.55  2018     12   17        0  \n",
              "1        0.002615   253.554444  1169.20  2020      1   26        6  \n",
              "2        0.000103    68.391389  1128.00  2021      3    5        4  \n",
              "3        0.000041    31.700556  1172.89  2016     12   11        6  \n",
              "4        0.001743    58.193056  1120.44  2018     11   30        4  \n",
              "...           ...          ...      ...   ...    ...  ...      ...  \n",
              "234641   0.000618    89.884167  1122.13  2017      6    1        3  \n",
              "234642   0.000939  1095.597222  1189.61  2020      3    2        0  \n",
              "234643   0.002615   144.061389  1185.51  2019     10   16        2  \n",
              "234644   0.000155    41.482222  1132.90  2021      3   23        1  \n",
              "234645   0.000990     7.485278  1095.51  2015      1    8        3  \n",
              "\n",
              "[234646 rows x 26 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ARI_CO</th>\n",
              "      <th>ARI_PO</th>\n",
              "      <th>SHIP_TYPE_CATEGORY</th>\n",
              "      <th>DIST</th>\n",
              "      <th>ID</th>\n",
              "      <th>BREADTH</th>\n",
              "      <th>BUILT</th>\n",
              "      <th>DEADWEIGHT</th>\n",
              "      <th>DEPTH</th>\n",
              "      <th>DRAUGHT</th>\n",
              "      <th>...</th>\n",
              "      <th>V_WIND</th>\n",
              "      <th>AIR_TEMPERATURE</th>\n",
              "      <th>BN</th>\n",
              "      <th>ATA_LT</th>\n",
              "      <th>PORT_SIZE</th>\n",
              "      <th>종가</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>1.826589</td>\n",
              "      <td>8718</td>\n",
              "      <td>50.0</td>\n",
              "      <td>18</td>\n",
              "      <td>117000</td>\n",
              "      <td>30.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.63000</td>\n",
              "      <td>27.100000</td>\n",
              "      <td>1.587063</td>\n",
              "      <td>19</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>1212.50</td>\n",
              "      <td>2020</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "      <td>25.399386</td>\n",
              "      <td>3731</td>\n",
              "      <td>10.0</td>\n",
              "      <td>13</td>\n",
              "      <td>3810</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.33000</td>\n",
              "      <td>14.200000</td>\n",
              "      <td>2.663972</td>\n",
              "      <td>6</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>1116.61</td>\n",
              "      <td>2021</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>111.079467</td>\n",
              "      <td>4962</td>\n",
              "      <td>20.0</td>\n",
              "      <td>26</td>\n",
              "      <td>10900</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.91000</td>\n",
              "      <td>9.300000</td>\n",
              "      <td>3.255315</td>\n",
              "      <td>8</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>1167.48</td>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>9.175258</td>\n",
              "      <td>653</td>\n",
              "      <td>30.0</td>\n",
              "      <td>9</td>\n",
              "      <td>55800</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.06336</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>1169.64</td>\n",
              "      <td>2015</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>750</td>\n",
              "      <td>30.0</td>\n",
              "      <td>19</td>\n",
              "      <td>39800</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.06336</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>10</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>1139.86</td>\n",
              "      <td>2018</td>\n",
              "      <td>10</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220486</th>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>21.712733</td>\n",
              "      <td>4073</td>\n",
              "      <td>60.0</td>\n",
              "      <td>15</td>\n",
              "      <td>298000</td>\n",
              "      <td>30.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.06336</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>1125.10</td>\n",
              "      <td>2017</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220487</th>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>4.870490</td>\n",
              "      <td>14092</td>\n",
              "      <td>30.0</td>\n",
              "      <td>9</td>\n",
              "      <td>80500</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.06336</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>13</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>1123.74</td>\n",
              "      <td>2016</td>\n",
              "      <td>8</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220488</th>\n",
              "      <td>8</td>\n",
              "      <td>88</td>\n",
              "      <td>3</td>\n",
              "      <td>17.068286</td>\n",
              "      <td>17213</td>\n",
              "      <td>10.0</td>\n",
              "      <td>14</td>\n",
              "      <td>1200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.22000</td>\n",
              "      <td>27.400000</td>\n",
              "      <td>2.405268</td>\n",
              "      <td>23</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>1294.68</td>\n",
              "      <td>2022</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220489</th>\n",
              "      <td>4</td>\n",
              "      <td>62</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1869</td>\n",
              "      <td>10.0</td>\n",
              "      <td>27</td>\n",
              "      <td>3420</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.64000</td>\n",
              "      <td>8.200000</td>\n",
              "      <td>2.199039</td>\n",
              "      <td>22</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>1095.60</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220490</th>\n",
              "      <td>12</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>32.280098</td>\n",
              "      <td>11317</td>\n",
              "      <td>30.0</td>\n",
              "      <td>35</td>\n",
              "      <td>41800</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.06336</td>\n",
              "      <td>19.295802</td>\n",
              "      <td>2.658560</td>\n",
              "      <td>8</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>1152.78</td>\n",
              "      <td>2015</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220491 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        ARI_CO  ARI_PO  SHIP_TYPE_CATEGORY        DIST     ID  BREADTH  BUILT  \\\n",
              "0           17      21                   2    1.826589   8718     50.0     18   \n",
              "1            4      90                   1   25.399386   3731     10.0     13   \n",
              "2            4      45                   2  111.079467   4962     20.0     26   \n",
              "3            2      19                   0    9.175258    653     30.0      9   \n",
              "4            8      66                   2    0.000000    750     30.0     19   \n",
              "...        ...     ...                 ...         ...    ...      ...    ...   \n",
              "220486       1      76                   0   21.712733   4073     60.0     15   \n",
              "220487       2      23                   0    4.870490  14092     30.0      9   \n",
              "220488       8      88                   3   17.068286  17213     10.0     14   \n",
              "220489       4      62                   1    0.000000   1869     10.0     27   \n",
              "220490      12      53                   2   32.280098  11317     30.0     35   \n",
              "\n",
              "        DEADWEIGHT  DEPTH  DRAUGHT  ...   V_WIND  AIR_TEMPERATURE        BN  \\\n",
              "0           117000   30.0     20.0  ...  1.63000        27.100000  1.587063   \n",
              "1             3810   10.0     10.0  ... -2.33000        14.200000  2.663972   \n",
              "2            10900   10.0     10.0  ... -4.91000         9.300000  3.255315   \n",
              "3            55800   20.0     10.0  ... -0.06336        19.295802  2.658560   \n",
              "4            39800   20.0     10.0  ... -0.06336        19.295802  2.658560   \n",
              "...            ...    ...      ...  ...      ...              ...       ...   \n",
              "220486      298000   30.0     20.0  ... -0.06336        19.295802  2.658560   \n",
              "220487       80500   20.0     10.0  ... -0.06336        19.295802  2.658560   \n",
              "220488        1200    0.0      0.0  ...  1.22000        27.400000  2.405268   \n",
              "220489        3420   10.0      0.0  ... -0.64000         8.200000  2.199039   \n",
              "220490       41800   20.0     10.0  ... -0.06336        19.295802  2.658560   \n",
              "\n",
              "        ATA_LT  PORT_SIZE       종가  year  month  day  weekday  \n",
              "0           19   0.002615  1212.50  2020      6   18        3  \n",
              "1            6   0.001028  1116.61  2021      5   26        2  \n",
              "2            8   0.001743  1167.48  2019     12   16        0  \n",
              "3            2   0.000182  1169.64  2015     11   16        0  \n",
              "4           10   0.000552  1139.86  2018     10   24        2  \n",
              "...        ...        ...      ...   ...    ...  ...      ...  \n",
              "220486      13   0.000080  1125.10  2017      4    4        1  \n",
              "220487      13   0.000039  1123.74  2016      8   22        0  \n",
              "220488      23   0.000264  1294.68  2022      7   10        6  \n",
              "220489      22   0.000595  1095.60  2020     12   28        0  \n",
              "220490       8   0.000054  1152.78  2015     11    7        5  \n",
              "\n",
              "[220491 rows x 25 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.to_csv('SAINT_TRAIN.csv',index=False)\n",
        "test.to_csv('SAINT_TEST.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>30.150824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>28.454451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>26.296280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>86.548218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220486</th>\n",
              "      <td>TEST_220486</td>\n",
              "      <td>92.346962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220487</th>\n",
              "      <td>TEST_220487</td>\n",
              "      <td>60.992531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220488</th>\n",
              "      <td>TEST_220488</td>\n",
              "      <td>26.162733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220489</th>\n",
              "      <td>TEST_220489</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220490</th>\n",
              "      <td>TEST_220490</td>\n",
              "      <td>34.408936</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220491 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID    CI_HOUR\n",
              "0       TEST_000000  30.150824\n",
              "1       TEST_000001  28.454451\n",
              "2       TEST_000002  26.296280\n",
              "3       TEST_000003  86.548218\n",
              "4       TEST_000004   0.000000\n",
              "...             ...        ...\n",
              "220486  TEST_220486  92.346962\n",
              "220487  TEST_220487  60.992531\n",
              "220488  TEST_220488  26.162733\n",
              "220489  TEST_220489   0.000000\n",
              "220490  TEST_220490  34.408936\n",
              "\n",
              "[220491 rows x 2 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "all = pd.concat([test,submission],axis=1)\n",
        "#all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "all.loc[all['DIST'] == 0, 'CI_HOUR'] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "#submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('SEP_NEW_EXCLUDE_0.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>234646.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>103.450118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>210.795773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.002778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>12.703125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>36.002917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>99.740347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2159.130556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             CI_HOUR\n",
              "count  234646.000000\n",
              "mean      103.450118\n",
              "std       210.795773\n",
              "min         0.002778\n",
              "25%        12.703125\n",
              "50%        36.002917\n",
              "75%        99.740347\n",
              "max      2159.130556"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = train[['CI_HOUR']].copy()\n",
        "z = z[z['CI_HOUR'] != 0]\n",
        "z.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>146652.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>102.612986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>204.702926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.425004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>17.598591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>40.946835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>95.859177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2065.305176</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             CI_HOUR\n",
              "count  146652.000000\n",
              "mean      102.612986\n",
              "std       204.702926\n",
              "min         1.425004\n",
              "25%        17.598591\n",
              "50%        40.946835\n",
              "75%        95.859177\n",
              "max      2065.305176"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = pd.read_csv('NEW_VERSION1.csv')\n",
        "a = a[a['CI_HOUR'] != 0]\n",
        "a.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>132061.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>64.002258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>98.202330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.217417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>20.696900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>39.623740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>76.239235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1542.976800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             CI_HOUR\n",
              "count  132061.000000\n",
              "mean       64.002258\n",
              "std        98.202330\n",
              "min         0.217417\n",
              "25%        20.696900\n",
              "50%        39.623740\n",
              "75%        76.239235\n",
              "max      1542.976800"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b = pd.read_csv('NEW_V_P.csv')\n",
        "b = b[b['CI_HOUR'] != 0]\n",
        "b.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>4.201626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>17.887573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>30.305592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>101.903278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220486</th>\n",
              "      <td>TEST_220486</td>\n",
              "      <td>62.115261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220487</th>\n",
              "      <td>TEST_220487</td>\n",
              "      <td>54.566376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220488</th>\n",
              "      <td>TEST_220488</td>\n",
              "      <td>28.786565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220489</th>\n",
              "      <td>TEST_220489</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220490</th>\n",
              "      <td>TEST_220490</td>\n",
              "      <td>68.924713</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220491 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID     CI_HOUR\n",
              "0       TEST_000000    4.201626\n",
              "1       TEST_000001   17.887573\n",
              "2       TEST_000002   30.305592\n",
              "3       TEST_000003  101.903278\n",
              "4       TEST_000004    0.000000\n",
              "...             ...         ...\n",
              "220486  TEST_220486   62.115261\n",
              "220487  TEST_220487   54.566376\n",
              "220488  TEST_220488   28.786565\n",
              "220489  TEST_220489    0.000000\n",
              "220490  TEST_220490   68.924713\n",
              "\n",
              "[220491 rows x 2 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_v_p_data = pd.read_csv('NEW_V_P.csv')\n",
        "\n",
        "min_value = new_v_p_data['CI_HOUR'].min()\n",
        "max_value = new_v_p_data['CI_HOUR'].max()\n",
        "scaled_ci_hour = new_v_p_data.copy()\n",
        "scaled_ci_hour['CI_HOUR'] = (new_v_p_data['CI_HOUR'] - min_value) / (max_value - min_value) * 2000\n",
        "scaled_ci_hour['CI_HOUR'] = (new_v_p_data['CI_HOUR']*0.5 + scaled_ci_hour['CI_HOUR']*0.5)\n",
        "scaled_ci_hour.to_csv('testing2.csv',index=False)\n",
        "scaled_ci_hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>31.217644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>22.565344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>25.581632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>89.963154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220486</th>\n",
              "      <td>TEST_220486</td>\n",
              "      <td>72.539057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220487</th>\n",
              "      <td>TEST_220487</td>\n",
              "      <td>52.043848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220488</th>\n",
              "      <td>TEST_220488</td>\n",
              "      <td>27.433267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220489</th>\n",
              "      <td>TEST_220489</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220490</th>\n",
              "      <td>TEST_220490</td>\n",
              "      <td>45.642019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220491 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID    CI_HOUR\n",
              "0       TEST_000000  31.217644\n",
              "1       TEST_000001  22.565344\n",
              "2       TEST_000002  25.581632\n",
              "3       TEST_000003  89.963154\n",
              "4       TEST_000004   0.000000\n",
              "...             ...        ...\n",
              "220486  TEST_220486  72.539057\n",
              "220487  TEST_220487  52.043848\n",
              "220488  TEST_220488  27.433267\n",
              "220489  TEST_220489   0.000000\n",
              "220490  TEST_220490  45.642019\n",
              "\n",
              "[220491 rows x 2 columns]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = pd.read_csv('NEW_V_P.csv')\n",
        "sub = submission.copy()\n",
        "sub['CI_HOUR'] = (a['CI_HOUR']*0.5 + submission['CI_HOUR']*0.5)\n",
        "sub.to_csv('jugwang.csv',index=False)\n",
        "sub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "#import xgboost as xgb\n",
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def objective(trial: Trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "            'iterations':trial.suggest_int(\"iterations\", 300, 1000),\n",
        "            'learning_rate' : trial.suggest_uniform('learning_rate',0.1, 1),\n",
        "            'depth': trial.suggest_int('depth',5, 16),\n",
        "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
        "            'reg_lambda': trial.suggest_uniform('reg_lambda',30,100),\n",
        "            'subsample': trial.suggest_uniform('subsample',0.3,1),\n",
        "            'random_strength': trial.suggest_uniform('random_strength',10,100),\n",
        "            'od_wait':trial.suggest_int('od_wait', 10, 150),\n",
        "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,20),\n",
        "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 1, 100),\n",
        "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0., 1.0),\n",
        "            'random_state' : 724,\n",
        "            'verbose' : 0,\n",
        "        }\n",
        "    #'task_type' : 'GPU',\n",
        "    #\"eval_metric\":'RMSE',\n",
        "    cat = CatBoostRegressor(**params)\n",
        "    cat.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val,y_val)],cat_features=cat_features,\n",
        "              verbose=False)\n",
        "    cat_pred = cat.predict(X_val)\n",
        "    score = mean_absolute_error(y_val, cat_pred)\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import bisect\n",
        "'''\n",
        "# Categorical 컬럼 인코딩\n",
        "categorical_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "encoders = {}\n",
        "\n",
        "for feature in tqdm(categorical_features, desc=\"Encoding features\"):\n",
        "    le = LabelEncoder()\n",
        "    train[feature] = le.fit_transform(train[feature].astype(str))\n",
        "    le_classes_set = set(le.classes_)\n",
        "    test[feature] = test[feature].map(lambda s: '-1' if s not in le_classes_set else s)\n",
        "    le_classes = le.classes_.tolist()\n",
        "    bisect.insort_left(le_classes, '-1')\n",
        "    le.classes_ = np.array(le_classes)\n",
        "    test[feature] = le.transform(test[feature].astype(str))\n",
        "    encoders[feature] = le\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG']\n",
        "\n",
        "# 결측치 처리\n",
        "train_x.fillna(train_x.mean(), inplace=True)\n",
        "test.fillna(train_x.mean(), inplace=True)\n",
        "'''\n",
        "cat_features = ['ARI_CO', 'ARI_PO', 'SHIP_TYPE_CATEGORY', 'ID', 'SHIPMANAGER', 'FLAG', 'country_cluster', 'PORT_SIZE_Zone', 'BREADTH', 'DEPTH', 'DRAUGHT']\n",
        "\n",
        "# 수치형 변수만 대상으로 결측치 대체\n",
        "numeric_cols = train_x.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 훈련 데이터의 평균 계산\n",
        "mean_values = train_x[numeric_cols].mean()\n",
        "\n",
        "# 결측치 대체\n",
        "train_x[numeric_cols] = train_x[numeric_cols].fillna(mean_values)\n",
        "test[numeric_cols] = test[numeric_cols].fillna(mean_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 220055 entries, 0 to 220054\n",
            "Data columns (total 57 columns):\n",
            " #   Column                 Non-Null Count   Dtype  \n",
            "---  ------                 --------------   -----  \n",
            " 0   ARI_CO                 220055 non-null  object \n",
            " 1   ARI_PO                 220055 non-null  object \n",
            " 2   SHIP_TYPE_CATEGORY     220055 non-null  object \n",
            " 3   DIST                   220055 non-null  float64\n",
            " 4   ID                     220055 non-null  object \n",
            " 5   BREADTH                220055 non-null  object \n",
            " 6   BUILT                  220055 non-null  int64  \n",
            " 7   DEADWEIGHT             220055 non-null  int64  \n",
            " 8   DEPTH                  220055 non-null  object \n",
            " 9   DRAUGHT                220055 non-null  object \n",
            " 10  GT                     220055 non-null  int64  \n",
            " 11  LENGTH                 220055 non-null  float64\n",
            " 12  SHIPMANAGER            220055 non-null  object \n",
            " 13  FLAG                   220055 non-null  object \n",
            " 14  U_WIND                 220055 non-null  float64\n",
            " 15  V_WIND                 220055 non-null  float64\n",
            " 16  AIR_TEMPERATURE        220055 non-null  float64\n",
            " 17  BN                     220055 non-null  float64\n",
            " 18  ATA_LT                 220055 non-null  int64  \n",
            " 19  DUBAI                  220055 non-null  float64\n",
            " 20  BRENT                  220055 non-null  float64\n",
            " 21  WTI                    220055 non-null  float64\n",
            " 22  BDI_ADJ                220055 non-null  float64\n",
            " 23  PORT_SIZE              220055 non-null  float64\n",
            " 24  year                   220055 non-null  int64  \n",
            " 25  month                  220055 non-null  int64  \n",
            " 26  day                    220055 non-null  int64  \n",
            " 27  weekday                220055 non-null  int64  \n",
            " 28  rounded_hour           220055 non-null  int64  \n",
            " 29  month_sin              220055 non-null  float64\n",
            " 30  month_cos              220055 non-null  float64\n",
            " 31  day_sin                220055 non-null  float64\n",
            " 32  day_cos                220055 non-null  float64\n",
            " 33  weekday_sin            220055 non-null  float64\n",
            " 34  weekday_cos            220055 non-null  float64\n",
            " 35  rounded_hour_sin       220055 non-null  float64\n",
            " 36  rounded_hour_cos       220055 non-null  float64\n",
            " 37  mean_enc_ARI_CO        220055 non-null  float64\n",
            " 38  std_enc_ARI_CO         220055 non-null  float64\n",
            " 39  mean_enc_ARI_PO        220055 non-null  float64\n",
            " 40  std_enc_ARI_PO         220055 non-null  float64\n",
            " 41  mean_enc_year          220055 non-null  float64\n",
            " 42  std_enc_year           220055 non-null  float64\n",
            " 43  mean_enc_month         220055 non-null  float64\n",
            " 44  std_enc_month          220055 non-null  float64\n",
            " 45  mean_enc_day           220055 non-null  float64\n",
            " 46  std_enc_day            220055 non-null  float64\n",
            " 47  mean_enc_weekday       220055 non-null  float64\n",
            " 48  std_enc_weekday        220055 non-null  float64\n",
            " 49  mean_enc_rounded_hour  220055 non-null  float64\n",
            " 50  std_enc_rounded_hour   220055 non-null  float64\n",
            " 51  DUBAI_BRENT            220055 non-null  float64\n",
            " 52  DUBAI_WTI              220055 non-null  float64\n",
            " 53  BRENT_WTI              220055 non-null  float64\n",
            " 54  DUBAI_BRENT_WTI        220055 non-null  float64\n",
            " 55  country_cluster        220055 non-null  object \n",
            " 56  PORT_SIZE_Zone         220055 non-null  object \n",
            "dtypes: float64(37), int64(9), object(11)\n",
            "memory usage: 95.7+ MB\n"
          ]
        }
      ],
      "source": [
        "train_x.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모든 카테고리형 변수를 문자열로 변환\n",
        "categorical_cols = train_x.select_dtypes(include=['object', 'category']).columns\n",
        "train_x[categorical_cols] = train_x[categorical_cols].astype(str)\n",
        "test[categorical_cols] = test[categorical_cols].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-05 23:13:14,949]\u001b[0m A new study created in memory with name: no-name-d5bdd458-3bc8-4f77-9659-7cde3598a38d\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-05 23:15:17,832]\u001b[0m Trial 0 finished with value: 55.43213467773245 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 55.43213467773245.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:17:27,695]\u001b[0m Trial 1 finished with value: 46.73698461577896 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 46.73698461577896.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:19:36,021]\u001b[0m Trial 2 finished with value: 46.62981528541205 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 2 with value: 46.62981528541205.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:34:52,448]\u001b[0m Trial 3 finished with value: 74.28485108171094 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 2 with value: 46.62981528541205.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:36:54,216]\u001b[0m Trial 4 finished with value: 54.26968333125182 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 2 with value: 46.62981528541205.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:52:36,066]\u001b[0m Trial 5 finished with value: 63.15186508619714 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 2 with value: 46.62981528541205.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:55:38,924]\u001b[0m Trial 6 finished with value: 44.95889748083088 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-05 23:58:44,801]\u001b[0m Trial 7 finished with value: 49.657188254768535 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:00:20,943]\u001b[0m Trial 8 finished with value: 66.80661066382237 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:20:16,993]\u001b[0m Trial 9 finished with value: 56.57158302952333 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:21:19,395]\u001b[0m Trial 10 finished with value: 82.3248366699982 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:22:46,639]\u001b[0m Trial 11 finished with value: 57.13620617491669 and parameters: {'iterations': 629, 'learning_rate': 0.2764615591885931, 'depth': 6, 'min_data_in_leaf': 6, 'reg_lambda': 98.40975370477312, 'subsample': 0.6137915450890752, 'random_strength': 13.607559365869914, 'od_wait': 12, 'leaf_estimation_iterations': 13, 'bagging_temperature': 99.66177975539146, 'colsample_bylevel': 0.6037888285524304}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:26:08,909]\u001b[0m Trial 12 finished with value: 49.14633388193534 and parameters: {'iterations': 312, 'learning_rate': 0.3353168481850217, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 83.3833616301794, 'subsample': 0.6018370841486229, 'random_strength': 19.02310110968608, 'od_wait': 11, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.2397968272557836, 'colsample_bylevel': 0.6798103370630707}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:28:25,938]\u001b[0m Trial 13 finished with value: 45.26200780404295 and parameters: {'iterations': 768, 'learning_rate': 0.39733087730033134, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 51.709612487085614, 'subsample': 0.8428965893650274, 'random_strength': 68.23293339853619, 'od_wait': 31, 'leaf_estimation_iterations': 10, 'bagging_temperature': 13.355307798299567, 'colsample_bylevel': 0.7317116575399958}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:30:19,639]\u001b[0m Trial 14 finished with value: 63.67955570219674 and parameters: {'iterations': 741, 'learning_rate': 0.12115442835598422, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 51.397128917456506, 'subsample': 0.879970061817357, 'random_strength': 74.51101113965332, 'od_wait': 94, 'leaf_estimation_iterations': 10, 'bagging_temperature': 39.38019209537576, 'colsample_bylevel': 0.5241786552111687}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:31:27,115]\u001b[0m Trial 15 finished with value: 64.94279214643767 and parameters: {'iterations': 815, 'learning_rate': 0.32308158949960997, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 49.37245683486181, 'subsample': 0.9930815447318664, 'random_strength': 66.99328267466255, 'od_wait': 41, 'leaf_estimation_iterations': 13, 'bagging_temperature': 14.566066853374414, 'colsample_bylevel': 0.061943926372203806}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:32:41,363]\u001b[0m Trial 16 finished with value: 66.59544160127574 and parameters: {'iterations': 592, 'learning_rate': 0.2407235903215652, 'depth': 5, 'min_data_in_leaf': 8, 'reg_lambda': 62.839769611979925, 'subsample': 0.8595044436039624, 'random_strength': 98.23074027293896, 'od_wait': 31, 'leaf_estimation_iterations': 8, 'bagging_temperature': 20.16763561176596, 'colsample_bylevel': 0.7715955209093116}. Best is trial 6 with value: 44.95889748083088.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:35:02,315]\u001b[0m Trial 17 finished with value: 42.25324087037929 and parameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}. Best is trial 17 with value: 42.25324087037929.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:42:54,049]\u001b[0m Trial 18 finished with value: 51.32044274263317 and parameters: {'iterations': 838, 'learning_rate': 0.4270062418190881, 'depth': 12, 'min_data_in_leaf': 13, 'reg_lambda': 40.65971705190806, 'subsample': 0.6934883378818296, 'random_strength': 87.60982739927447, 'od_wait': 86, 'leaf_estimation_iterations': 15, 'bagging_temperature': 55.57442205671075, 'colsample_bylevel': 0.22089840378700903}. Best is trial 17 with value: 42.25324087037929.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:45:56,620]\u001b[0m Trial 19 finished with value: 45.429234667463305 and parameters: {'iterations': 996, 'learning_rate': 0.18484933160405892, 'depth': 8, 'min_data_in_leaf': 20, 'reg_lambda': 42.564868866728915, 'subsample': 0.729625123932983, 'random_strength': 45.629127031175706, 'od_wait': 63, 'leaf_estimation_iterations': 17, 'bagging_temperature': 50.428094719702216, 'colsample_bylevel': 0.46751740730087066}. Best is trial 17 with value: 42.25324087037929.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 1: 42.25324087037929\n",
            "Best hyperparameters for fold 1: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "0:\tlearn: 210.0952407\ttotal: 96.6ms\tremaining: 1m 16s\n",
            "1:\tlearn: 210.0940552\ttotal: 147ms\tremaining: 58.1s\n",
            "2:\tlearn: 210.0938004\ttotal: 208ms\tremaining: 54.6s\n",
            "3:\tlearn: 209.1358980\ttotal: 362ms\tremaining: 1m 11s\n",
            "4:\tlearn: 207.6132118\ttotal: 492ms\tremaining: 1m 17s\n",
            "5:\tlearn: 206.2839777\ttotal: 653ms\tremaining: 1m 25s\n",
            "6:\tlearn: 206.0279903\ttotal: 791ms\tremaining: 1m 28s\n",
            "7:\tlearn: 204.0681873\ttotal: 966ms\tremaining: 1m 34s\n",
            "8:\tlearn: 204.0638360\ttotal: 1.01s\tremaining: 1m 28s\n",
            "9:\tlearn: 203.4280451\ttotal: 1.22s\tremaining: 1m 35s\n",
            "10:\tlearn: 203.1901894\ttotal: 1.39s\tremaining: 1m 38s\n",
            "11:\tlearn: 203.1871571\ttotal: 1.44s\tremaining: 1m 33s\n",
            "12:\tlearn: 203.1871571\ttotal: 1.48s\tremaining: 1m 28s\n",
            "13:\tlearn: 202.7864161\ttotal: 1.64s\tremaining: 1m 31s\n",
            "14:\tlearn: 202.7498112\ttotal: 1.79s\tremaining: 1m 32s\n",
            "15:\tlearn: 202.7169463\ttotal: 1.88s\tremaining: 1m 31s\n",
            "16:\tlearn: 202.7116879\ttotal: 1.94s\tremaining: 1m 28s\n",
            "17:\tlearn: 202.6794050\ttotal: 2s\tremaining: 1m 25s\n",
            "18:\tlearn: 202.5796489\ttotal: 2.15s\tremaining: 1m 27s\n",
            "19:\tlearn: 201.9102861\ttotal: 2.31s\tremaining: 1m 29s\n",
            "20:\tlearn: 201.7871639\ttotal: 2.46s\tremaining: 1m 30s\n",
            "21:\tlearn: 201.1280271\ttotal: 2.62s\tremaining: 1m 31s\n",
            "22:\tlearn: 201.0473370\ttotal: 2.76s\tremaining: 1m 32s\n",
            "23:\tlearn: 201.0349141\ttotal: 2.85s\tremaining: 1m 31s\n",
            "24:\tlearn: 201.0337697\ttotal: 2.92s\tremaining: 1m 29s\n",
            "25:\tlearn: 200.9380558\ttotal: 3.07s\tremaining: 1m 30s\n",
            "26:\tlearn: 199.9271637\ttotal: 3.21s\tremaining: 1m 31s\n",
            "27:\tlearn: 199.8806071\ttotal: 3.35s\tremaining: 1m 31s\n",
            "28:\tlearn: 199.8806071\ttotal: 3.39s\tremaining: 1m 29s\n",
            "29:\tlearn: 199.8760280\ttotal: 3.49s\tremaining: 1m 28s\n",
            "30:\tlearn: 199.8224237\ttotal: 3.65s\tremaining: 1m 29s\n",
            "31:\tlearn: 199.7900642\ttotal: 3.76s\tremaining: 1m 29s\n",
            "32:\tlearn: 199.1561864\ttotal: 3.84s\tremaining: 1m 28s\n",
            "33:\tlearn: 199.1077818\ttotal: 3.95s\tremaining: 1m 27s\n",
            "34:\tlearn: 198.0753486\ttotal: 4.08s\tremaining: 1m 28s\n",
            "35:\tlearn: 197.3901252\ttotal: 4.23s\tremaining: 1m 28s\n",
            "36:\tlearn: 196.5337674\ttotal: 4.37s\tremaining: 1m 29s\n",
            "37:\tlearn: 195.9275310\ttotal: 4.45s\tremaining: 1m 28s\n",
            "38:\tlearn: 194.2686065\ttotal: 4.6s\tremaining: 1m 28s\n",
            "39:\tlearn: 193.1647405\ttotal: 4.77s\tremaining: 1m 29s\n",
            "40:\tlearn: 191.9579967\ttotal: 4.92s\tremaining: 1m 30s\n",
            "41:\tlearn: 191.0653347\ttotal: 5.06s\tremaining: 1m 30s\n",
            "42:\tlearn: 190.0334194\ttotal: 5.22s\tremaining: 1m 30s\n",
            "43:\tlearn: 186.4136590\ttotal: 5.36s\tremaining: 1m 31s\n",
            "44:\tlearn: 182.6431525\ttotal: 5.49s\tremaining: 1m 31s\n",
            "45:\tlearn: 181.7990732\ttotal: 5.63s\tremaining: 1m 31s\n",
            "46:\tlearn: 181.4313116\ttotal: 5.78s\tremaining: 1m 31s\n",
            "47:\tlearn: 181.1349264\ttotal: 5.94s\tremaining: 1m 32s\n",
            "48:\tlearn: 179.8998331\ttotal: 6.06s\tremaining: 1m 31s\n",
            "49:\tlearn: 179.5230399\ttotal: 6.2s\tremaining: 1m 32s\n",
            "50:\tlearn: 178.9687448\ttotal: 6.33s\tremaining: 1m 31s\n",
            "51:\tlearn: 177.2375298\ttotal: 6.45s\tremaining: 1m 31s\n",
            "52:\tlearn: 175.8033348\ttotal: 6.58s\tremaining: 1m 31s\n",
            "53:\tlearn: 172.4107905\ttotal: 6.71s\tremaining: 1m 31s\n",
            "54:\tlearn: 170.5903894\ttotal: 6.85s\tremaining: 1m 31s\n",
            "55:\tlearn: 168.6452129\ttotal: 7s\tremaining: 1m 31s\n",
            "56:\tlearn: 168.2381520\ttotal: 7.15s\tremaining: 1m 32s\n",
            "57:\tlearn: 167.6856517\ttotal: 7.28s\tremaining: 1m 32s\n",
            "58:\tlearn: 167.0560245\ttotal: 7.42s\tremaining: 1m 32s\n",
            "59:\tlearn: 165.1588766\ttotal: 7.56s\tremaining: 1m 32s\n",
            "60:\tlearn: 164.8136277\ttotal: 7.69s\tremaining: 1m 32s\n",
            "61:\tlearn: 164.4797273\ttotal: 7.86s\tremaining: 1m 32s\n",
            "62:\tlearn: 163.4114617\ttotal: 7.99s\tremaining: 1m 32s\n",
            "63:\tlearn: 162.8691691\ttotal: 8.12s\tremaining: 1m 32s\n",
            "64:\tlearn: 162.4686740\ttotal: 8.26s\tremaining: 1m 32s\n",
            "65:\tlearn: 161.7235236\ttotal: 8.41s\tremaining: 1m 32s\n",
            "66:\tlearn: 161.7221740\ttotal: 8.56s\tremaining: 1m 32s\n",
            "67:\tlearn: 160.6943269\ttotal: 8.7s\tremaining: 1m 32s\n",
            "68:\tlearn: 160.4930070\ttotal: 8.85s\tremaining: 1m 32s\n",
            "69:\tlearn: 159.6979241\ttotal: 8.98s\tremaining: 1m 32s\n",
            "70:\tlearn: 159.3741101\ttotal: 9.14s\tremaining: 1m 32s\n",
            "71:\tlearn: 158.1264455\ttotal: 9.28s\tremaining: 1m 32s\n",
            "72:\tlearn: 156.4218738\ttotal: 9.42s\tremaining: 1m 32s\n",
            "73:\tlearn: 155.7613544\ttotal: 9.55s\tremaining: 1m 32s\n",
            "74:\tlearn: 154.8884126\ttotal: 9.69s\tremaining: 1m 32s\n",
            "75:\tlearn: 154.6643223\ttotal: 9.84s\tremaining: 1m 32s\n",
            "76:\tlearn: 154.3643960\ttotal: 10s\tremaining: 1m 32s\n",
            "77:\tlearn: 153.4344879\ttotal: 10.1s\tremaining: 1m 32s\n",
            "78:\tlearn: 153.1934894\ttotal: 10.3s\tremaining: 1m 32s\n",
            "79:\tlearn: 151.1676158\ttotal: 10.4s\tremaining: 1m 32s\n",
            "80:\tlearn: 149.8963672\ttotal: 10.5s\tremaining: 1m 32s\n",
            "81:\tlearn: 148.7130206\ttotal: 10.7s\tremaining: 1m 32s\n",
            "82:\tlearn: 148.4868546\ttotal: 10.8s\tremaining: 1m 32s\n",
            "83:\tlearn: 148.0747566\ttotal: 11s\tremaining: 1m 32s\n",
            "84:\tlearn: 147.7053038\ttotal: 11.1s\tremaining: 1m 32s\n",
            "85:\tlearn: 146.8548651\ttotal: 11.2s\tremaining: 1m 32s\n",
            "86:\tlearn: 146.3716506\ttotal: 11.4s\tremaining: 1m 32s\n",
            "87:\tlearn: 146.1204575\ttotal: 11.5s\tremaining: 1m 32s\n",
            "88:\tlearn: 145.9078541\ttotal: 11.7s\tremaining: 1m 32s\n",
            "89:\tlearn: 145.2668398\ttotal: 11.8s\tremaining: 1m 32s\n",
            "90:\tlearn: 142.9933332\ttotal: 11.9s\tremaining: 1m 32s\n",
            "91:\tlearn: 142.3837054\ttotal: 12.1s\tremaining: 1m 31s\n",
            "92:\tlearn: 140.7833630\ttotal: 12.2s\tremaining: 1m 31s\n",
            "93:\tlearn: 139.8799827\ttotal: 12.4s\tremaining: 1m 31s\n",
            "94:\tlearn: 139.4488272\ttotal: 12.5s\tremaining: 1m 31s\n",
            "95:\tlearn: 139.2982959\ttotal: 12.6s\tremaining: 1m 31s\n",
            "96:\tlearn: 139.0891601\ttotal: 12.8s\tremaining: 1m 31s\n",
            "97:\tlearn: 138.6579933\ttotal: 12.9s\tremaining: 1m 31s\n",
            "98:\tlearn: 138.1032388\ttotal: 13.1s\tremaining: 1m 31s\n",
            "99:\tlearn: 137.3941482\ttotal: 13.2s\tremaining: 1m 31s\n",
            "100:\tlearn: 136.9778564\ttotal: 13.3s\tremaining: 1m 31s\n",
            "101:\tlearn: 136.7626121\ttotal: 13.5s\tremaining: 1m 31s\n",
            "102:\tlearn: 136.7606318\ttotal: 13.6s\tremaining: 1m 31s\n",
            "103:\tlearn: 136.6103311\ttotal: 13.8s\tremaining: 1m 31s\n",
            "104:\tlearn: 136.3113936\ttotal: 13.9s\tremaining: 1m 30s\n",
            "105:\tlearn: 136.2095836\ttotal: 14.1s\tremaining: 1m 31s\n",
            "106:\tlearn: 135.9230702\ttotal: 14.2s\tremaining: 1m 31s\n",
            "107:\tlearn: 135.8137929\ttotal: 14.4s\tremaining: 1m 30s\n",
            "108:\tlearn: 135.5717783\ttotal: 14.5s\tremaining: 1m 30s\n",
            "109:\tlearn: 134.9473610\ttotal: 14.6s\tremaining: 1m 30s\n",
            "110:\tlearn: 133.8762430\ttotal: 14.8s\tremaining: 1m 30s\n",
            "111:\tlearn: 133.3428223\ttotal: 14.9s\tremaining: 1m 30s\n",
            "112:\tlearn: 133.1797235\ttotal: 15.1s\tremaining: 1m 30s\n",
            "113:\tlearn: 132.6931764\ttotal: 15.2s\tremaining: 1m 30s\n",
            "114:\tlearn: 131.9367748\ttotal: 15.3s\tremaining: 1m 30s\n",
            "115:\tlearn: 131.1855789\ttotal: 15.5s\tremaining: 1m 30s\n",
            "116:\tlearn: 130.8149515\ttotal: 15.6s\tremaining: 1m 29s\n",
            "117:\tlearn: 130.2619289\ttotal: 15.7s\tremaining: 1m 29s\n",
            "118:\tlearn: 129.9223056\ttotal: 15.9s\tremaining: 1m 29s\n",
            "119:\tlearn: 129.8130120\ttotal: 16s\tremaining: 1m 29s\n",
            "120:\tlearn: 129.6323397\ttotal: 16.1s\tremaining: 1m 29s\n",
            "121:\tlearn: 129.4790607\ttotal: 16.3s\tremaining: 1m 29s\n",
            "122:\tlearn: 128.8986469\ttotal: 16.4s\tremaining: 1m 29s\n",
            "123:\tlearn: 128.5807345\ttotal: 16.5s\tremaining: 1m 29s\n",
            "124:\tlearn: 127.8073098\ttotal: 16.7s\tremaining: 1m 28s\n",
            "125:\tlearn: 126.5196171\ttotal: 16.8s\tremaining: 1m 28s\n",
            "126:\tlearn: 126.3781441\ttotal: 16.9s\tremaining: 1m 28s\n",
            "127:\tlearn: 126.0887387\ttotal: 17.1s\tremaining: 1m 28s\n",
            "128:\tlearn: 125.9257902\ttotal: 17.2s\tremaining: 1m 28s\n",
            "129:\tlearn: 125.7848247\ttotal: 17.4s\tremaining: 1m 28s\n",
            "130:\tlearn: 125.3476768\ttotal: 17.6s\tremaining: 1m 28s\n",
            "131:\tlearn: 123.6859347\ttotal: 17.7s\tremaining: 1m 28s\n",
            "132:\tlearn: 123.3636746\ttotal: 17.8s\tremaining: 1m 28s\n",
            "133:\tlearn: 123.2226466\ttotal: 18s\tremaining: 1m 28s\n",
            "134:\tlearn: 122.8461255\ttotal: 18.1s\tremaining: 1m 28s\n",
            "135:\tlearn: 122.3126261\ttotal: 18.2s\tremaining: 1m 27s\n",
            "136:\tlearn: 121.3855679\ttotal: 18.4s\tremaining: 1m 27s\n",
            "137:\tlearn: 121.0656076\ttotal: 18.5s\tremaining: 1m 27s\n",
            "138:\tlearn: 120.9348360\ttotal: 18.6s\tremaining: 1m 27s\n",
            "139:\tlearn: 120.7479529\ttotal: 18.8s\tremaining: 1m 27s\n",
            "140:\tlearn: 120.6465056\ttotal: 18.9s\tremaining: 1m 27s\n",
            "141:\tlearn: 120.5414389\ttotal: 19.1s\tremaining: 1m 27s\n",
            "142:\tlearn: 119.7243098\ttotal: 19.2s\tremaining: 1m 27s\n",
            "143:\tlearn: 119.5769180\ttotal: 19.4s\tremaining: 1m 27s\n",
            "144:\tlearn: 119.3580356\ttotal: 19.5s\tremaining: 1m 27s\n",
            "145:\tlearn: 119.2272936\ttotal: 19.7s\tremaining: 1m 26s\n",
            "146:\tlearn: 119.0441174\ttotal: 19.8s\tremaining: 1m 26s\n",
            "147:\tlearn: 117.9700071\ttotal: 19.9s\tremaining: 1m 26s\n",
            "148:\tlearn: 117.8059741\ttotal: 20.1s\tremaining: 1m 26s\n",
            "149:\tlearn: 117.6200735\ttotal: 20.2s\tremaining: 1m 26s\n",
            "150:\tlearn: 117.5087627\ttotal: 20.4s\tremaining: 1m 26s\n",
            "151:\tlearn: 117.3282076\ttotal: 20.5s\tremaining: 1m 26s\n",
            "152:\tlearn: 116.8791058\ttotal: 20.7s\tremaining: 1m 26s\n",
            "153:\tlearn: 116.5544420\ttotal: 20.8s\tremaining: 1m 26s\n",
            "154:\tlearn: 115.6938623\ttotal: 20.9s\tremaining: 1m 25s\n",
            "155:\tlearn: 115.5739669\ttotal: 21.1s\tremaining: 1m 25s\n",
            "156:\tlearn: 115.3302841\ttotal: 21.2s\tremaining: 1m 25s\n",
            "157:\tlearn: 114.6562768\ttotal: 21.3s\tremaining: 1m 25s\n",
            "158:\tlearn: 114.5403863\ttotal: 21.5s\tremaining: 1m 25s\n",
            "159:\tlearn: 114.0321405\ttotal: 21.6s\tremaining: 1m 25s\n",
            "160:\tlearn: 113.4554827\ttotal: 21.8s\tremaining: 1m 25s\n",
            "161:\tlearn: 113.2252116\ttotal: 21.9s\tremaining: 1m 25s\n",
            "162:\tlearn: 112.6154248\ttotal: 22.1s\tremaining: 1m 25s\n",
            "163:\tlearn: 112.3924245\ttotal: 22.2s\tremaining: 1m 25s\n",
            "164:\tlearn: 112.1799044\ttotal: 22.3s\tremaining: 1m 24s\n",
            "165:\tlearn: 111.8524878\ttotal: 22.5s\tremaining: 1m 24s\n",
            "166:\tlearn: 111.6615266\ttotal: 22.6s\tremaining: 1m 24s\n",
            "167:\tlearn: 111.3793556\ttotal: 22.7s\tremaining: 1m 24s\n",
            "168:\tlearn: 111.1843352\ttotal: 22.9s\tremaining: 1m 24s\n",
            "169:\tlearn: 110.6985066\ttotal: 23s\tremaining: 1m 24s\n",
            "170:\tlearn: 110.6192167\ttotal: 23.2s\tremaining: 1m 24s\n",
            "171:\tlearn: 109.9474991\ttotal: 23.3s\tremaining: 1m 24s\n",
            "172:\tlearn: 109.8585393\ttotal: 23.5s\tremaining: 1m 24s\n",
            "173:\tlearn: 109.7345032\ttotal: 23.6s\tremaining: 1m 23s\n",
            "174:\tlearn: 109.6423525\ttotal: 23.8s\tremaining: 1m 23s\n",
            "175:\tlearn: 109.5816780\ttotal: 23.9s\tremaining: 1m 23s\n",
            "176:\tlearn: 108.9044012\ttotal: 24s\tremaining: 1m 23s\n",
            "177:\tlearn: 108.4430840\ttotal: 24.2s\tremaining: 1m 23s\n",
            "178:\tlearn: 108.0803470\ttotal: 24.3s\tremaining: 1m 23s\n",
            "179:\tlearn: 107.8248946\ttotal: 24.4s\tremaining: 1m 23s\n",
            "180:\tlearn: 107.6708706\ttotal: 24.6s\tremaining: 1m 22s\n",
            "181:\tlearn: 107.5961077\ttotal: 24.8s\tremaining: 1m 22s\n",
            "182:\tlearn: 107.0231279\ttotal: 24.9s\tremaining: 1m 22s\n",
            "183:\tlearn: 106.8187897\ttotal: 25s\tremaining: 1m 22s\n",
            "184:\tlearn: 106.5582515\ttotal: 25.1s\tremaining: 1m 22s\n",
            "185:\tlearn: 106.3967160\ttotal: 25.3s\tremaining: 1m 22s\n",
            "186:\tlearn: 105.9589557\ttotal: 25.4s\tremaining: 1m 22s\n",
            "187:\tlearn: 105.7116903\ttotal: 25.6s\tremaining: 1m 22s\n",
            "188:\tlearn: 105.4703524\ttotal: 25.7s\tremaining: 1m 21s\n",
            "189:\tlearn: 105.3597666\ttotal: 25.9s\tremaining: 1m 21s\n",
            "190:\tlearn: 105.1278874\ttotal: 26s\tremaining: 1m 21s\n",
            "191:\tlearn: 104.7975760\ttotal: 26.1s\tremaining: 1m 21s\n",
            "192:\tlearn: 104.6585242\ttotal: 26.3s\tremaining: 1m 21s\n",
            "193:\tlearn: 104.5676483\ttotal: 26.4s\tremaining: 1m 21s\n",
            "194:\tlearn: 104.3572228\ttotal: 26.6s\tremaining: 1m 21s\n",
            "195:\tlearn: 104.1865305\ttotal: 26.7s\tremaining: 1m 21s\n",
            "196:\tlearn: 104.0987645\ttotal: 26.9s\tremaining: 1m 21s\n",
            "197:\tlearn: 104.0952030\ttotal: 27s\tremaining: 1m 21s\n",
            "198:\tlearn: 104.0177617\ttotal: 27.2s\tremaining: 1m 21s\n",
            "199:\tlearn: 103.9413795\ttotal: 27.3s\tremaining: 1m 20s\n",
            "200:\tlearn: 103.5236940\ttotal: 27.5s\tremaining: 1m 20s\n",
            "201:\tlearn: 103.4409015\ttotal: 27.6s\tremaining: 1m 20s\n",
            "202:\tlearn: 103.0300789\ttotal: 27.8s\tremaining: 1m 20s\n",
            "203:\tlearn: 102.9039120\ttotal: 27.9s\tremaining: 1m 20s\n",
            "204:\tlearn: 102.7611080\ttotal: 28s\tremaining: 1m 20s\n",
            "205:\tlearn: 102.5969076\ttotal: 28.2s\tremaining: 1m 20s\n",
            "206:\tlearn: 102.5292034\ttotal: 28.4s\tremaining: 1m 20s\n",
            "207:\tlearn: 101.9416208\ttotal: 28.5s\tremaining: 1m 19s\n",
            "208:\tlearn: 101.6522649\ttotal: 28.6s\tremaining: 1m 19s\n",
            "209:\tlearn: 101.5840252\ttotal: 28.7s\tremaining: 1m 19s\n",
            "210:\tlearn: 101.5025119\ttotal: 28.9s\tremaining: 1m 19s\n",
            "211:\tlearn: 101.1531987\ttotal: 29s\tremaining: 1m 19s\n",
            "212:\tlearn: 100.9116228\ttotal: 29.1s\tremaining: 1m 19s\n",
            "213:\tlearn: 100.8049936\ttotal: 29.3s\tremaining: 1m 19s\n",
            "214:\tlearn: 100.4064304\ttotal: 29.5s\tremaining: 1m 19s\n",
            "215:\tlearn: 100.3139164\ttotal: 29.6s\tremaining: 1m 18s\n",
            "216:\tlearn: 100.2646989\ttotal: 29.8s\tremaining: 1m 18s\n",
            "217:\tlearn: 100.1654750\ttotal: 29.9s\tremaining: 1m 18s\n",
            "218:\tlearn: 99.8221844\ttotal: 30s\tremaining: 1m 18s\n",
            "219:\tlearn: 99.5320405\ttotal: 30.2s\tremaining: 1m 18s\n",
            "220:\tlearn: 99.3838228\ttotal: 30.3s\tremaining: 1m 18s\n",
            "221:\tlearn: 99.2297651\ttotal: 30.5s\tremaining: 1m 18s\n",
            "222:\tlearn: 99.0951114\ttotal: 30.6s\tremaining: 1m 18s\n",
            "223:\tlearn: 98.8019940\ttotal: 30.7s\tremaining: 1m 17s\n",
            "224:\tlearn: 98.7163208\ttotal: 30.9s\tremaining: 1m 17s\n",
            "225:\tlearn: 98.6012922\ttotal: 31s\tremaining: 1m 17s\n",
            "226:\tlearn: 98.4886869\ttotal: 31.2s\tremaining: 1m 17s\n",
            "227:\tlearn: 98.4228923\ttotal: 31.3s\tremaining: 1m 17s\n",
            "228:\tlearn: 98.2588328\ttotal: 31.4s\tremaining: 1m 17s\n",
            "229:\tlearn: 98.2555381\ttotal: 31.6s\tremaining: 1m 17s\n",
            "230:\tlearn: 98.1583016\ttotal: 31.7s\tremaining: 1m 17s\n",
            "231:\tlearn: 97.9899179\ttotal: 31.9s\tremaining: 1m 16s\n",
            "232:\tlearn: 97.9228007\ttotal: 32.1s\tremaining: 1m 16s\n",
            "233:\tlearn: 97.8867925\ttotal: 32.2s\tremaining: 1m 16s\n",
            "234:\tlearn: 97.4058282\ttotal: 32.3s\tremaining: 1m 16s\n",
            "235:\tlearn: 97.1630261\ttotal: 32.5s\tremaining: 1m 16s\n",
            "236:\tlearn: 96.9987648\ttotal: 32.6s\tremaining: 1m 16s\n",
            "237:\tlearn: 96.8220212\ttotal: 32.8s\tremaining: 1m 16s\n",
            "238:\tlearn: 96.7310125\ttotal: 32.9s\tremaining: 1m 16s\n",
            "239:\tlearn: 96.4690182\ttotal: 33s\tremaining: 1m 15s\n",
            "240:\tlearn: 96.3302006\ttotal: 33.2s\tremaining: 1m 15s\n",
            "241:\tlearn: 96.1690217\ttotal: 33.3s\tremaining: 1m 15s\n",
            "242:\tlearn: 96.0563863\ttotal: 33.4s\tremaining: 1m 15s\n",
            "243:\tlearn: 95.6922593\ttotal: 33.6s\tremaining: 1m 15s\n",
            "244:\tlearn: 95.1598318\ttotal: 33.7s\tremaining: 1m 15s\n",
            "245:\tlearn: 95.0527915\ttotal: 33.8s\tremaining: 1m 15s\n",
            "246:\tlearn: 94.9894984\ttotal: 34s\tremaining: 1m 15s\n",
            "247:\tlearn: 94.4988418\ttotal: 34.1s\tremaining: 1m 14s\n",
            "248:\tlearn: 94.4143138\ttotal: 34.3s\tremaining: 1m 14s\n",
            "249:\tlearn: 94.3383201\ttotal: 34.5s\tremaining: 1m 14s\n",
            "250:\tlearn: 94.2190453\ttotal: 34.6s\tremaining: 1m 14s\n",
            "251:\tlearn: 94.0627239\ttotal: 34.7s\tremaining: 1m 14s\n",
            "252:\tlearn: 93.9443145\ttotal: 34.9s\tremaining: 1m 14s\n",
            "253:\tlearn: 93.7600114\ttotal: 35s\tremaining: 1m 14s\n",
            "254:\tlearn: 93.7068131\ttotal: 35.2s\tremaining: 1m 14s\n",
            "255:\tlearn: 93.6615921\ttotal: 35.3s\tremaining: 1m 13s\n",
            "256:\tlearn: 93.5533111\ttotal: 35.5s\tremaining: 1m 13s\n",
            "257:\tlearn: 93.4294652\ttotal: 35.6s\tremaining: 1m 13s\n",
            "258:\tlearn: 93.4118247\ttotal: 35.8s\tremaining: 1m 13s\n",
            "259:\tlearn: 93.1414778\ttotal: 35.9s\tremaining: 1m 13s\n",
            "260:\tlearn: 93.0664370\ttotal: 36s\tremaining: 1m 13s\n",
            "261:\tlearn: 92.9614661\ttotal: 36.2s\tremaining: 1m 13s\n",
            "262:\tlearn: 92.6407340\ttotal: 36.3s\tremaining: 1m 13s\n",
            "263:\tlearn: 92.5719822\ttotal: 36.5s\tremaining: 1m 12s\n",
            "264:\tlearn: 92.5262793\ttotal: 36.6s\tremaining: 1m 12s\n",
            "265:\tlearn: 92.2736954\ttotal: 36.7s\tremaining: 1m 12s\n",
            "266:\tlearn: 91.9038996\ttotal: 36.9s\tremaining: 1m 12s\n",
            "267:\tlearn: 91.8206471\ttotal: 37s\tremaining: 1m 12s\n",
            "268:\tlearn: 91.5246603\ttotal: 37.2s\tremaining: 1m 12s\n",
            "269:\tlearn: 91.3301693\ttotal: 37.3s\tremaining: 1m 12s\n",
            "270:\tlearn: 91.2557751\ttotal: 37.5s\tremaining: 1m 12s\n",
            "271:\tlearn: 91.0788696\ttotal: 37.6s\tremaining: 1m 11s\n",
            "272:\tlearn: 91.0491470\ttotal: 37.7s\tremaining: 1m 11s\n",
            "273:\tlearn: 90.9145443\ttotal: 37.9s\tremaining: 1m 11s\n",
            "274:\tlearn: 90.8515932\ttotal: 38.1s\tremaining: 1m 11s\n",
            "275:\tlearn: 90.6601013\ttotal: 38.2s\tremaining: 1m 11s\n",
            "276:\tlearn: 90.5290841\ttotal: 38.3s\tremaining: 1m 11s\n",
            "277:\tlearn: 90.4225213\ttotal: 38.5s\tremaining: 1m 11s\n",
            "278:\tlearn: 90.3225507\ttotal: 38.6s\tremaining: 1m 10s\n",
            "279:\tlearn: 90.0679889\ttotal: 38.7s\tremaining: 1m 10s\n",
            "280:\tlearn: 89.8516685\ttotal: 38.9s\tremaining: 1m 10s\n",
            "281:\tlearn: 89.6412157\ttotal: 39s\tremaining: 1m 10s\n",
            "282:\tlearn: 89.2795076\ttotal: 39.1s\tremaining: 1m 10s\n",
            "283:\tlearn: 89.1586855\ttotal: 39.2s\tremaining: 1m 10s\n",
            "284:\tlearn: 89.0657634\ttotal: 39.4s\tremaining: 1m 10s\n",
            "285:\tlearn: 88.9136256\ttotal: 39.5s\tremaining: 1m 9s\n",
            "286:\tlearn: 88.8432720\ttotal: 39.6s\tremaining: 1m 9s\n",
            "287:\tlearn: 88.5716723\ttotal: 39.8s\tremaining: 1m 9s\n",
            "288:\tlearn: 88.5214619\ttotal: 39.9s\tremaining: 1m 9s\n",
            "289:\tlearn: 88.4195664\ttotal: 40.1s\tremaining: 1m 9s\n",
            "290:\tlearn: 88.3795960\ttotal: 40.2s\tremaining: 1m 9s\n",
            "291:\tlearn: 88.3791199\ttotal: 40.3s\tremaining: 1m 9s\n",
            "292:\tlearn: 88.3284695\ttotal: 40.5s\tremaining: 1m 8s\n",
            "293:\tlearn: 88.2566886\ttotal: 40.7s\tremaining: 1m 8s\n",
            "294:\tlearn: 88.1147694\ttotal: 40.8s\tremaining: 1m 8s\n",
            "295:\tlearn: 88.0260199\ttotal: 40.9s\tremaining: 1m 8s\n",
            "296:\tlearn: 87.9016193\ttotal: 41.1s\tremaining: 1m 8s\n",
            "297:\tlearn: 87.4024339\ttotal: 41.2s\tremaining: 1m 8s\n",
            "298:\tlearn: 87.3544105\ttotal: 41.3s\tremaining: 1m 8s\n",
            "299:\tlearn: 87.2884400\ttotal: 41.5s\tremaining: 1m 8s\n",
            "300:\tlearn: 87.2270228\ttotal: 41.6s\tremaining: 1m 7s\n",
            "301:\tlearn: 87.1800454\ttotal: 41.8s\tremaining: 1m 7s\n",
            "302:\tlearn: 87.1450682\ttotal: 41.9s\tremaining: 1m 7s\n",
            "303:\tlearn: 87.0687204\ttotal: 42s\tremaining: 1m 7s\n",
            "304:\tlearn: 86.9836885\ttotal: 42.2s\tremaining: 1m 7s\n",
            "305:\tlearn: 86.5528032\ttotal: 42.3s\tremaining: 1m 7s\n",
            "306:\tlearn: 86.4940688\ttotal: 42.5s\tremaining: 1m 7s\n",
            "307:\tlearn: 86.3028859\ttotal: 42.6s\tremaining: 1m 6s\n",
            "308:\tlearn: 86.0787085\ttotal: 42.7s\tremaining: 1m 6s\n",
            "309:\tlearn: 85.9067495\ttotal: 42.9s\tremaining: 1m 6s\n",
            "310:\tlearn: 85.8391400\ttotal: 43s\tremaining: 1m 6s\n",
            "311:\tlearn: 85.7512263\ttotal: 43.2s\tremaining: 1m 6s\n",
            "312:\tlearn: 85.7467144\ttotal: 43.3s\tremaining: 1m 6s\n",
            "313:\tlearn: 85.4857352\ttotal: 43.5s\tremaining: 1m 6s\n",
            "314:\tlearn: 85.3779462\ttotal: 43.6s\tremaining: 1m 6s\n",
            "315:\tlearn: 85.3299013\ttotal: 43.8s\tremaining: 1m 5s\n",
            "316:\tlearn: 85.1600544\ttotal: 43.9s\tremaining: 1m 5s\n",
            "317:\tlearn: 85.1534932\ttotal: 44.1s\tremaining: 1m 5s\n",
            "318:\tlearn: 85.0258301\ttotal: 44.2s\tremaining: 1m 5s\n",
            "319:\tlearn: 84.7393158\ttotal: 44.3s\tremaining: 1m 5s\n",
            "320:\tlearn: 84.4781467\ttotal: 44.5s\tremaining: 1m 5s\n",
            "321:\tlearn: 84.1134675\ttotal: 44.6s\tremaining: 1m 5s\n",
            "322:\tlearn: 84.0821525\ttotal: 44.8s\tremaining: 1m 5s\n",
            "323:\tlearn: 83.9962692\ttotal: 44.9s\tremaining: 1m 4s\n",
            "324:\tlearn: 83.9391172\ttotal: 45.1s\tremaining: 1m 4s\n",
            "325:\tlearn: 83.8363487\ttotal: 45.2s\tremaining: 1m 4s\n",
            "326:\tlearn: 83.6814025\ttotal: 45.3s\tremaining: 1m 4s\n",
            "327:\tlearn: 83.6389709\ttotal: 45.5s\tremaining: 1m 4s\n",
            "328:\tlearn: 83.5747815\ttotal: 45.6s\tremaining: 1m 4s\n",
            "329:\tlearn: 83.2345399\ttotal: 45.8s\tremaining: 1m 4s\n",
            "330:\tlearn: 83.1009248\ttotal: 45.9s\tremaining: 1m 3s\n",
            "331:\tlearn: 83.0555108\ttotal: 46s\tremaining: 1m 3s\n",
            "332:\tlearn: 82.9577545\ttotal: 46.2s\tremaining: 1m 3s\n",
            "333:\tlearn: 82.8745350\ttotal: 46.4s\tremaining: 1m 3s\n",
            "334:\tlearn: 82.8456486\ttotal: 46.5s\tremaining: 1m 3s\n",
            "335:\tlearn: 82.8009643\ttotal: 46.7s\tremaining: 1m 3s\n",
            "336:\tlearn: 82.6796327\ttotal: 46.8s\tremaining: 1m 3s\n",
            "337:\tlearn: 82.6471595\ttotal: 46.9s\tremaining: 1m 3s\n",
            "338:\tlearn: 82.5979914\ttotal: 47.1s\tremaining: 1m 2s\n",
            "339:\tlearn: 82.5563679\ttotal: 47.2s\tremaining: 1m 2s\n",
            "340:\tlearn: 82.4677663\ttotal: 47.4s\tremaining: 1m 2s\n",
            "341:\tlearn: 82.2750726\ttotal: 47.5s\tremaining: 1m 2s\n",
            "342:\tlearn: 82.2310974\ttotal: 47.6s\tremaining: 1m 2s\n",
            "343:\tlearn: 82.1824708\ttotal: 47.8s\tremaining: 1m 2s\n",
            "344:\tlearn: 81.9636938\ttotal: 47.9s\tremaining: 1m 2s\n",
            "345:\tlearn: 81.8780595\ttotal: 48s\tremaining: 1m 1s\n",
            "346:\tlearn: 81.7334089\ttotal: 48.2s\tremaining: 1m 1s\n",
            "347:\tlearn: 81.6435730\ttotal: 48.3s\tremaining: 1m 1s\n",
            "348:\tlearn: 81.4583800\ttotal: 48.4s\tremaining: 1m 1s\n",
            "349:\tlearn: 81.2540126\ttotal: 48.6s\tremaining: 1m 1s\n",
            "350:\tlearn: 80.9853170\ttotal: 48.7s\tremaining: 1m 1s\n",
            "351:\tlearn: 80.9060687\ttotal: 48.8s\tremaining: 1m 1s\n",
            "352:\tlearn: 80.6547573\ttotal: 49s\tremaining: 1m\n",
            "353:\tlearn: 80.5485153\ttotal: 49.1s\tremaining: 1m\n",
            "354:\tlearn: 80.4477279\ttotal: 49.2s\tremaining: 1m\n",
            "355:\tlearn: 80.2885025\ttotal: 49.4s\tremaining: 1m\n",
            "356:\tlearn: 80.1365619\ttotal: 49.5s\tremaining: 1m\n",
            "357:\tlearn: 80.0741090\ttotal: 49.7s\tremaining: 1m\n",
            "358:\tlearn: 79.9313582\ttotal: 49.8s\tremaining: 1m\n",
            "359:\tlearn: 79.8402174\ttotal: 49.9s\tremaining: 59.9s\n",
            "360:\tlearn: 79.8051427\ttotal: 50.1s\tremaining: 59.8s\n",
            "361:\tlearn: 79.7320503\ttotal: 50.2s\tremaining: 59.6s\n",
            "362:\tlearn: 79.6501544\ttotal: 50.4s\tremaining: 59.5s\n",
            "363:\tlearn: 79.5826251\ttotal: 50.5s\tremaining: 59.4s\n",
            "364:\tlearn: 79.4850593\ttotal: 50.7s\tremaining: 59.3s\n",
            "365:\tlearn: 79.4398292\ttotal: 50.8s\tremaining: 59.1s\n",
            "366:\tlearn: 79.4017319\ttotal: 51s\tremaining: 59s\n",
            "367:\tlearn: 79.3645566\ttotal: 51.1s\tremaining: 58.9s\n",
            "368:\tlearn: 79.3113147\ttotal: 51.2s\tremaining: 58.7s\n",
            "369:\tlearn: 79.2915525\ttotal: 51.4s\tremaining: 58.6s\n",
            "370:\tlearn: 79.2724540\ttotal: 51.5s\tremaining: 58.5s\n",
            "371:\tlearn: 79.2477565\ttotal: 51.7s\tremaining: 58.3s\n",
            "372:\tlearn: 79.1177308\ttotal: 51.8s\tremaining: 58.2s\n",
            "373:\tlearn: 79.0846326\ttotal: 52s\tremaining: 58.1s\n",
            "374:\tlearn: 79.0342010\ttotal: 52.1s\tremaining: 58s\n",
            "375:\tlearn: 78.8171296\ttotal: 52.3s\tremaining: 57.8s\n",
            "376:\tlearn: 78.5978990\ttotal: 52.4s\tremaining: 57.7s\n",
            "377:\tlearn: 78.4459374\ttotal: 52.6s\tremaining: 57.6s\n",
            "378:\tlearn: 78.1735751\ttotal: 52.7s\tremaining: 57.4s\n",
            "379:\tlearn: 78.0136050\ttotal: 52.8s\tremaining: 57.3s\n",
            "380:\tlearn: 77.9503475\ttotal: 53s\tremaining: 57.1s\n",
            "381:\tlearn: 77.7959099\ttotal: 53.1s\tremaining: 57s\n",
            "382:\tlearn: 77.7330849\ttotal: 53.2s\tremaining: 56.8s\n",
            "383:\tlearn: 77.6405469\ttotal: 53.4s\tremaining: 56.7s\n",
            "384:\tlearn: 77.5804824\ttotal: 53.5s\tremaining: 56.6s\n",
            "385:\tlearn: 77.5447644\ttotal: 53.7s\tremaining: 56.4s\n",
            "386:\tlearn: 77.4870655\ttotal: 53.8s\tremaining: 56.3s\n",
            "387:\tlearn: 77.4472273\ttotal: 53.9s\tremaining: 56.2s\n",
            "388:\tlearn: 77.3766107\ttotal: 54.1s\tremaining: 56s\n",
            "389:\tlearn: 77.2724003\ttotal: 54.2s\tremaining: 55.9s\n",
            "390:\tlearn: 77.2205656\ttotal: 54.4s\tremaining: 55.8s\n",
            "391:\tlearn: 77.0691593\ttotal: 54.5s\tremaining: 55.7s\n",
            "392:\tlearn: 76.9968090\ttotal: 54.7s\tremaining: 55.5s\n",
            "393:\tlearn: 76.9529848\ttotal: 54.8s\tremaining: 55.4s\n",
            "394:\tlearn: 76.9116059\ttotal: 55s\tremaining: 55.2s\n",
            "395:\tlearn: 76.7923924\ttotal: 55.1s\tremaining: 55.1s\n",
            "396:\tlearn: 76.6737542\ttotal: 55.2s\tremaining: 55s\n",
            "397:\tlearn: 76.6348262\ttotal: 55.4s\tremaining: 54.9s\n",
            "398:\tlearn: 76.5682582\ttotal: 55.6s\tremaining: 54.7s\n",
            "399:\tlearn: 76.4101807\ttotal: 55.7s\tremaining: 54.6s\n",
            "400:\tlearn: 76.3141457\ttotal: 55.8s\tremaining: 54.4s\n",
            "401:\tlearn: 76.2031585\ttotal: 56s\tremaining: 54.3s\n",
            "402:\tlearn: 76.1622903\ttotal: 56.1s\tremaining: 54.2s\n",
            "403:\tlearn: 76.1117252\ttotal: 56.3s\tremaining: 54s\n",
            "404:\tlearn: 76.0523039\ttotal: 56.4s\tremaining: 53.9s\n",
            "405:\tlearn: 76.0016501\ttotal: 56.5s\tremaining: 53.7s\n",
            "406:\tlearn: 75.9943911\ttotal: 56.7s\tremaining: 53.6s\n",
            "407:\tlearn: 75.8604529\ttotal: 56.8s\tremaining: 53.5s\n",
            "408:\tlearn: 75.6249110\ttotal: 56.9s\tremaining: 53.3s\n",
            "409:\tlearn: 75.5424635\ttotal: 57.1s\tremaining: 53.2s\n",
            "410:\tlearn: 75.4639407\ttotal: 57.2s\tremaining: 53s\n",
            "411:\tlearn: 75.3983138\ttotal: 57.3s\tremaining: 52.9s\n",
            "412:\tlearn: 75.2921931\ttotal: 57.5s\tremaining: 52.7s\n",
            "413:\tlearn: 75.1248316\ttotal: 57.6s\tremaining: 52.6s\n",
            "414:\tlearn: 75.0867803\ttotal: 57.8s\tremaining: 52.5s\n",
            "415:\tlearn: 75.0179058\ttotal: 57.9s\tremaining: 52.3s\n",
            "416:\tlearn: 74.9746241\ttotal: 58.1s\tremaining: 52.2s\n",
            "417:\tlearn: 74.8247660\ttotal: 58.2s\tremaining: 52.1s\n",
            "418:\tlearn: 74.8031363\ttotal: 58.3s\tremaining: 51.9s\n",
            "419:\tlearn: 74.6609451\ttotal: 58.5s\tremaining: 51.8s\n",
            "420:\tlearn: 74.6016595\ttotal: 58.7s\tremaining: 51.7s\n",
            "421:\tlearn: 74.6011135\ttotal: 58.8s\tremaining: 51.6s\n",
            "422:\tlearn: 74.5460513\ttotal: 59s\tremaining: 51.4s\n",
            "423:\tlearn: 74.4769403\ttotal: 59.1s\tremaining: 51.3s\n",
            "424:\tlearn: 74.3680078\ttotal: 59.2s\tremaining: 51.2s\n",
            "425:\tlearn: 74.3662350\ttotal: 59.4s\tremaining: 51s\n",
            "426:\tlearn: 74.3250904\ttotal: 59.6s\tremaining: 50.9s\n",
            "427:\tlearn: 74.2331580\ttotal: 59.7s\tremaining: 50.8s\n",
            "428:\tlearn: 74.0479383\ttotal: 59.8s\tremaining: 50.6s\n",
            "429:\tlearn: 74.0082953\ttotal: 60s\tremaining: 50.5s\n",
            "430:\tlearn: 73.9409199\ttotal: 1m\tremaining: 50.4s\n",
            "431:\tlearn: 73.8988610\ttotal: 1m\tremaining: 50.2s\n",
            "432:\tlearn: 73.8965182\ttotal: 1m\tremaining: 50.1s\n",
            "433:\tlearn: 73.8251291\ttotal: 1m\tremaining: 50s\n",
            "434:\tlearn: 73.7360814\ttotal: 1m\tremaining: 49.8s\n",
            "435:\tlearn: 73.6016035\ttotal: 1m\tremaining: 49.7s\n",
            "436:\tlearn: 73.4376540\ttotal: 1m\tremaining: 49.5s\n",
            "437:\tlearn: 73.3664067\ttotal: 1m 1s\tremaining: 49.4s\n",
            "438:\tlearn: 73.3358121\ttotal: 1m 1s\tremaining: 49.3s\n",
            "439:\tlearn: 73.2832545\ttotal: 1m 1s\tremaining: 49.1s\n",
            "440:\tlearn: 73.2471984\ttotal: 1m 1s\tremaining: 49s\n",
            "441:\tlearn: 73.1903418\ttotal: 1m 1s\tremaining: 48.9s\n",
            "442:\tlearn: 73.0441281\ttotal: 1m 1s\tremaining: 48.7s\n",
            "443:\tlearn: 73.0270613\ttotal: 1m 2s\tremaining: 48.6s\n",
            "444:\tlearn: 73.0050855\ttotal: 1m 2s\tremaining: 48.5s\n",
            "445:\tlearn: 72.8780010\ttotal: 1m 2s\tremaining: 48.3s\n",
            "446:\tlearn: 72.8443946\ttotal: 1m 2s\tremaining: 48.2s\n",
            "447:\tlearn: 72.7229265\ttotal: 1m 2s\tremaining: 48.1s\n",
            "448:\tlearn: 72.6669421\ttotal: 1m 2s\tremaining: 47.9s\n",
            "449:\tlearn: 72.4824414\ttotal: 1m 2s\tremaining: 47.8s\n",
            "450:\tlearn: 72.4041128\ttotal: 1m 3s\tremaining: 47.6s\n",
            "451:\tlearn: 72.2986963\ttotal: 1m 3s\tremaining: 47.5s\n",
            "452:\tlearn: 72.2648535\ttotal: 1m 3s\tremaining: 47.4s\n",
            "453:\tlearn: 72.1917517\ttotal: 1m 3s\tremaining: 47.2s\n",
            "454:\tlearn: 72.1329614\ttotal: 1m 3s\tremaining: 47.1s\n",
            "455:\tlearn: 72.1165889\ttotal: 1m 3s\tremaining: 46.9s\n",
            "456:\tlearn: 71.9472045\ttotal: 1m 3s\tremaining: 46.8s\n",
            "457:\tlearn: 71.8560349\ttotal: 1m 3s\tremaining: 46.6s\n",
            "458:\tlearn: 71.8225058\ttotal: 1m 4s\tremaining: 46.5s\n",
            "459:\tlearn: 71.6630837\ttotal: 1m 4s\tremaining: 46.4s\n",
            "460:\tlearn: 71.6174450\ttotal: 1m 4s\tremaining: 46.2s\n",
            "461:\tlearn: 71.5445162\ttotal: 1m 4s\tremaining: 46.1s\n",
            "462:\tlearn: 71.5070721\ttotal: 1m 4s\tremaining: 46s\n",
            "463:\tlearn: 71.4572478\ttotal: 1m 4s\tremaining: 45.8s\n",
            "464:\tlearn: 71.2493042\ttotal: 1m 4s\tremaining: 45.7s\n",
            "465:\tlearn: 71.2022383\ttotal: 1m 5s\tremaining: 45.6s\n",
            "466:\tlearn: 71.0979199\ttotal: 1m 5s\tremaining: 45.4s\n",
            "467:\tlearn: 71.0552381\ttotal: 1m 5s\tremaining: 45.3s\n",
            "468:\tlearn: 71.0119020\ttotal: 1m 5s\tremaining: 45.2s\n",
            "469:\tlearn: 70.9733483\ttotal: 1m 5s\tremaining: 45.1s\n",
            "470:\tlearn: 70.9709226\ttotal: 1m 5s\tremaining: 44.9s\n",
            "471:\tlearn: 70.9290368\ttotal: 1m 6s\tremaining: 44.8s\n",
            "472:\tlearn: 70.9108040\ttotal: 1m 6s\tremaining: 44.7s\n",
            "473:\tlearn: 70.8395554\ttotal: 1m 6s\tremaining: 44.5s\n",
            "474:\tlearn: 70.8390769\ttotal: 1m 6s\tremaining: 44.4s\n",
            "475:\tlearn: 70.8056060\ttotal: 1m 6s\tremaining: 44.3s\n",
            "476:\tlearn: 70.6626189\ttotal: 1m 6s\tremaining: 44.1s\n",
            "477:\tlearn: 70.5731035\ttotal: 1m 6s\tremaining: 44s\n",
            "478:\tlearn: 70.4904589\ttotal: 1m 7s\tremaining: 43.8s\n",
            "479:\tlearn: 70.3927080\ttotal: 1m 7s\tremaining: 43.7s\n",
            "480:\tlearn: 70.2874175\ttotal: 1m 7s\tremaining: 43.5s\n",
            "481:\tlearn: 70.1556552\ttotal: 1m 7s\tremaining: 43.4s\n",
            "482:\tlearn: 70.1021025\ttotal: 1m 7s\tremaining: 43.3s\n",
            "483:\tlearn: 70.0353710\ttotal: 1m 7s\tremaining: 43.1s\n",
            "484:\tlearn: 69.9940671\ttotal: 1m 7s\tremaining: 43s\n",
            "485:\tlearn: 69.9459247\ttotal: 1m 8s\tremaining: 42.8s\n",
            "486:\tlearn: 69.8121789\ttotal: 1m 8s\tremaining: 42.7s\n",
            "487:\tlearn: 69.7466782\ttotal: 1m 8s\tremaining: 42.6s\n",
            "488:\tlearn: 69.6390879\ttotal: 1m 8s\tremaining: 42.4s\n",
            "489:\tlearn: 69.4488232\ttotal: 1m 8s\tremaining: 42.3s\n",
            "490:\tlearn: 69.3816829\ttotal: 1m 8s\tremaining: 42.1s\n",
            "491:\tlearn: 69.1836276\ttotal: 1m 8s\tremaining: 42s\n",
            "492:\tlearn: 69.1230936\ttotal: 1m 9s\tremaining: 41.9s\n",
            "493:\tlearn: 68.9690766\ttotal: 1m 9s\tremaining: 41.7s\n",
            "494:\tlearn: 68.9354255\ttotal: 1m 9s\tremaining: 41.6s\n",
            "495:\tlearn: 68.8183321\ttotal: 1m 9s\tremaining: 41.4s\n",
            "496:\tlearn: 68.7767292\ttotal: 1m 9s\tremaining: 41.3s\n",
            "497:\tlearn: 68.5839995\ttotal: 1m 9s\tremaining: 41.2s\n",
            "498:\tlearn: 68.4758046\ttotal: 1m 9s\tremaining: 41s\n",
            "499:\tlearn: 68.3635430\ttotal: 1m 9s\tremaining: 40.9s\n",
            "500:\tlearn: 68.2849603\ttotal: 1m 10s\tremaining: 40.7s\n",
            "501:\tlearn: 68.2585574\ttotal: 1m 10s\tremaining: 40.6s\n",
            "502:\tlearn: 68.2289922\ttotal: 1m 10s\tremaining: 40.5s\n",
            "503:\tlearn: 68.1776745\ttotal: 1m 10s\tremaining: 40.3s\n",
            "504:\tlearn: 68.1572915\ttotal: 1m 10s\tremaining: 40.2s\n",
            "505:\tlearn: 68.1031967\ttotal: 1m 10s\tremaining: 40.1s\n",
            "506:\tlearn: 67.9908737\ttotal: 1m 10s\tremaining: 39.9s\n",
            "507:\tlearn: 67.9370951\ttotal: 1m 11s\tremaining: 39.8s\n",
            "508:\tlearn: 67.7839160\ttotal: 1m 11s\tremaining: 39.6s\n",
            "509:\tlearn: 67.7298678\ttotal: 1m 11s\tremaining: 39.5s\n",
            "510:\tlearn: 67.6104168\ttotal: 1m 11s\tremaining: 39.4s\n",
            "511:\tlearn: 67.5336265\ttotal: 1m 11s\tremaining: 39.2s\n",
            "512:\tlearn: 67.4858269\ttotal: 1m 11s\tremaining: 39.1s\n",
            "513:\tlearn: 67.2671376\ttotal: 1m 11s\tremaining: 38.9s\n",
            "514:\tlearn: 67.2411281\ttotal: 1m 12s\tremaining: 38.8s\n",
            "515:\tlearn: 67.2184066\ttotal: 1m 12s\tremaining: 38.7s\n",
            "516:\tlearn: 67.1327680\ttotal: 1m 12s\tremaining: 38.5s\n",
            "517:\tlearn: 67.0992348\ttotal: 1m 12s\tremaining: 38.4s\n",
            "518:\tlearn: 67.0343408\ttotal: 1m 12s\tremaining: 38.2s\n",
            "519:\tlearn: 66.9249168\ttotal: 1m 12s\tremaining: 38.1s\n",
            "520:\tlearn: 66.8752357\ttotal: 1m 12s\tremaining: 38s\n",
            "521:\tlearn: 66.8527306\ttotal: 1m 13s\tremaining: 37.8s\n",
            "522:\tlearn: 66.8025527\ttotal: 1m 13s\tremaining: 37.7s\n",
            "523:\tlearn: 66.7519086\ttotal: 1m 13s\tremaining: 37.5s\n",
            "524:\tlearn: 66.7039152\ttotal: 1m 13s\tremaining: 37.4s\n",
            "525:\tlearn: 66.6309345\ttotal: 1m 13s\tremaining: 37.3s\n",
            "526:\tlearn: 66.5507670\ttotal: 1m 13s\tremaining: 37.1s\n",
            "527:\tlearn: 66.3949932\ttotal: 1m 13s\tremaining: 37s\n",
            "528:\tlearn: 66.3405780\ttotal: 1m 14s\tremaining: 36.8s\n",
            "529:\tlearn: 66.2411039\ttotal: 1m 14s\tremaining: 36.7s\n",
            "530:\tlearn: 66.1421116\ttotal: 1m 14s\tremaining: 36.6s\n",
            "531:\tlearn: 66.0445200\ttotal: 1m 14s\tremaining: 36.4s\n",
            "532:\tlearn: 65.9795074\ttotal: 1m 14s\tremaining: 36.3s\n",
            "533:\tlearn: 65.9787213\ttotal: 1m 14s\tremaining: 36.1s\n",
            "534:\tlearn: 65.9359941\ttotal: 1m 14s\tremaining: 36s\n",
            "535:\tlearn: 65.8136643\ttotal: 1m 15s\tremaining: 35.9s\n",
            "536:\tlearn: 65.7535630\ttotal: 1m 15s\tremaining: 35.7s\n",
            "537:\tlearn: 65.6485117\ttotal: 1m 15s\tremaining: 35.6s\n",
            "538:\tlearn: 65.5344329\ttotal: 1m 15s\tremaining: 35.4s\n",
            "539:\tlearn: 65.4731401\ttotal: 1m 15s\tremaining: 35.3s\n",
            "540:\tlearn: 65.4516568\ttotal: 1m 15s\tremaining: 35.2s\n",
            "541:\tlearn: 65.4148526\ttotal: 1m 15s\tremaining: 35s\n",
            "542:\tlearn: 65.3482105\ttotal: 1m 16s\tremaining: 34.9s\n",
            "543:\tlearn: 65.1455633\ttotal: 1m 16s\tremaining: 34.7s\n",
            "544:\tlearn: 64.9818270\ttotal: 1m 16s\tremaining: 34.6s\n",
            "545:\tlearn: 64.9297538\ttotal: 1m 16s\tremaining: 34.5s\n",
            "546:\tlearn: 64.8343040\ttotal: 1m 16s\tremaining: 34.3s\n",
            "547:\tlearn: 64.7065614\ttotal: 1m 16s\tremaining: 34.2s\n",
            "548:\tlearn: 64.6691363\ttotal: 1m 16s\tremaining: 34s\n",
            "549:\tlearn: 64.5966521\ttotal: 1m 17s\tremaining: 33.9s\n",
            "550:\tlearn: 64.5050819\ttotal: 1m 17s\tremaining: 33.8s\n",
            "551:\tlearn: 64.4369848\ttotal: 1m 17s\tremaining: 33.6s\n",
            "552:\tlearn: 64.3903815\ttotal: 1m 17s\tremaining: 33.5s\n",
            "553:\tlearn: 64.3488136\ttotal: 1m 17s\tremaining: 33.3s\n",
            "554:\tlearn: 64.3310980\ttotal: 1m 17s\tremaining: 33.2s\n",
            "555:\tlearn: 64.3004304\ttotal: 1m 17s\tremaining: 33.1s\n",
            "556:\tlearn: 64.2517186\ttotal: 1m 18s\tremaining: 32.9s\n",
            "557:\tlearn: 64.1820046\ttotal: 1m 18s\tremaining: 32.8s\n",
            "558:\tlearn: 64.1152299\ttotal: 1m 18s\tremaining: 32.6s\n",
            "559:\tlearn: 64.0779616\ttotal: 1m 18s\tremaining: 32.5s\n",
            "560:\tlearn: 64.0372780\ttotal: 1m 18s\tremaining: 32.4s\n",
            "561:\tlearn: 63.9711224\ttotal: 1m 18s\tremaining: 32.2s\n",
            "562:\tlearn: 63.9541621\ttotal: 1m 18s\tremaining: 32.1s\n",
            "563:\tlearn: 63.8997063\ttotal: 1m 18s\tremaining: 31.9s\n",
            "564:\tlearn: 63.8357844\ttotal: 1m 19s\tremaining: 31.8s\n",
            "565:\tlearn: 63.6946922\ttotal: 1m 19s\tremaining: 31.7s\n",
            "566:\tlearn: 63.6494588\ttotal: 1m 19s\tremaining: 31.5s\n",
            "567:\tlearn: 63.6025990\ttotal: 1m 19s\tremaining: 31.4s\n",
            "568:\tlearn: 63.5587479\ttotal: 1m 19s\tremaining: 31.2s\n",
            "569:\tlearn: 63.5004143\ttotal: 1m 19s\tremaining: 31.1s\n",
            "570:\tlearn: 63.4624969\ttotal: 1m 19s\tremaining: 31s\n",
            "571:\tlearn: 63.4056903\ttotal: 1m 20s\tremaining: 30.8s\n",
            "572:\tlearn: 63.3679137\ttotal: 1m 20s\tremaining: 30.7s\n",
            "573:\tlearn: 63.3033794\ttotal: 1m 20s\tremaining: 30.5s\n",
            "574:\tlearn: 63.2431832\ttotal: 1m 20s\tremaining: 30.4s\n",
            "575:\tlearn: 63.0889838\ttotal: 1m 20s\tremaining: 30.3s\n",
            "576:\tlearn: 62.9585318\ttotal: 1m 20s\tremaining: 30.1s\n",
            "577:\tlearn: 62.8105519\ttotal: 1m 20s\tremaining: 30s\n",
            "578:\tlearn: 62.7798390\ttotal: 1m 21s\tremaining: 29.8s\n",
            "579:\tlearn: 62.7016830\ttotal: 1m 21s\tremaining: 29.7s\n",
            "580:\tlearn: 62.6429932\ttotal: 1m 21s\tremaining: 29.5s\n",
            "581:\tlearn: 62.6005027\ttotal: 1m 21s\tremaining: 29.4s\n",
            "582:\tlearn: 62.5999811\ttotal: 1m 21s\tremaining: 29.3s\n",
            "583:\tlearn: 62.5667990\ttotal: 1m 21s\tremaining: 29.1s\n",
            "584:\tlearn: 62.5072122\ttotal: 1m 21s\tremaining: 29s\n",
            "585:\tlearn: 62.4657459\ttotal: 1m 22s\tremaining: 28.8s\n",
            "586:\tlearn: 62.4358917\ttotal: 1m 22s\tremaining: 28.7s\n",
            "587:\tlearn: 62.3462895\ttotal: 1m 22s\tremaining: 28.6s\n",
            "588:\tlearn: 62.3285501\ttotal: 1m 22s\tremaining: 28.4s\n",
            "589:\tlearn: 62.2328385\ttotal: 1m 22s\tremaining: 28.3s\n",
            "590:\tlearn: 62.1326551\ttotal: 1m 22s\tremaining: 28.2s\n",
            "591:\tlearn: 62.1158818\ttotal: 1m 22s\tremaining: 28s\n",
            "592:\tlearn: 62.1145517\ttotal: 1m 23s\tremaining: 27.9s\n",
            "593:\tlearn: 62.1139691\ttotal: 1m 23s\tremaining: 27.8s\n",
            "594:\tlearn: 62.0624302\ttotal: 1m 23s\tremaining: 27.6s\n",
            "595:\tlearn: 61.9998415\ttotal: 1m 23s\tremaining: 27.5s\n",
            "596:\tlearn: 61.9072026\ttotal: 1m 23s\tremaining: 27.3s\n",
            "597:\tlearn: 61.8803013\ttotal: 1m 23s\tremaining: 27.2s\n",
            "598:\tlearn: 61.8422169\ttotal: 1m 23s\tremaining: 27s\n",
            "599:\tlearn: 61.7681182\ttotal: 1m 24s\tremaining: 26.9s\n",
            "600:\tlearn: 61.7154320\ttotal: 1m 24s\tremaining: 26.8s\n",
            "601:\tlearn: 61.6946354\ttotal: 1m 24s\tremaining: 26.6s\n",
            "602:\tlearn: 61.6633122\ttotal: 1m 24s\tremaining: 26.5s\n",
            "603:\tlearn: 61.6632006\ttotal: 1m 24s\tremaining: 26.4s\n",
            "604:\tlearn: 61.5265087\ttotal: 1m 24s\tremaining: 26.2s\n",
            "605:\tlearn: 61.3867282\ttotal: 1m 24s\tremaining: 26.1s\n",
            "606:\tlearn: 61.3443516\ttotal: 1m 25s\tremaining: 25.9s\n",
            "607:\tlearn: 61.2820864\ttotal: 1m 25s\tremaining: 25.8s\n",
            "608:\tlearn: 61.2146205\ttotal: 1m 25s\tremaining: 25.7s\n",
            "609:\tlearn: 61.1306671\ttotal: 1m 25s\tremaining: 25.5s\n",
            "610:\tlearn: 61.0720161\ttotal: 1m 25s\tremaining: 25.4s\n",
            "611:\tlearn: 60.9831173\ttotal: 1m 25s\tremaining: 25.2s\n",
            "612:\tlearn: 60.9481307\ttotal: 1m 25s\tremaining: 25.1s\n",
            "613:\tlearn: 60.9197588\ttotal: 1m 26s\tremaining: 24.9s\n",
            "614:\tlearn: 60.8898651\ttotal: 1m 26s\tremaining: 24.8s\n",
            "615:\tlearn: 60.8481397\ttotal: 1m 26s\tremaining: 24.7s\n",
            "616:\tlearn: 60.7914096\ttotal: 1m 26s\tremaining: 24.5s\n",
            "617:\tlearn: 60.7240701\ttotal: 1m 26s\tremaining: 24.4s\n",
            "618:\tlearn: 60.6980103\ttotal: 1m 26s\tremaining: 24.3s\n",
            "619:\tlearn: 60.6386289\ttotal: 1m 26s\tremaining: 24.1s\n",
            "620:\tlearn: 60.5958519\ttotal: 1m 27s\tremaining: 24s\n",
            "621:\tlearn: 60.4992616\ttotal: 1m 27s\tremaining: 23.8s\n",
            "622:\tlearn: 60.4549816\ttotal: 1m 27s\tremaining: 23.7s\n",
            "623:\tlearn: 60.3450776\ttotal: 1m 27s\tremaining: 23.5s\n",
            "624:\tlearn: 60.2637184\ttotal: 1m 27s\tremaining: 23.4s\n",
            "625:\tlearn: 60.1905188\ttotal: 1m 27s\tremaining: 23.3s\n",
            "626:\tlearn: 60.1458314\ttotal: 1m 27s\tremaining: 23.1s\n",
            "627:\tlearn: 60.0745560\ttotal: 1m 27s\tremaining: 23s\n",
            "628:\tlearn: 60.0403213\ttotal: 1m 28s\tremaining: 22.8s\n",
            "629:\tlearn: 60.0184613\ttotal: 1m 28s\tremaining: 22.7s\n",
            "630:\tlearn: 59.9920419\ttotal: 1m 28s\tremaining: 22.6s\n",
            "631:\tlearn: 59.9565557\ttotal: 1m 28s\tremaining: 22.4s\n",
            "632:\tlearn: 59.9550921\ttotal: 1m 28s\tremaining: 22.3s\n",
            "633:\tlearn: 59.9189334\ttotal: 1m 28s\tremaining: 22.2s\n",
            "634:\tlearn: 59.8955844\ttotal: 1m 29s\tremaining: 22s\n",
            "635:\tlearn: 59.8645277\ttotal: 1m 29s\tremaining: 21.9s\n",
            "636:\tlearn: 59.7656402\ttotal: 1m 29s\tremaining: 21.7s\n",
            "637:\tlearn: 59.7654740\ttotal: 1m 29s\tremaining: 21.6s\n",
            "638:\tlearn: 59.7554027\ttotal: 1m 29s\tremaining: 21.5s\n",
            "639:\tlearn: 59.6821942\ttotal: 1m 29s\tremaining: 21.3s\n",
            "640:\tlearn: 59.6391539\ttotal: 1m 29s\tremaining: 21.2s\n",
            "641:\tlearn: 59.6036983\ttotal: 1m 30s\tremaining: 21s\n",
            "642:\tlearn: 59.5413581\ttotal: 1m 30s\tremaining: 20.9s\n",
            "643:\tlearn: 59.5127111\ttotal: 1m 30s\tremaining: 20.8s\n",
            "644:\tlearn: 59.4528132\ttotal: 1m 30s\tremaining: 20.6s\n",
            "645:\tlearn: 59.4521673\ttotal: 1m 30s\tremaining: 20.5s\n",
            "646:\tlearn: 59.4107720\ttotal: 1m 30s\tremaining: 20.3s\n",
            "647:\tlearn: 59.3878654\ttotal: 1m 30s\tremaining: 20.2s\n",
            "648:\tlearn: 59.3712053\ttotal: 1m 31s\tremaining: 20.1s\n",
            "649:\tlearn: 59.3383775\ttotal: 1m 31s\tremaining: 19.9s\n",
            "650:\tlearn: 59.2965812\ttotal: 1m 31s\tremaining: 19.8s\n",
            "651:\tlearn: 59.2516849\ttotal: 1m 31s\tremaining: 19.6s\n",
            "652:\tlearn: 59.1375575\ttotal: 1m 31s\tremaining: 19.5s\n",
            "653:\tlearn: 59.0972976\ttotal: 1m 31s\tremaining: 19.4s\n",
            "654:\tlearn: 59.0645640\ttotal: 1m 31s\tremaining: 19.2s\n",
            "655:\tlearn: 59.0107776\ttotal: 1m 32s\tremaining: 19.1s\n",
            "656:\tlearn: 58.9832020\ttotal: 1m 32s\tremaining: 18.9s\n",
            "657:\tlearn: 58.9565773\ttotal: 1m 32s\tremaining: 18.8s\n",
            "658:\tlearn: 58.9019787\ttotal: 1m 32s\tremaining: 18.7s\n",
            "659:\tlearn: 58.8115698\ttotal: 1m 32s\tremaining: 18.5s\n",
            "660:\tlearn: 58.7197729\ttotal: 1m 32s\tremaining: 18.4s\n",
            "661:\tlearn: 58.6404916\ttotal: 1m 32s\tremaining: 18.2s\n",
            "662:\tlearn: 58.6100751\ttotal: 1m 33s\tremaining: 18.1s\n",
            "663:\tlearn: 58.5994354\ttotal: 1m 33s\tremaining: 18s\n",
            "664:\tlearn: 58.5991957\ttotal: 1m 33s\tremaining: 17.8s\n",
            "665:\tlearn: 58.5674656\ttotal: 1m 33s\tremaining: 17.7s\n",
            "666:\tlearn: 58.4954470\ttotal: 1m 33s\tremaining: 17.5s\n",
            "667:\tlearn: 58.4290025\ttotal: 1m 33s\tremaining: 17.4s\n",
            "668:\tlearn: 58.4073805\ttotal: 1m 33s\tremaining: 17.3s\n",
            "669:\tlearn: 58.3292054\ttotal: 1m 34s\tremaining: 17.1s\n",
            "670:\tlearn: 58.2838827\ttotal: 1m 34s\tremaining: 17s\n",
            "671:\tlearn: 58.2485499\ttotal: 1m 34s\tremaining: 16.8s\n",
            "672:\tlearn: 58.1586069\ttotal: 1m 34s\tremaining: 16.7s\n",
            "673:\tlearn: 58.1206902\ttotal: 1m 34s\tremaining: 16.6s\n",
            "674:\tlearn: 58.1025971\ttotal: 1m 34s\tremaining: 16.4s\n",
            "675:\tlearn: 58.0147787\ttotal: 1m 34s\tremaining: 16.3s\n",
            "676:\tlearn: 57.9878416\ttotal: 1m 35s\tremaining: 16.1s\n",
            "677:\tlearn: 57.9590058\ttotal: 1m 35s\tremaining: 16s\n",
            "678:\tlearn: 57.9285904\ttotal: 1m 35s\tremaining: 15.9s\n",
            "679:\tlearn: 57.8389171\ttotal: 1m 35s\tremaining: 15.7s\n",
            "680:\tlearn: 57.7970998\ttotal: 1m 35s\tremaining: 15.6s\n",
            "681:\tlearn: 57.6274711\ttotal: 1m 35s\tremaining: 15.4s\n",
            "682:\tlearn: 57.5890612\ttotal: 1m 35s\tremaining: 15.3s\n",
            "683:\tlearn: 57.5117155\ttotal: 1m 35s\tremaining: 15.2s\n",
            "684:\tlearn: 57.4801208\ttotal: 1m 36s\tremaining: 15s\n",
            "685:\tlearn: 57.3716855\ttotal: 1m 36s\tremaining: 14.9s\n",
            "686:\tlearn: 57.3349832\ttotal: 1m 36s\tremaining: 14.7s\n",
            "687:\tlearn: 57.2974939\ttotal: 1m 36s\tremaining: 14.6s\n",
            "688:\tlearn: 57.2410447\ttotal: 1m 36s\tremaining: 14.5s\n",
            "689:\tlearn: 57.2025623\ttotal: 1m 36s\tremaining: 14.3s\n",
            "690:\tlearn: 57.1561669\ttotal: 1m 36s\tremaining: 14.2s\n",
            "691:\tlearn: 57.1098823\ttotal: 1m 37s\tremaining: 14s\n",
            "692:\tlearn: 57.0746328\ttotal: 1m 37s\tremaining: 13.9s\n",
            "693:\tlearn: 57.0712014\ttotal: 1m 37s\tremaining: 13.8s\n",
            "694:\tlearn: 57.0458687\ttotal: 1m 37s\tremaining: 13.6s\n",
            "695:\tlearn: 57.0001953\ttotal: 1m 37s\tremaining: 13.5s\n",
            "696:\tlearn: 56.9740003\ttotal: 1m 37s\tremaining: 13.3s\n",
            "697:\tlearn: 56.8819576\ttotal: 1m 37s\tremaining: 13.2s\n",
            "698:\tlearn: 56.7906202\ttotal: 1m 38s\tremaining: 13.1s\n",
            "699:\tlearn: 56.7670339\ttotal: 1m 38s\tremaining: 12.9s\n",
            "700:\tlearn: 56.5818046\ttotal: 1m 38s\tremaining: 12.8s\n",
            "701:\tlearn: 56.5058365\ttotal: 1m 38s\tremaining: 12.6s\n",
            "702:\tlearn: 56.4910414\ttotal: 1m 38s\tremaining: 12.5s\n",
            "703:\tlearn: 56.4678645\ttotal: 1m 38s\tremaining: 12.4s\n",
            "704:\tlearn: 56.4476849\ttotal: 1m 39s\tremaining: 12.2s\n",
            "705:\tlearn: 56.3986061\ttotal: 1m 39s\tremaining: 12.1s\n",
            "706:\tlearn: 56.3705047\ttotal: 1m 39s\tremaining: 11.9s\n",
            "707:\tlearn: 56.2654420\ttotal: 1m 39s\tremaining: 11.8s\n",
            "708:\tlearn: 56.2145947\ttotal: 1m 39s\tremaining: 11.7s\n",
            "709:\tlearn: 56.1870986\ttotal: 1m 39s\tremaining: 11.5s\n",
            "710:\tlearn: 56.1340806\ttotal: 1m 39s\tremaining: 11.4s\n",
            "711:\tlearn: 56.0991101\ttotal: 1m 40s\tremaining: 11.2s\n",
            "712:\tlearn: 55.9801230\ttotal: 1m 40s\tremaining: 11.1s\n",
            "713:\tlearn: 55.9501902\ttotal: 1m 40s\tremaining: 11s\n",
            "714:\tlearn: 55.8792845\ttotal: 1m 40s\tremaining: 10.8s\n",
            "715:\tlearn: 55.8781958\ttotal: 1m 40s\tremaining: 10.7s\n",
            "716:\tlearn: 55.8685649\ttotal: 1m 40s\tremaining: 10.5s\n",
            "717:\tlearn: 55.8230608\ttotal: 1m 40s\tremaining: 10.4s\n",
            "718:\tlearn: 55.8030532\ttotal: 1m 41s\tremaining: 10.3s\n",
            "719:\tlearn: 55.7031226\ttotal: 1m 41s\tremaining: 10.1s\n",
            "720:\tlearn: 55.6329107\ttotal: 1m 41s\tremaining: 9.98s\n",
            "721:\tlearn: 55.5993995\ttotal: 1m 41s\tremaining: 9.83s\n",
            "722:\tlearn: 55.5798983\ttotal: 1m 41s\tremaining: 9.69s\n",
            "723:\tlearn: 55.5429299\ttotal: 1m 41s\tremaining: 9.55s\n",
            "724:\tlearn: 55.5020664\ttotal: 1m 41s\tremaining: 9.41s\n",
            "725:\tlearn: 55.4793305\ttotal: 1m 41s\tremaining: 9.27s\n",
            "726:\tlearn: 55.4580257\ttotal: 1m 42s\tremaining: 9.13s\n",
            "727:\tlearn: 55.4219571\ttotal: 1m 42s\tremaining: 8.99s\n",
            "728:\tlearn: 55.3876325\ttotal: 1m 42s\tremaining: 8.85s\n",
            "729:\tlearn: 55.3653148\ttotal: 1m 42s\tremaining: 8.72s\n",
            "730:\tlearn: 55.3251254\ttotal: 1m 42s\tremaining: 8.58s\n",
            "731:\tlearn: 55.3012479\ttotal: 1m 42s\tremaining: 8.44s\n",
            "732:\tlearn: 55.2933218\ttotal: 1m 43s\tremaining: 8.3s\n",
            "733:\tlearn: 55.2643949\ttotal: 1m 43s\tremaining: 8.16s\n",
            "734:\tlearn: 55.2104012\ttotal: 1m 43s\tremaining: 8.02s\n",
            "735:\tlearn: 55.1680201\ttotal: 1m 43s\tremaining: 7.88s\n",
            "736:\tlearn: 55.1196615\ttotal: 1m 43s\tremaining: 7.73s\n",
            "737:\tlearn: 55.0890845\ttotal: 1m 43s\tremaining: 7.59s\n",
            "738:\tlearn: 55.0709156\ttotal: 1m 43s\tremaining: 7.45s\n",
            "739:\tlearn: 55.0418371\ttotal: 1m 44s\tremaining: 7.31s\n",
            "740:\tlearn: 55.0092318\ttotal: 1m 44s\tremaining: 7.17s\n",
            "741:\tlearn: 54.9639214\ttotal: 1m 44s\tremaining: 7.03s\n",
            "742:\tlearn: 54.9446077\ttotal: 1m 44s\tremaining: 6.89s\n",
            "743:\tlearn: 54.8607749\ttotal: 1m 44s\tremaining: 6.75s\n",
            "744:\tlearn: 54.8252929\ttotal: 1m 44s\tremaining: 6.61s\n",
            "745:\tlearn: 54.7948859\ttotal: 1m 44s\tremaining: 6.47s\n",
            "746:\tlearn: 54.7498540\ttotal: 1m 45s\tremaining: 6.33s\n",
            "747:\tlearn: 54.7072270\ttotal: 1m 45s\tremaining: 6.19s\n",
            "748:\tlearn: 54.6451664\ttotal: 1m 45s\tremaining: 6.05s\n",
            "749:\tlearn: 54.5965624\ttotal: 1m 45s\tremaining: 5.91s\n",
            "750:\tlearn: 54.5790229\ttotal: 1m 45s\tremaining: 5.76s\n",
            "751:\tlearn: 54.5415091\ttotal: 1m 45s\tremaining: 5.63s\n",
            "752:\tlearn: 54.5230983\ttotal: 1m 45s\tremaining: 5.49s\n",
            "753:\tlearn: 54.4763546\ttotal: 1m 46s\tremaining: 5.34s\n",
            "754:\tlearn: 54.3826679\ttotal: 1m 46s\tremaining: 5.2s\n",
            "755:\tlearn: 54.2979825\ttotal: 1m 46s\tremaining: 5.06s\n",
            "756:\tlearn: 54.2510973\ttotal: 1m 46s\tremaining: 4.92s\n",
            "757:\tlearn: 54.1360433\ttotal: 1m 46s\tremaining: 4.78s\n",
            "758:\tlearn: 54.0976123\ttotal: 1m 46s\tremaining: 4.64s\n",
            "759:\tlearn: 54.0590655\ttotal: 1m 46s\tremaining: 4.5s\n",
            "760:\tlearn: 54.0168278\ttotal: 1m 47s\tremaining: 4.36s\n",
            "761:\tlearn: 53.9712913\ttotal: 1m 47s\tremaining: 4.22s\n",
            "762:\tlearn: 53.9704534\ttotal: 1m 47s\tremaining: 4.08s\n",
            "763:\tlearn: 53.9562095\ttotal: 1m 47s\tremaining: 3.94s\n",
            "764:\tlearn: 53.9285823\ttotal: 1m 47s\tremaining: 3.8s\n",
            "765:\tlearn: 53.8945865\ttotal: 1m 47s\tremaining: 3.65s\n",
            "766:\tlearn: 53.8476251\ttotal: 1m 47s\tremaining: 3.51s\n",
            "767:\tlearn: 53.7727747\ttotal: 1m 47s\tremaining: 3.37s\n",
            "768:\tlearn: 53.7428036\ttotal: 1m 48s\tremaining: 3.23s\n",
            "769:\tlearn: 53.6516019\ttotal: 1m 48s\tremaining: 3.09s\n",
            "770:\tlearn: 53.6372747\ttotal: 1m 48s\tremaining: 2.95s\n",
            "771:\tlearn: 53.6065705\ttotal: 1m 48s\tremaining: 2.81s\n",
            "772:\tlearn: 53.6053941\ttotal: 1m 48s\tremaining: 2.67s\n",
            "773:\tlearn: 53.5056563\ttotal: 1m 48s\tremaining: 2.53s\n",
            "774:\tlearn: 53.4724680\ttotal: 1m 48s\tremaining: 2.39s\n",
            "775:\tlearn: 53.3382876\ttotal: 1m 49s\tremaining: 2.25s\n",
            "776:\tlearn: 53.2998527\ttotal: 1m 49s\tremaining: 2.11s\n",
            "777:\tlearn: 53.2787192\ttotal: 1m 49s\tremaining: 1.97s\n",
            "778:\tlearn: 53.2242874\ttotal: 1m 49s\tremaining: 1.83s\n",
            "779:\tlearn: 53.1719417\ttotal: 1m 49s\tremaining: 1.69s\n",
            "780:\tlearn: 53.1060424\ttotal: 1m 49s\tremaining: 1.54s\n",
            "781:\tlearn: 53.0742274\ttotal: 1m 49s\tremaining: 1.41s\n",
            "782:\tlearn: 53.0624505\ttotal: 1m 50s\tremaining: 1.26s\n",
            "783:\tlearn: 52.9911780\ttotal: 1m 50s\tremaining: 1.12s\n",
            "784:\tlearn: 52.9752041\ttotal: 1m 50s\tremaining: 984ms\n",
            "785:\tlearn: 52.9645784\ttotal: 1m 50s\tremaining: 843ms\n",
            "786:\tlearn: 52.8950278\ttotal: 1m 50s\tremaining: 703ms\n",
            "787:\tlearn: 52.8124991\ttotal: 1m 50s\tremaining: 562ms\n",
            "788:\tlearn: 52.7877428\ttotal: 1m 50s\tremaining: 422ms\n",
            "789:\tlearn: 52.7655689\ttotal: 1m 51s\tremaining: 281ms\n",
            "790:\tlearn: 52.7245685\ttotal: 1m 51s\tremaining: 141ms\n",
            "791:\tlearn: 52.6426412\ttotal: 1m 51s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 00:47:49,969]\u001b[0m A new study created in memory with name: no-name-34d199d9-2296-4ac7-aab4-4affaab2fe81\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 00:49:50,662]\u001b[0m Trial 0 finished with value: 55.43807050501562 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 55.43807050501562.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:51:58,839]\u001b[0m Trial 1 finished with value: 46.94340540096268 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 46.94340540096268.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 00:54:03,178]\u001b[0m Trial 2 finished with value: 46.6410394064305 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 2 with value: 46.6410394064305.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:09:49,764]\u001b[0m Trial 3 finished with value: 71.68530020741635 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 2 with value: 46.6410394064305.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:11:56,972]\u001b[0m Trial 4 finished with value: 55.37986954123148 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 2 with value: 46.6410394064305.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:28:07,671]\u001b[0m Trial 5 finished with value: 69.1836400384031 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 2 with value: 46.6410394064305.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:31:20,332]\u001b[0m Trial 6 finished with value: 46.56191676648264 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:34:22,077]\u001b[0m Trial 7 finished with value: 49.97255745046673 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:35:58,206]\u001b[0m Trial 8 finished with value: 68.5849081431324 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:56:01,550]\u001b[0m Trial 9 finished with value: 55.86651997949391 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:57:04,850]\u001b[0m Trial 10 finished with value: 83.54449536524108 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 01:58:40,351]\u001b[0m Trial 11 finished with value: 57.436170968311984 and parameters: {'iterations': 629, 'learning_rate': 0.2764615591885931, 'depth': 6, 'min_data_in_leaf': 6, 'reg_lambda': 98.40975370477312, 'subsample': 0.6137915450890752, 'random_strength': 13.607559365869914, 'od_wait': 12, 'leaf_estimation_iterations': 13, 'bagging_temperature': 99.66177975539146, 'colsample_bylevel': 0.6037888285524304}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:02:19,647]\u001b[0m Trial 12 finished with value: 49.018176081587626 and parameters: {'iterations': 312, 'learning_rate': 0.3353168481850217, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 83.3833616301794, 'subsample': 0.6018370841486229, 'random_strength': 19.02310110968608, 'od_wait': 11, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.2397968272557836, 'colsample_bylevel': 0.6798103370630707}. Best is trial 6 with value: 46.56191676648264.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:04:46,095]\u001b[0m Trial 13 finished with value: 46.163782205728744 and parameters: {'iterations': 768, 'learning_rate': 0.39733087730033134, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 51.709612487085614, 'subsample': 0.8428965893650274, 'random_strength': 68.23293339853619, 'od_wait': 31, 'leaf_estimation_iterations': 10, 'bagging_temperature': 13.355307798299567, 'colsample_bylevel': 0.7317116575399958}. Best is trial 13 with value: 46.163782205728744.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:06:49,189]\u001b[0m Trial 14 finished with value: 63.39437740801802 and parameters: {'iterations': 741, 'learning_rate': 0.12115442835598422, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 51.397128917456506, 'subsample': 0.879970061817357, 'random_strength': 74.51101113965332, 'od_wait': 94, 'leaf_estimation_iterations': 10, 'bagging_temperature': 39.38019209537576, 'colsample_bylevel': 0.5241786552111687}. Best is trial 13 with value: 46.163782205728744.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:08:08,780]\u001b[0m Trial 15 finished with value: 64.41581750617603 and parameters: {'iterations': 815, 'learning_rate': 0.32308158949960997, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 49.37245683486181, 'subsample': 0.9930815447318664, 'random_strength': 66.99328267466255, 'od_wait': 41, 'leaf_estimation_iterations': 13, 'bagging_temperature': 14.566066853374414, 'colsample_bylevel': 0.061943926372203806}. Best is trial 13 with value: 46.163782205728744.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:09:29,684]\u001b[0m Trial 16 finished with value: 67.755824227023 and parameters: {'iterations': 592, 'learning_rate': 0.2407235903215652, 'depth': 5, 'min_data_in_leaf': 8, 'reg_lambda': 62.839769611979925, 'subsample': 0.8595044436039624, 'random_strength': 98.23074027293896, 'od_wait': 31, 'leaf_estimation_iterations': 8, 'bagging_temperature': 20.16763561176596, 'colsample_bylevel': 0.7715955209093116}. Best is trial 13 with value: 46.163782205728744.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:12:08,220]\u001b[0m Trial 17 finished with value: 44.239773522743256 and parameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}. Best is trial 17 with value: 44.239773522743256.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:19:15,919]\u001b[0m Trial 18 finished with value: 51.7384520797867 and parameters: {'iterations': 764, 'learning_rate': 0.4270062418190881, 'depth': 12, 'min_data_in_leaf': 13, 'reg_lambda': 40.65971705190806, 'subsample': 0.6934883378818296, 'random_strength': 86.15280749591527, 'od_wait': 64, 'leaf_estimation_iterations': 15, 'bagging_temperature': 56.02552081166322, 'colsample_bylevel': 0.22089840378700903}. Best is trial 17 with value: 44.239773522743256.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:21:35,650]\u001b[0m Trial 19 finished with value: 45.86744070140521 and parameters: {'iterations': 673, 'learning_rate': 0.3989455666851299, 'depth': 8, 'min_data_in_leaf': 20, 'reg_lambda': 42.564868866728915, 'subsample': 0.729625123932983, 'random_strength': 84.99456385371188, 'od_wait': 25, 'leaf_estimation_iterations': 17, 'bagging_temperature': 9.35910112420739, 'colsample_bylevel': 0.49862570897953884}. Best is trial 17 with value: 44.239773522743256.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 2: 44.239773522743256\n",
            "Best hyperparameters for fold 2: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "0:\tlearn: 207.3189292\ttotal: 170ms\tremaining: 2m 14s\n",
            "1:\tlearn: 206.0674098\ttotal: 293ms\tremaining: 1m 55s\n",
            "2:\tlearn: 205.6772481\ttotal: 479ms\tremaining: 2m 6s\n",
            "3:\tlearn: 205.4774880\ttotal: 558ms\tremaining: 1m 49s\n",
            "4:\tlearn: 204.5742147\ttotal: 739ms\tremaining: 1m 56s\n",
            "5:\tlearn: 204.3597055\ttotal: 888ms\tremaining: 1m 56s\n",
            "6:\tlearn: 203.5728966\ttotal: 1.03s\tremaining: 1m 55s\n",
            "7:\tlearn: 203.5728702\ttotal: 1.1s\tremaining: 1m 48s\n",
            "8:\tlearn: 203.5404589\ttotal: 1.21s\tremaining: 1m 45s\n",
            "9:\tlearn: 202.6561341\ttotal: 1.37s\tremaining: 1m 47s\n",
            "10:\tlearn: 202.6514240\ttotal: 1.43s\tremaining: 1m 41s\n",
            "11:\tlearn: 202.4720807\ttotal: 1.6s\tremaining: 1m 44s\n",
            "12:\tlearn: 201.8706778\ttotal: 1.77s\tremaining: 1m 46s\n",
            "13:\tlearn: 201.2110156\ttotal: 1.88s\tremaining: 1m 44s\n",
            "14:\tlearn: 201.1152666\ttotal: 2.06s\tremaining: 1m 46s\n",
            "15:\tlearn: 201.1141910\ttotal: 2.15s\tremaining: 1m 44s\n",
            "16:\tlearn: 201.0668308\ttotal: 2.27s\tremaining: 1m 43s\n",
            "17:\tlearn: 200.5607246\ttotal: 2.43s\tremaining: 1m 44s\n",
            "18:\tlearn: 199.7489055\ttotal: 2.59s\tremaining: 1m 45s\n",
            "19:\tlearn: 199.6448655\ttotal: 2.65s\tremaining: 1m 42s\n",
            "20:\tlearn: 199.4965538\ttotal: 2.8s\tremaining: 1m 42s\n",
            "21:\tlearn: 199.4630398\ttotal: 2.9s\tremaining: 1m 41s\n",
            "22:\tlearn: 199.2697364\ttotal: 3.06s\tremaining: 1m 42s\n",
            "23:\tlearn: 199.2697364\ttotal: 3.11s\tremaining: 1m 39s\n",
            "24:\tlearn: 198.8305705\ttotal: 3.26s\tremaining: 1m 40s\n",
            "25:\tlearn: 198.5897160\ttotal: 3.44s\tremaining: 1m 41s\n",
            "26:\tlearn: 198.5143668\ttotal: 3.61s\tremaining: 1m 42s\n",
            "27:\tlearn: 198.5143668\ttotal: 3.65s\tremaining: 1m 39s\n",
            "28:\tlearn: 198.4399385\ttotal: 3.78s\tremaining: 1m 39s\n",
            "29:\tlearn: 198.2047314\ttotal: 3.94s\tremaining: 1m 40s\n",
            "30:\tlearn: 198.0866774\ttotal: 4.05s\tremaining: 1m 39s\n",
            "31:\tlearn: 198.0739424\ttotal: 4.11s\tremaining: 1m 37s\n",
            "32:\tlearn: 197.9247174\ttotal: 4.29s\tremaining: 1m 38s\n",
            "33:\tlearn: 197.9247174\ttotal: 4.34s\tremaining: 1m 36s\n",
            "34:\tlearn: 196.9729549\ttotal: 4.48s\tremaining: 1m 36s\n",
            "35:\tlearn: 196.0524651\ttotal: 4.55s\tremaining: 1m 35s\n",
            "36:\tlearn: 194.9066093\ttotal: 4.69s\tremaining: 1m 35s\n",
            "37:\tlearn: 193.0964621\ttotal: 4.86s\tremaining: 1m 36s\n",
            "38:\tlearn: 191.2790210\ttotal: 5.02s\tremaining: 1m 36s\n",
            "39:\tlearn: 190.0543386\ttotal: 5.19s\tremaining: 1m 37s\n",
            "40:\tlearn: 187.4116179\ttotal: 5.34s\tremaining: 1m 37s\n",
            "41:\tlearn: 186.4954937\ttotal: 5.48s\tremaining: 1m 37s\n",
            "42:\tlearn: 185.3311921\ttotal: 5.66s\tremaining: 1m 38s\n",
            "43:\tlearn: 184.8825898\ttotal: 5.83s\tremaining: 1m 39s\n",
            "44:\tlearn: 184.1993113\ttotal: 5.98s\tremaining: 1m 39s\n",
            "45:\tlearn: 183.5924554\ttotal: 6.12s\tremaining: 1m 39s\n",
            "46:\tlearn: 183.0416070\ttotal: 6.29s\tremaining: 1m 39s\n",
            "47:\tlearn: 181.4337417\ttotal: 6.44s\tremaining: 1m 39s\n",
            "48:\tlearn: 181.0443297\ttotal: 6.59s\tremaining: 1m 39s\n",
            "49:\tlearn: 180.6837432\ttotal: 6.76s\tremaining: 1m 40s\n",
            "50:\tlearn: 179.1259059\ttotal: 6.9s\tremaining: 1m 40s\n",
            "51:\tlearn: 178.7081892\ttotal: 7.06s\tremaining: 1m 40s\n",
            "52:\tlearn: 178.3231941\ttotal: 7.22s\tremaining: 1m 40s\n",
            "53:\tlearn: 175.5725203\ttotal: 7.37s\tremaining: 1m 40s\n",
            "54:\tlearn: 175.3052818\ttotal: 7.52s\tremaining: 1m 40s\n",
            "55:\tlearn: 173.5898817\ttotal: 7.66s\tremaining: 1m 40s\n",
            "56:\tlearn: 173.0771917\ttotal: 7.8s\tremaining: 1m 40s\n",
            "57:\tlearn: 172.7771094\ttotal: 7.97s\tremaining: 1m 40s\n",
            "58:\tlearn: 170.7573477\ttotal: 8.11s\tremaining: 1m 40s\n",
            "59:\tlearn: 170.3393400\ttotal: 8.26s\tremaining: 1m 40s\n",
            "60:\tlearn: 169.7098525\ttotal: 8.4s\tremaining: 1m 40s\n",
            "61:\tlearn: 169.5801832\ttotal: 8.57s\tremaining: 1m 40s\n",
            "62:\tlearn: 168.8364086\ttotal: 8.72s\tremaining: 1m 40s\n",
            "63:\tlearn: 167.3478433\ttotal: 8.86s\tremaining: 1m 40s\n",
            "64:\tlearn: 167.0485031\ttotal: 9.02s\tremaining: 1m 40s\n",
            "65:\tlearn: 166.0908931\ttotal: 9.16s\tremaining: 1m 40s\n",
            "66:\tlearn: 164.8926295\ttotal: 9.31s\tremaining: 1m 40s\n",
            "67:\tlearn: 163.9220266\ttotal: 9.45s\tremaining: 1m 40s\n",
            "68:\tlearn: 163.5746354\ttotal: 9.6s\tremaining: 1m 40s\n",
            "69:\tlearn: 162.8425386\ttotal: 9.76s\tremaining: 1m 40s\n",
            "70:\tlearn: 162.6698663\ttotal: 9.92s\tremaining: 1m 40s\n",
            "71:\tlearn: 161.4314943\ttotal: 10.1s\tremaining: 1m 40s\n",
            "72:\tlearn: 161.1543149\ttotal: 10.2s\tremaining: 1m 40s\n",
            "73:\tlearn: 160.3692914\ttotal: 10.4s\tremaining: 1m 40s\n",
            "74:\tlearn: 159.1687554\ttotal: 10.5s\tremaining: 1m 40s\n",
            "75:\tlearn: 158.0129164\ttotal: 10.7s\tremaining: 1m 40s\n",
            "76:\tlearn: 157.1621244\ttotal: 10.8s\tremaining: 1m 40s\n",
            "77:\tlearn: 156.1621105\ttotal: 11s\tremaining: 1m 40s\n",
            "78:\tlearn: 155.5190055\ttotal: 11.1s\tremaining: 1m 40s\n",
            "79:\tlearn: 154.8394672\ttotal: 11.3s\tremaining: 1m 40s\n",
            "80:\tlearn: 154.6914146\ttotal: 11.4s\tremaining: 1m 40s\n",
            "81:\tlearn: 154.2493012\ttotal: 11.6s\tremaining: 1m 40s\n",
            "82:\tlearn: 152.1032323\ttotal: 11.7s\tremaining: 1m 40s\n",
            "83:\tlearn: 151.2019445\ttotal: 11.9s\tremaining: 1m 40s\n",
            "84:\tlearn: 150.3560170\ttotal: 12s\tremaining: 1m 39s\n",
            "85:\tlearn: 149.2767084\ttotal: 12.2s\tremaining: 1m 39s\n",
            "86:\tlearn: 149.0447459\ttotal: 12.3s\tremaining: 1m 40s\n",
            "87:\tlearn: 148.4729548\ttotal: 12.5s\tremaining: 1m 39s\n",
            "88:\tlearn: 148.1559388\ttotal: 12.6s\tremaining: 1m 39s\n",
            "89:\tlearn: 147.9929772\ttotal: 12.8s\tremaining: 1m 39s\n",
            "90:\tlearn: 147.7046693\ttotal: 12.9s\tremaining: 1m 39s\n",
            "91:\tlearn: 147.1562018\ttotal: 13.1s\tremaining: 1m 39s\n",
            "92:\tlearn: 146.8782324\ttotal: 13.2s\tremaining: 1m 39s\n",
            "93:\tlearn: 145.9687810\ttotal: 13.4s\tremaining: 1m 39s\n",
            "94:\tlearn: 145.1540416\ttotal: 13.5s\tremaining: 1m 39s\n",
            "95:\tlearn: 144.9911316\ttotal: 13.7s\tremaining: 1m 39s\n",
            "96:\tlearn: 144.4096673\ttotal: 13.8s\tremaining: 1m 39s\n",
            "97:\tlearn: 143.8359914\ttotal: 13.9s\tremaining: 1m 38s\n",
            "98:\tlearn: 143.3878256\ttotal: 14.1s\tremaining: 1m 38s\n",
            "99:\tlearn: 141.8598332\ttotal: 14.2s\tremaining: 1m 38s\n",
            "100:\tlearn: 141.3499718\ttotal: 14.4s\tremaining: 1m 38s\n",
            "101:\tlearn: 140.9399011\ttotal: 14.5s\tremaining: 1m 38s\n",
            "102:\tlearn: 140.3626285\ttotal: 14.7s\tremaining: 1m 38s\n",
            "103:\tlearn: 139.7188246\ttotal: 14.8s\tremaining: 1m 37s\n",
            "104:\tlearn: 138.8029567\ttotal: 14.9s\tremaining: 1m 37s\n",
            "105:\tlearn: 137.8949568\ttotal: 15.1s\tremaining: 1m 37s\n",
            "106:\tlearn: 137.6331879\ttotal: 15.2s\tremaining: 1m 37s\n",
            "107:\tlearn: 137.2656788\ttotal: 15.4s\tremaining: 1m 37s\n",
            "108:\tlearn: 136.7935215\ttotal: 15.5s\tremaining: 1m 37s\n",
            "109:\tlearn: 136.4413677\ttotal: 15.7s\tremaining: 1m 37s\n",
            "110:\tlearn: 135.8561884\ttotal: 15.8s\tremaining: 1m 37s\n",
            "111:\tlearn: 135.5011915\ttotal: 16s\tremaining: 1m 37s\n",
            "112:\tlearn: 135.1649080\ttotal: 16.1s\tremaining: 1m 36s\n",
            "113:\tlearn: 134.9943756\ttotal: 16.3s\tremaining: 1m 36s\n",
            "114:\tlearn: 134.7696851\ttotal: 16.4s\tremaining: 1m 36s\n",
            "115:\tlearn: 134.3082649\ttotal: 16.6s\tremaining: 1m 36s\n",
            "116:\tlearn: 134.1072828\ttotal: 16.7s\tremaining: 1m 36s\n",
            "117:\tlearn: 133.2223736\ttotal: 16.9s\tremaining: 1m 36s\n",
            "118:\tlearn: 133.1445360\ttotal: 17s\tremaining: 1m 36s\n",
            "119:\tlearn: 132.9822970\ttotal: 17.2s\tremaining: 1m 36s\n",
            "120:\tlearn: 132.5903188\ttotal: 17.4s\tremaining: 1m 36s\n",
            "121:\tlearn: 131.4112228\ttotal: 17.5s\tremaining: 1m 36s\n",
            "122:\tlearn: 131.2322269\ttotal: 17.6s\tremaining: 1m 35s\n",
            "123:\tlearn: 130.3775810\ttotal: 17.8s\tremaining: 1m 35s\n",
            "124:\tlearn: 129.7496537\ttotal: 17.9s\tremaining: 1m 35s\n",
            "125:\tlearn: 129.1766605\ttotal: 18s\tremaining: 1m 35s\n",
            "126:\tlearn: 128.9965344\ttotal: 18.2s\tremaining: 1m 35s\n",
            "127:\tlearn: 127.8121777\ttotal: 18.3s\tremaining: 1m 35s\n",
            "128:\tlearn: 127.3913962\ttotal: 18.5s\tremaining: 1m 35s\n",
            "129:\tlearn: 126.9176991\ttotal: 18.6s\tremaining: 1m 34s\n",
            "130:\tlearn: 126.4399393\ttotal: 18.8s\tremaining: 1m 34s\n",
            "131:\tlearn: 126.3295270\ttotal: 18.9s\tremaining: 1m 34s\n",
            "132:\tlearn: 125.8252241\ttotal: 19.1s\tremaining: 1m 34s\n",
            "133:\tlearn: 125.2911464\ttotal: 19.2s\tremaining: 1m 34s\n",
            "134:\tlearn: 125.1911507\ttotal: 19.4s\tremaining: 1m 34s\n",
            "135:\tlearn: 125.0658681\ttotal: 19.5s\tremaining: 1m 34s\n",
            "136:\tlearn: 124.4600676\ttotal: 19.7s\tremaining: 1m 34s\n",
            "137:\tlearn: 124.2496493\ttotal: 19.8s\tremaining: 1m 33s\n",
            "138:\tlearn: 124.1073951\ttotal: 20s\tremaining: 1m 33s\n",
            "139:\tlearn: 123.4270305\ttotal: 20.1s\tremaining: 1m 33s\n",
            "140:\tlearn: 123.3018297\ttotal: 20.3s\tremaining: 1m 33s\n",
            "141:\tlearn: 123.0550246\ttotal: 20.4s\tremaining: 1m 33s\n",
            "142:\tlearn: 122.7832903\ttotal: 20.6s\tremaining: 1m 33s\n",
            "143:\tlearn: 122.6738424\ttotal: 20.8s\tremaining: 1m 33s\n",
            "144:\tlearn: 122.4610573\ttotal: 20.9s\tremaining: 1m 33s\n",
            "145:\tlearn: 122.3268046\ttotal: 21.1s\tremaining: 1m 33s\n",
            "146:\tlearn: 121.9011051\ttotal: 21.2s\tremaining: 1m 33s\n",
            "147:\tlearn: 121.3676734\ttotal: 21.3s\tremaining: 1m 32s\n",
            "148:\tlearn: 121.0600502\ttotal: 21.5s\tremaining: 1m 32s\n",
            "149:\tlearn: 120.7287593\ttotal: 21.6s\tremaining: 1m 32s\n",
            "150:\tlearn: 120.6229109\ttotal: 21.8s\tremaining: 1m 32s\n",
            "151:\tlearn: 120.2036248\ttotal: 21.9s\tremaining: 1m 32s\n",
            "152:\tlearn: 119.5076807\ttotal: 22.1s\tremaining: 1m 32s\n",
            "153:\tlearn: 118.6088388\ttotal: 22.2s\tremaining: 1m 32s\n",
            "154:\tlearn: 118.4954111\ttotal: 22.4s\tremaining: 1m 32s\n",
            "155:\tlearn: 118.2511144\ttotal: 22.5s\tremaining: 1m 31s\n",
            "156:\tlearn: 118.0776658\ttotal: 22.7s\tremaining: 1m 31s\n",
            "157:\tlearn: 117.7881618\ttotal: 22.9s\tremaining: 1m 31s\n",
            "158:\tlearn: 117.5000907\ttotal: 23s\tremaining: 1m 31s\n",
            "159:\tlearn: 117.3646745\ttotal: 23.2s\tremaining: 1m 31s\n",
            "160:\tlearn: 117.1682155\ttotal: 23.3s\tremaining: 1m 31s\n",
            "161:\tlearn: 116.9450584\ttotal: 23.5s\tremaining: 1m 31s\n",
            "162:\tlearn: 116.6877940\ttotal: 23.6s\tremaining: 1m 31s\n",
            "163:\tlearn: 116.6862129\ttotal: 23.8s\tremaining: 1m 30s\n",
            "164:\tlearn: 116.5319202\ttotal: 23.9s\tremaining: 1m 30s\n",
            "165:\tlearn: 116.3414486\ttotal: 24.1s\tremaining: 1m 30s\n",
            "166:\tlearn: 116.2072167\ttotal: 24.2s\tremaining: 1m 30s\n",
            "167:\tlearn: 116.0728173\ttotal: 24.3s\tremaining: 1m 30s\n",
            "168:\tlearn: 115.9923454\ttotal: 24.5s\tremaining: 1m 30s\n",
            "169:\tlearn: 115.9800620\ttotal: 24.7s\tremaining: 1m 30s\n",
            "170:\tlearn: 115.8818624\ttotal: 24.8s\tremaining: 1m 30s\n",
            "171:\tlearn: 114.9830658\ttotal: 25s\tremaining: 1m 30s\n",
            "172:\tlearn: 114.7435424\ttotal: 25.1s\tremaining: 1m 29s\n",
            "173:\tlearn: 113.3602485\ttotal: 25.3s\tremaining: 1m 29s\n",
            "174:\tlearn: 113.1233917\ttotal: 25.4s\tremaining: 1m 29s\n",
            "175:\tlearn: 112.9928173\ttotal: 25.6s\tremaining: 1m 29s\n",
            "176:\tlearn: 112.7230750\ttotal: 25.7s\tremaining: 1m 29s\n",
            "177:\tlearn: 111.9089072\ttotal: 25.8s\tremaining: 1m 29s\n",
            "178:\tlearn: 111.8058720\ttotal: 26s\tremaining: 1m 29s\n",
            "179:\tlearn: 111.6667102\ttotal: 26.2s\tremaining: 1m 29s\n",
            "180:\tlearn: 111.5487201\ttotal: 26.3s\tremaining: 1m 28s\n",
            "181:\tlearn: 111.0949453\ttotal: 26.5s\tremaining: 1m 28s\n",
            "182:\tlearn: 111.0172006\ttotal: 26.6s\tremaining: 1m 28s\n",
            "183:\tlearn: 110.7278591\ttotal: 26.8s\tremaining: 1m 28s\n",
            "184:\tlearn: 110.7156872\ttotal: 26.9s\tremaining: 1m 28s\n",
            "185:\tlearn: 110.6089028\ttotal: 27.1s\tremaining: 1m 28s\n",
            "186:\tlearn: 110.1717436\ttotal: 27.3s\tremaining: 1m 28s\n",
            "187:\tlearn: 109.8709657\ttotal: 27.4s\tremaining: 1m 28s\n",
            "188:\tlearn: 109.6831459\ttotal: 27.5s\tremaining: 1m 27s\n",
            "189:\tlearn: 109.3083435\ttotal: 27.7s\tremaining: 1m 27s\n",
            "190:\tlearn: 109.2127890\ttotal: 27.8s\tremaining: 1m 27s\n",
            "191:\tlearn: 108.5495821\ttotal: 28s\tremaining: 1m 27s\n",
            "192:\tlearn: 107.9401197\ttotal: 28.1s\tremaining: 1m 27s\n",
            "193:\tlearn: 107.7260251\ttotal: 28.2s\tremaining: 1m 27s\n",
            "194:\tlearn: 107.3060951\ttotal: 28.4s\tremaining: 1m 26s\n",
            "195:\tlearn: 106.7233220\ttotal: 28.5s\tremaining: 1m 26s\n",
            "196:\tlearn: 106.5105435\ttotal: 28.7s\tremaining: 1m 26s\n",
            "197:\tlearn: 106.3400247\ttotal: 28.8s\tremaining: 1m 26s\n",
            "198:\tlearn: 105.1791843\ttotal: 29s\tremaining: 1m 26s\n",
            "199:\tlearn: 105.0158709\ttotal: 29.1s\tremaining: 1m 26s\n",
            "200:\tlearn: 104.7157046\ttotal: 29.3s\tremaining: 1m 26s\n",
            "201:\tlearn: 104.5959886\ttotal: 29.4s\tremaining: 1m 25s\n",
            "202:\tlearn: 104.5242989\ttotal: 29.6s\tremaining: 1m 25s\n",
            "203:\tlearn: 104.3305191\ttotal: 29.7s\tremaining: 1m 25s\n",
            "204:\tlearn: 104.2103069\ttotal: 29.9s\tremaining: 1m 25s\n",
            "205:\tlearn: 104.1339838\ttotal: 30.1s\tremaining: 1m 25s\n",
            "206:\tlearn: 104.0517517\ttotal: 30.2s\tremaining: 1m 25s\n",
            "207:\tlearn: 103.7730602\ttotal: 30.4s\tremaining: 1m 25s\n",
            "208:\tlearn: 103.3031139\ttotal: 30.5s\tremaining: 1m 25s\n",
            "209:\tlearn: 103.0815648\ttotal: 30.6s\tremaining: 1m 24s\n",
            "210:\tlearn: 102.8151437\ttotal: 30.8s\tremaining: 1m 24s\n",
            "211:\tlearn: 102.6505102\ttotal: 31s\tremaining: 1m 24s\n",
            "212:\tlearn: 102.5950226\ttotal: 31.1s\tremaining: 1m 24s\n",
            "213:\tlearn: 102.5739522\ttotal: 31.3s\tremaining: 1m 24s\n",
            "214:\tlearn: 102.0733297\ttotal: 31.4s\tremaining: 1m 24s\n",
            "215:\tlearn: 101.9507017\ttotal: 31.6s\tremaining: 1m 24s\n",
            "216:\tlearn: 101.5749903\ttotal: 31.7s\tremaining: 1m 24s\n",
            "217:\tlearn: 101.4990375\ttotal: 31.9s\tremaining: 1m 23s\n",
            "218:\tlearn: 101.4486285\ttotal: 32s\tremaining: 1m 23s\n",
            "219:\tlearn: 101.2585277\ttotal: 32.2s\tremaining: 1m 23s\n",
            "220:\tlearn: 101.1137226\ttotal: 32.3s\tremaining: 1m 23s\n",
            "221:\tlearn: 100.8649268\ttotal: 32.5s\tremaining: 1m 23s\n",
            "222:\tlearn: 100.5411176\ttotal: 32.6s\tremaining: 1m 23s\n",
            "223:\tlearn: 100.4672248\ttotal: 32.8s\tremaining: 1m 23s\n",
            "224:\tlearn: 100.2452783\ttotal: 32.9s\tremaining: 1m 22s\n",
            "225:\tlearn: 100.0073020\ttotal: 33.1s\tremaining: 1m 22s\n",
            "226:\tlearn: 99.7956331\ttotal: 33.2s\tremaining: 1m 22s\n",
            "227:\tlearn: 99.6336281\ttotal: 33.4s\tremaining: 1m 22s\n",
            "228:\tlearn: 99.5295725\ttotal: 33.5s\tremaining: 1m 22s\n",
            "229:\tlearn: 99.3592864\ttotal: 33.7s\tremaining: 1m 22s\n",
            "230:\tlearn: 99.3544755\ttotal: 33.8s\tremaining: 1m 22s\n",
            "231:\tlearn: 99.2926387\ttotal: 34s\tremaining: 1m 22s\n",
            "232:\tlearn: 99.0528817\ttotal: 34.1s\tremaining: 1m 21s\n",
            "233:\tlearn: 98.8891519\ttotal: 34.3s\tremaining: 1m 21s\n",
            "234:\tlearn: 98.7642814\ttotal: 34.4s\tremaining: 1m 21s\n",
            "235:\tlearn: 98.6105087\ttotal: 34.6s\tremaining: 1m 21s\n",
            "236:\tlearn: 98.3824696\ttotal: 34.7s\tremaining: 1m 21s\n",
            "237:\tlearn: 98.1380445\ttotal: 34.9s\tremaining: 1m 21s\n",
            "238:\tlearn: 98.0531341\ttotal: 35s\tremaining: 1m 21s\n",
            "239:\tlearn: 98.0098502\ttotal: 35.2s\tremaining: 1m 20s\n",
            "240:\tlearn: 97.9661124\ttotal: 35.3s\tremaining: 1m 20s\n",
            "241:\tlearn: 97.8628586\ttotal: 35.5s\tremaining: 1m 20s\n",
            "242:\tlearn: 97.8580105\ttotal: 35.7s\tremaining: 1m 20s\n",
            "243:\tlearn: 97.5167563\ttotal: 35.8s\tremaining: 1m 20s\n",
            "244:\tlearn: 97.5123194\ttotal: 36s\tremaining: 1m 20s\n",
            "245:\tlearn: 97.5117912\ttotal: 36.1s\tremaining: 1m 20s\n",
            "246:\tlearn: 97.4685390\ttotal: 36.3s\tremaining: 1m 20s\n",
            "247:\tlearn: 97.0490440\ttotal: 36.4s\tremaining: 1m 19s\n",
            "248:\tlearn: 96.9881033\ttotal: 36.6s\tremaining: 1m 19s\n",
            "249:\tlearn: 96.8288468\ttotal: 36.7s\tremaining: 1m 19s\n",
            "250:\tlearn: 96.7669031\ttotal: 36.9s\tremaining: 1m 19s\n",
            "251:\tlearn: 96.3881407\ttotal: 37.1s\tremaining: 1m 19s\n",
            "252:\tlearn: 96.0364950\ttotal: 37.2s\tremaining: 1m 19s\n",
            "253:\tlearn: 95.9434419\ttotal: 37.4s\tremaining: 1m 19s\n",
            "254:\tlearn: 95.7304462\ttotal: 37.5s\tremaining: 1m 18s\n",
            "255:\tlearn: 95.6211994\ttotal: 37.6s\tremaining: 1m 18s\n",
            "256:\tlearn: 95.4792752\ttotal: 37.8s\tremaining: 1m 18s\n",
            "257:\tlearn: 95.3226175\ttotal: 37.9s\tremaining: 1m 18s\n",
            "258:\tlearn: 95.2596977\ttotal: 38.1s\tremaining: 1m 18s\n",
            "259:\tlearn: 95.0825837\ttotal: 38.2s\tremaining: 1m 18s\n",
            "260:\tlearn: 94.9892201\ttotal: 38.4s\tremaining: 1m 18s\n",
            "261:\tlearn: 94.7993220\ttotal: 38.6s\tremaining: 1m 18s\n",
            "262:\tlearn: 94.6948507\ttotal: 38.7s\tremaining: 1m 17s\n",
            "263:\tlearn: 94.4275976\ttotal: 38.9s\tremaining: 1m 17s\n",
            "264:\tlearn: 94.0115750\ttotal: 39s\tremaining: 1m 17s\n",
            "265:\tlearn: 93.4523899\ttotal: 39.2s\tremaining: 1m 17s\n",
            "266:\tlearn: 93.2487203\ttotal: 39.3s\tremaining: 1m 17s\n",
            "267:\tlearn: 93.0207667\ttotal: 39.5s\tremaining: 1m 17s\n",
            "268:\tlearn: 92.9283092\ttotal: 39.6s\tremaining: 1m 17s\n",
            "269:\tlearn: 92.8274150\ttotal: 39.8s\tremaining: 1m 16s\n",
            "270:\tlearn: 92.5070951\ttotal: 39.9s\tremaining: 1m 16s\n",
            "271:\tlearn: 92.4053594\ttotal: 40.1s\tremaining: 1m 16s\n",
            "272:\tlearn: 92.2106283\ttotal: 40.3s\tremaining: 1m 16s\n",
            "273:\tlearn: 92.0270660\ttotal: 40.4s\tremaining: 1m 16s\n",
            "274:\tlearn: 91.9411537\ttotal: 40.6s\tremaining: 1m 16s\n",
            "275:\tlearn: 91.8027362\ttotal: 40.7s\tremaining: 1m 16s\n",
            "276:\tlearn: 91.5476815\ttotal: 40.8s\tremaining: 1m 15s\n",
            "277:\tlearn: 91.4645165\ttotal: 41s\tremaining: 1m 15s\n",
            "278:\tlearn: 91.3990574\ttotal: 41.2s\tremaining: 1m 15s\n",
            "279:\tlearn: 91.3486359\ttotal: 41.3s\tremaining: 1m 15s\n",
            "280:\tlearn: 91.1969389\ttotal: 41.5s\tremaining: 1m 15s\n",
            "281:\tlearn: 91.1629909\ttotal: 41.6s\tremaining: 1m 15s\n",
            "282:\tlearn: 91.1005790\ttotal: 41.8s\tremaining: 1m 15s\n",
            "283:\tlearn: 90.7584979\ttotal: 41.9s\tremaining: 1m 14s\n",
            "284:\tlearn: 90.5140819\ttotal: 42.1s\tremaining: 1m 14s\n",
            "285:\tlearn: 90.2980837\ttotal: 42.2s\tremaining: 1m 14s\n",
            "286:\tlearn: 90.1826513\ttotal: 42.4s\tremaining: 1m 14s\n",
            "287:\tlearn: 90.0904984\ttotal: 42.5s\tremaining: 1m 14s\n",
            "288:\tlearn: 89.8865480\ttotal: 42.7s\tremaining: 1m 14s\n",
            "289:\tlearn: 89.6608165\ttotal: 42.8s\tremaining: 1m 14s\n",
            "290:\tlearn: 89.2760127\ttotal: 43s\tremaining: 1m 13s\n",
            "291:\tlearn: 89.1867341\ttotal: 43.1s\tremaining: 1m 13s\n",
            "292:\tlearn: 89.0933744\ttotal: 43.3s\tremaining: 1m 13s\n",
            "293:\tlearn: 88.9033050\ttotal: 43.4s\tremaining: 1m 13s\n",
            "294:\tlearn: 88.5213281\ttotal: 43.6s\tremaining: 1m 13s\n",
            "295:\tlearn: 88.4146214\ttotal: 43.7s\tremaining: 1m 13s\n",
            "296:\tlearn: 88.3746024\ttotal: 43.9s\tremaining: 1m 13s\n",
            "297:\tlearn: 88.1787440\ttotal: 44s\tremaining: 1m 12s\n",
            "298:\tlearn: 88.0771716\ttotal: 44.2s\tremaining: 1m 12s\n",
            "299:\tlearn: 88.0308848\ttotal: 44.3s\tremaining: 1m 12s\n",
            "300:\tlearn: 87.9516740\ttotal: 44.5s\tremaining: 1m 12s\n",
            "301:\tlearn: 87.8637653\ttotal: 44.6s\tremaining: 1m 12s\n",
            "302:\tlearn: 87.7867297\ttotal: 44.8s\tremaining: 1m 12s\n",
            "303:\tlearn: 87.7215045\ttotal: 44.9s\tremaining: 1m 12s\n",
            "304:\tlearn: 87.5108833\ttotal: 45s\tremaining: 1m 11s\n",
            "305:\tlearn: 87.3525269\ttotal: 45.2s\tremaining: 1m 11s\n",
            "306:\tlearn: 87.3098283\ttotal: 45.4s\tremaining: 1m 11s\n",
            "307:\tlearn: 87.0593383\ttotal: 45.5s\tremaining: 1m 11s\n",
            "308:\tlearn: 86.9706309\ttotal: 45.7s\tremaining: 1m 11s\n",
            "309:\tlearn: 86.9526580\ttotal: 45.9s\tremaining: 1m 11s\n",
            "310:\tlearn: 86.9519251\ttotal: 46s\tremaining: 1m 11s\n",
            "311:\tlearn: 86.8826114\ttotal: 46.2s\tremaining: 1m 11s\n",
            "312:\tlearn: 86.7999332\ttotal: 46.3s\tremaining: 1m 10s\n",
            "313:\tlearn: 86.6064807\ttotal: 46.5s\tremaining: 1m 10s\n",
            "314:\tlearn: 86.5175543\ttotal: 46.7s\tremaining: 1m 10s\n",
            "315:\tlearn: 86.3412422\ttotal: 46.8s\tremaining: 1m 10s\n",
            "316:\tlearn: 86.1438812\ttotal: 46.9s\tremaining: 1m 10s\n",
            "317:\tlearn: 85.7901996\ttotal: 47.1s\tremaining: 1m 10s\n",
            "318:\tlearn: 85.5714340\ttotal: 47.2s\tremaining: 1m 10s\n",
            "319:\tlearn: 85.4363200\ttotal: 47.4s\tremaining: 1m 9s\n",
            "320:\tlearn: 85.3555500\ttotal: 47.5s\tremaining: 1m 9s\n",
            "321:\tlearn: 85.3143394\ttotal: 47.7s\tremaining: 1m 9s\n",
            "322:\tlearn: 85.2264483\ttotal: 47.8s\tremaining: 1m 9s\n",
            "323:\tlearn: 85.1927684\ttotal: 48s\tremaining: 1m 9s\n",
            "324:\tlearn: 85.0671457\ttotal: 48.2s\tremaining: 1m 9s\n",
            "325:\tlearn: 84.9578549\ttotal: 48.3s\tremaining: 1m 9s\n",
            "326:\tlearn: 84.7155426\ttotal: 48.5s\tremaining: 1m 8s\n",
            "327:\tlearn: 84.6207366\ttotal: 48.6s\tremaining: 1m 8s\n",
            "328:\tlearn: 84.5188891\ttotal: 48.8s\tremaining: 1m 8s\n",
            "329:\tlearn: 84.5159757\ttotal: 48.9s\tremaining: 1m 8s\n",
            "330:\tlearn: 84.4619233\ttotal: 49.1s\tremaining: 1m 8s\n",
            "331:\tlearn: 84.3792099\ttotal: 49.3s\tremaining: 1m 8s\n",
            "332:\tlearn: 84.2524557\ttotal: 49.4s\tremaining: 1m 8s\n",
            "333:\tlearn: 84.2146146\ttotal: 49.6s\tremaining: 1m 8s\n",
            "334:\tlearn: 84.1339364\ttotal: 49.7s\tremaining: 1m 7s\n",
            "335:\tlearn: 84.0788092\ttotal: 49.9s\tremaining: 1m 7s\n",
            "336:\tlearn: 84.0745579\ttotal: 50.1s\tremaining: 1m 7s\n",
            "337:\tlearn: 83.9702088\ttotal: 50.2s\tremaining: 1m 7s\n",
            "338:\tlearn: 83.9263560\ttotal: 50.4s\tremaining: 1m 7s\n",
            "339:\tlearn: 83.7288960\ttotal: 50.5s\tremaining: 1m 7s\n",
            "340:\tlearn: 83.6269711\ttotal: 50.7s\tremaining: 1m 7s\n",
            "341:\tlearn: 83.4221344\ttotal: 50.8s\tremaining: 1m 6s\n",
            "342:\tlearn: 83.3069874\ttotal: 51s\tremaining: 1m 6s\n",
            "343:\tlearn: 83.2143734\ttotal: 51.1s\tremaining: 1m 6s\n",
            "344:\tlearn: 83.1776183\ttotal: 51.3s\tremaining: 1m 6s\n",
            "345:\tlearn: 82.9506557\ttotal: 51.4s\tremaining: 1m 6s\n",
            "346:\tlearn: 82.7775972\ttotal: 51.6s\tremaining: 1m 6s\n",
            "347:\tlearn: 82.4466101\ttotal: 51.7s\tremaining: 1m 5s\n",
            "348:\tlearn: 82.3519303\ttotal: 51.9s\tremaining: 1m 5s\n",
            "349:\tlearn: 82.1898811\ttotal: 52s\tremaining: 1m 5s\n",
            "350:\tlearn: 82.1471183\ttotal: 52.2s\tremaining: 1m 5s\n",
            "351:\tlearn: 82.0743422\ttotal: 52.3s\tremaining: 1m 5s\n",
            "352:\tlearn: 82.0175059\ttotal: 52.5s\tremaining: 1m 5s\n",
            "353:\tlearn: 81.9792333\ttotal: 52.6s\tremaining: 1m 5s\n",
            "354:\tlearn: 81.8522688\ttotal: 52.8s\tremaining: 1m 4s\n",
            "355:\tlearn: 81.7732447\ttotal: 52.9s\tremaining: 1m 4s\n",
            "356:\tlearn: 81.7130812\ttotal: 53.1s\tremaining: 1m 4s\n",
            "357:\tlearn: 81.3465186\ttotal: 53.2s\tremaining: 1m 4s\n",
            "358:\tlearn: 81.2934839\ttotal: 53.4s\tremaining: 1m 4s\n",
            "359:\tlearn: 81.2178580\ttotal: 53.5s\tremaining: 1m 4s\n",
            "360:\tlearn: 81.0162552\ttotal: 53.7s\tremaining: 1m 4s\n",
            "361:\tlearn: 80.7392868\ttotal: 53.8s\tremaining: 1m 3s\n",
            "362:\tlearn: 80.3872163\ttotal: 54s\tremaining: 1m 3s\n",
            "363:\tlearn: 80.2952290\ttotal: 54.2s\tremaining: 1m 3s\n",
            "364:\tlearn: 80.2588149\ttotal: 54.3s\tremaining: 1m 3s\n",
            "365:\tlearn: 80.1966241\ttotal: 54.5s\tremaining: 1m 3s\n",
            "366:\tlearn: 80.1135251\ttotal: 54.6s\tremaining: 1m 3s\n",
            "367:\tlearn: 80.0641571\ttotal: 54.8s\tremaining: 1m 3s\n",
            "368:\tlearn: 80.0508187\ttotal: 54.9s\tremaining: 1m 2s\n",
            "369:\tlearn: 80.0115097\ttotal: 55.1s\tremaining: 1m 2s\n",
            "370:\tlearn: 79.9557946\ttotal: 55.3s\tremaining: 1m 2s\n",
            "371:\tlearn: 79.8184672\ttotal: 55.4s\tremaining: 1m 2s\n",
            "372:\tlearn: 79.7467162\ttotal: 55.6s\tremaining: 1m 2s\n",
            "373:\tlearn: 79.6563808\ttotal: 55.7s\tremaining: 1m 2s\n",
            "374:\tlearn: 79.5728333\ttotal: 55.9s\tremaining: 1m 2s\n",
            "375:\tlearn: 79.4776355\ttotal: 56s\tremaining: 1m 1s\n",
            "376:\tlearn: 79.4035352\ttotal: 56.2s\tremaining: 1m 1s\n",
            "377:\tlearn: 79.3352076\ttotal: 56.3s\tremaining: 1m 1s\n",
            "378:\tlearn: 79.2704774\ttotal: 56.4s\tremaining: 1m 1s\n",
            "379:\tlearn: 79.2241345\ttotal: 56.6s\tremaining: 1m 1s\n",
            "380:\tlearn: 78.9603651\ttotal: 56.7s\tremaining: 1m 1s\n",
            "381:\tlearn: 78.8601306\ttotal: 56.9s\tremaining: 1m 1s\n",
            "382:\tlearn: 78.6955564\ttotal: 57s\tremaining: 1m\n",
            "383:\tlearn: 78.6353136\ttotal: 57.2s\tremaining: 1m\n",
            "384:\tlearn: 78.4458232\ttotal: 57.3s\tremaining: 1m\n",
            "385:\tlearn: 78.3874763\ttotal: 57.5s\tremaining: 1m\n",
            "386:\tlearn: 78.2023802\ttotal: 57.6s\tremaining: 1m\n",
            "387:\tlearn: 78.1120164\ttotal: 57.8s\tremaining: 1m\n",
            "388:\tlearn: 77.9932295\ttotal: 58s\tremaining: 1m\n",
            "389:\tlearn: 77.9414863\ttotal: 58.1s\tremaining: 59.9s\n",
            "390:\tlearn: 77.8912874\ttotal: 58.3s\tremaining: 59.8s\n",
            "391:\tlearn: 77.8367959\ttotal: 58.4s\tremaining: 59.6s\n",
            "392:\tlearn: 77.8354547\ttotal: 58.6s\tremaining: 59.5s\n",
            "393:\tlearn: 77.8002679\ttotal: 58.8s\tremaining: 59.4s\n",
            "394:\tlearn: 77.6848658\ttotal: 58.9s\tremaining: 59.2s\n",
            "395:\tlearn: 77.6561757\ttotal: 59.1s\tremaining: 59.1s\n",
            "396:\tlearn: 77.6056265\ttotal: 59.3s\tremaining: 59s\n",
            "397:\tlearn: 77.5069173\ttotal: 59.4s\tremaining: 58.8s\n",
            "398:\tlearn: 77.4678775\ttotal: 59.5s\tremaining: 58.6s\n",
            "399:\tlearn: 77.4671632\ttotal: 59.7s\tremaining: 58.5s\n",
            "400:\tlearn: 77.4197619\ttotal: 59.9s\tremaining: 58.4s\n",
            "401:\tlearn: 77.3605058\ttotal: 1m\tremaining: 58.3s\n",
            "402:\tlearn: 77.3600602\ttotal: 1m\tremaining: 58.1s\n",
            "403:\tlearn: 77.2124063\ttotal: 1m\tremaining: 57.9s\n",
            "404:\tlearn: 77.1334974\ttotal: 1m\tremaining: 57.8s\n",
            "405:\tlearn: 76.8048902\ttotal: 1m\tremaining: 57.7s\n",
            "406:\tlearn: 76.7234469\ttotal: 1m\tremaining: 57.5s\n",
            "407:\tlearn: 76.6626875\ttotal: 1m\tremaining: 57.4s\n",
            "408:\tlearn: 76.6171096\ttotal: 1m 1s\tremaining: 57.2s\n",
            "409:\tlearn: 76.5871267\ttotal: 1m 1s\tremaining: 57s\n",
            "410:\tlearn: 76.5420419\ttotal: 1m 1s\tremaining: 56.9s\n",
            "411:\tlearn: 76.2360269\ttotal: 1m 1s\tremaining: 56.8s\n",
            "412:\tlearn: 76.0123563\ttotal: 1m 1s\tremaining: 56.6s\n",
            "413:\tlearn: 75.9656165\ttotal: 1m 1s\tremaining: 56.5s\n",
            "414:\tlearn: 75.6957323\ttotal: 1m 1s\tremaining: 56.3s\n",
            "415:\tlearn: 75.6528665\ttotal: 1m 2s\tremaining: 56.2s\n",
            "416:\tlearn: 75.5904638\ttotal: 1m 2s\tremaining: 56s\n",
            "417:\tlearn: 75.5890211\ttotal: 1m 2s\tremaining: 55.9s\n",
            "418:\tlearn: 75.4532776\ttotal: 1m 2s\tremaining: 55.7s\n",
            "419:\tlearn: 75.3570886\ttotal: 1m 2s\tremaining: 55.6s\n",
            "420:\tlearn: 75.2020403\ttotal: 1m 2s\tremaining: 55.4s\n",
            "421:\tlearn: 75.1719990\ttotal: 1m 3s\tremaining: 55.3s\n",
            "422:\tlearn: 75.0233465\ttotal: 1m 3s\tremaining: 55.2s\n",
            "423:\tlearn: 74.9768875\ttotal: 1m 3s\tremaining: 55s\n",
            "424:\tlearn: 74.9477799\ttotal: 1m 3s\tremaining: 54.9s\n",
            "425:\tlearn: 74.9190711\ttotal: 1m 3s\tremaining: 54.8s\n",
            "426:\tlearn: 74.8900805\ttotal: 1m 3s\tremaining: 54.6s\n",
            "427:\tlearn: 74.6054962\ttotal: 1m 4s\tremaining: 54.5s\n",
            "428:\tlearn: 74.5407376\ttotal: 1m 4s\tremaining: 54.3s\n",
            "429:\tlearn: 74.3768581\ttotal: 1m 4s\tremaining: 54.2s\n",
            "430:\tlearn: 74.3045957\ttotal: 1m 4s\tremaining: 54s\n",
            "431:\tlearn: 74.1841060\ttotal: 1m 4s\tremaining: 53.8s\n",
            "432:\tlearn: 74.1286454\ttotal: 1m 4s\tremaining: 53.7s\n",
            "433:\tlearn: 74.0154078\ttotal: 1m 4s\tremaining: 53.5s\n",
            "434:\tlearn: 73.8725455\ttotal: 1m 5s\tremaining: 53.4s\n",
            "435:\tlearn: 73.7412247\ttotal: 1m 5s\tremaining: 53.2s\n",
            "436:\tlearn: 73.6870950\ttotal: 1m 5s\tremaining: 53.1s\n",
            "437:\tlearn: 73.5585262\ttotal: 1m 5s\tremaining: 53s\n",
            "438:\tlearn: 73.4502171\ttotal: 1m 5s\tremaining: 52.8s\n",
            "439:\tlearn: 73.3886845\ttotal: 1m 5s\tremaining: 52.7s\n",
            "440:\tlearn: 73.3171466\ttotal: 1m 5s\tremaining: 52.5s\n",
            "441:\tlearn: 73.2680959\ttotal: 1m 6s\tremaining: 52.4s\n",
            "442:\tlearn: 73.2293984\ttotal: 1m 6s\tremaining: 52.2s\n",
            "443:\tlearn: 73.1958972\ttotal: 1m 6s\tremaining: 52.1s\n",
            "444:\tlearn: 73.1435270\ttotal: 1m 6s\tremaining: 52s\n",
            "445:\tlearn: 73.1094052\ttotal: 1m 6s\tremaining: 51.8s\n",
            "446:\tlearn: 73.0608408\ttotal: 1m 6s\tremaining: 51.6s\n",
            "447:\tlearn: 72.9441887\ttotal: 1m 7s\tremaining: 51.5s\n",
            "448:\tlearn: 72.8852082\ttotal: 1m 7s\tremaining: 51.4s\n",
            "449:\tlearn: 72.7120800\ttotal: 1m 7s\tremaining: 51.2s\n",
            "450:\tlearn: 72.6947795\ttotal: 1m 7s\tremaining: 51.1s\n",
            "451:\tlearn: 72.6018802\ttotal: 1m 7s\tremaining: 50.9s\n",
            "452:\tlearn: 72.5467190\ttotal: 1m 7s\tremaining: 50.8s\n",
            "453:\tlearn: 72.4789287\ttotal: 1m 8s\tremaining: 50.6s\n",
            "454:\tlearn: 72.4189491\ttotal: 1m 8s\tremaining: 50.5s\n",
            "455:\tlearn: 72.3855960\ttotal: 1m 8s\tremaining: 50.4s\n",
            "456:\tlearn: 72.3847928\ttotal: 1m 8s\tremaining: 50.2s\n",
            "457:\tlearn: 72.2461631\ttotal: 1m 8s\tremaining: 50.1s\n",
            "458:\tlearn: 72.1670944\ttotal: 1m 8s\tremaining: 49.9s\n",
            "459:\tlearn: 72.1296207\ttotal: 1m 8s\tremaining: 49.8s\n",
            "460:\tlearn: 72.0556237\ttotal: 1m 9s\tremaining: 49.7s\n",
            "461:\tlearn: 72.0433549\ttotal: 1m 9s\tremaining: 49.5s\n",
            "462:\tlearn: 71.9692307\ttotal: 1m 9s\tremaining: 49.3s\n",
            "463:\tlearn: 71.9272550\ttotal: 1m 9s\tremaining: 49.2s\n",
            "464:\tlearn: 71.7658056\ttotal: 1m 9s\tremaining: 49s\n",
            "465:\tlearn: 71.5969479\ttotal: 1m 9s\tremaining: 48.9s\n",
            "466:\tlearn: 71.5464238\ttotal: 1m 10s\tremaining: 48.7s\n",
            "467:\tlearn: 71.5079710\ttotal: 1m 10s\tremaining: 48.6s\n",
            "468:\tlearn: 71.4427883\ttotal: 1m 10s\tremaining: 48.4s\n",
            "469:\tlearn: 71.4126379\ttotal: 1m 10s\tremaining: 48.3s\n",
            "470:\tlearn: 71.2787961\ttotal: 1m 10s\tremaining: 48.1s\n",
            "471:\tlearn: 71.1493504\ttotal: 1m 10s\tremaining: 48s\n",
            "472:\tlearn: 71.1182103\ttotal: 1m 10s\tremaining: 47.9s\n",
            "473:\tlearn: 71.0716917\ttotal: 1m 11s\tremaining: 47.7s\n",
            "474:\tlearn: 70.8745918\ttotal: 1m 11s\tremaining: 47.6s\n",
            "475:\tlearn: 70.8388786\ttotal: 1m 11s\tremaining: 47.4s\n",
            "476:\tlearn: 70.8154596\ttotal: 1m 11s\tremaining: 47.3s\n",
            "477:\tlearn: 70.8118483\ttotal: 1m 11s\tremaining: 47.1s\n",
            "478:\tlearn: 70.8081061\ttotal: 1m 11s\tremaining: 47s\n",
            "479:\tlearn: 70.6558778\ttotal: 1m 12s\tremaining: 46.8s\n",
            "480:\tlearn: 70.6547680\ttotal: 1m 12s\tremaining: 46.7s\n",
            "481:\tlearn: 70.5235477\ttotal: 1m 12s\tremaining: 46.5s\n",
            "482:\tlearn: 70.4983719\ttotal: 1m 12s\tremaining: 46.4s\n",
            "483:\tlearn: 70.3921825\ttotal: 1m 12s\tremaining: 46.3s\n",
            "484:\tlearn: 70.3659288\ttotal: 1m 12s\tremaining: 46.1s\n",
            "485:\tlearn: 70.2243928\ttotal: 1m 12s\tremaining: 46s\n",
            "486:\tlearn: 70.1856317\ttotal: 1m 13s\tremaining: 45.8s\n",
            "487:\tlearn: 70.1321296\ttotal: 1m 13s\tremaining: 45.7s\n",
            "488:\tlearn: 70.0662454\ttotal: 1m 13s\tremaining: 45.5s\n",
            "489:\tlearn: 70.0512648\ttotal: 1m 13s\tremaining: 45.4s\n",
            "490:\tlearn: 70.0489853\ttotal: 1m 13s\tremaining: 45.2s\n",
            "491:\tlearn: 70.0324959\ttotal: 1m 13s\tremaining: 45.1s\n",
            "492:\tlearn: 69.9907489\ttotal: 1m 14s\tremaining: 45s\n",
            "493:\tlearn: 69.8834242\ttotal: 1m 14s\tremaining: 44.8s\n",
            "494:\tlearn: 69.8152209\ttotal: 1m 14s\tremaining: 44.7s\n",
            "495:\tlearn: 69.6969167\ttotal: 1m 14s\tremaining: 44.5s\n",
            "496:\tlearn: 69.5261387\ttotal: 1m 14s\tremaining: 44.3s\n",
            "497:\tlearn: 69.3654221\ttotal: 1m 14s\tremaining: 44.2s\n",
            "498:\tlearn: 69.3327856\ttotal: 1m 15s\tremaining: 44.1s\n",
            "499:\tlearn: 69.3134337\ttotal: 1m 15s\tremaining: 43.9s\n",
            "500:\tlearn: 69.2424902\ttotal: 1m 15s\tremaining: 43.8s\n",
            "501:\tlearn: 69.1870317\ttotal: 1m 15s\tremaining: 43.6s\n",
            "502:\tlearn: 69.0128975\ttotal: 1m 15s\tremaining: 43.5s\n",
            "503:\tlearn: 68.9847852\ttotal: 1m 15s\tremaining: 43.3s\n",
            "504:\tlearn: 68.9111119\ttotal: 1m 15s\tremaining: 43.2s\n",
            "505:\tlearn: 68.8471063\ttotal: 1m 16s\tremaining: 43s\n",
            "506:\tlearn: 68.7260797\ttotal: 1m 16s\tremaining: 42.9s\n",
            "507:\tlearn: 68.6557661\ttotal: 1m 16s\tremaining: 42.7s\n",
            "508:\tlearn: 68.5885245\ttotal: 1m 16s\tremaining: 42.6s\n",
            "509:\tlearn: 68.5214375\ttotal: 1m 16s\tremaining: 42.4s\n",
            "510:\tlearn: 68.3959342\ttotal: 1m 16s\tremaining: 42.2s\n",
            "511:\tlearn: 68.3453252\ttotal: 1m 16s\tremaining: 42.1s\n",
            "512:\tlearn: 68.2687541\ttotal: 1m 17s\tremaining: 41.9s\n",
            "513:\tlearn: 68.2432628\ttotal: 1m 17s\tremaining: 41.8s\n",
            "514:\tlearn: 68.0901681\ttotal: 1m 17s\tremaining: 41.6s\n",
            "515:\tlearn: 68.0447756\ttotal: 1m 17s\tremaining: 41.5s\n",
            "516:\tlearn: 67.9266206\ttotal: 1m 17s\tremaining: 41.3s\n",
            "517:\tlearn: 67.8196776\ttotal: 1m 17s\tremaining: 41.2s\n",
            "518:\tlearn: 67.7577953\ttotal: 1m 17s\tremaining: 41s\n",
            "519:\tlearn: 67.6633584\ttotal: 1m 18s\tremaining: 40.8s\n",
            "520:\tlearn: 67.6133948\ttotal: 1m 18s\tremaining: 40.7s\n",
            "521:\tlearn: 67.5635941\ttotal: 1m 18s\tremaining: 40.6s\n",
            "522:\tlearn: 67.4339220\ttotal: 1m 18s\tremaining: 40.4s\n",
            "523:\tlearn: 67.3320620\ttotal: 1m 18s\tremaining: 40.2s\n",
            "524:\tlearn: 67.3078110\ttotal: 1m 18s\tremaining: 40.1s\n",
            "525:\tlearn: 67.2818714\ttotal: 1m 19s\tremaining: 40s\n",
            "526:\tlearn: 67.2269748\ttotal: 1m 19s\tremaining: 39.8s\n",
            "527:\tlearn: 67.1615952\ttotal: 1m 19s\tremaining: 39.6s\n",
            "528:\tlearn: 67.1275572\ttotal: 1m 19s\tremaining: 39.5s\n",
            "529:\tlearn: 66.9846293\ttotal: 1m 19s\tremaining: 39.3s\n",
            "530:\tlearn: 66.9414054\ttotal: 1m 19s\tremaining: 39.2s\n",
            "531:\tlearn: 66.8717126\ttotal: 1m 19s\tremaining: 39s\n",
            "532:\tlearn: 66.8388947\ttotal: 1m 20s\tremaining: 38.9s\n",
            "533:\tlearn: 66.7486145\ttotal: 1m 20s\tremaining: 38.7s\n",
            "534:\tlearn: 66.7236644\ttotal: 1m 20s\tremaining: 38.6s\n",
            "535:\tlearn: 66.5948547\ttotal: 1m 20s\tremaining: 38.4s\n",
            "536:\tlearn: 66.5744655\ttotal: 1m 20s\tremaining: 38.3s\n",
            "537:\tlearn: 66.5398017\ttotal: 1m 20s\tremaining: 38.1s\n",
            "538:\tlearn: 66.5213207\ttotal: 1m 20s\tremaining: 38s\n",
            "539:\tlearn: 66.4904514\ttotal: 1m 21s\tremaining: 37.8s\n",
            "540:\tlearn: 66.4601950\ttotal: 1m 21s\tremaining: 37.7s\n",
            "541:\tlearn: 66.4475657\ttotal: 1m 21s\tremaining: 37.6s\n",
            "542:\tlearn: 66.3838896\ttotal: 1m 21s\tremaining: 37.4s\n",
            "543:\tlearn: 66.2895332\ttotal: 1m 21s\tremaining: 37.3s\n",
            "544:\tlearn: 66.2318285\ttotal: 1m 21s\tremaining: 37.1s\n",
            "545:\tlearn: 66.1818211\ttotal: 1m 22s\tremaining: 37s\n",
            "546:\tlearn: 66.0599689\ttotal: 1m 22s\tremaining: 36.8s\n",
            "547:\tlearn: 66.0204694\ttotal: 1m 22s\tremaining: 36.7s\n",
            "548:\tlearn: 65.9956555\ttotal: 1m 22s\tremaining: 36.5s\n",
            "549:\tlearn: 65.9539654\ttotal: 1m 22s\tremaining: 36.4s\n",
            "550:\tlearn: 65.9086468\ttotal: 1m 22s\tremaining: 36.2s\n",
            "551:\tlearn: 65.6977893\ttotal: 1m 22s\tremaining: 36.1s\n",
            "552:\tlearn: 65.5603481\ttotal: 1m 23s\tremaining: 35.9s\n",
            "553:\tlearn: 65.4913559\ttotal: 1m 23s\tremaining: 35.8s\n",
            "554:\tlearn: 65.3949853\ttotal: 1m 23s\tremaining: 35.6s\n",
            "555:\tlearn: 65.3102731\ttotal: 1m 23s\tremaining: 35.5s\n",
            "556:\tlearn: 65.2837681\ttotal: 1m 23s\tremaining: 35.3s\n",
            "557:\tlearn: 65.2519499\ttotal: 1m 23s\tremaining: 35.2s\n",
            "558:\tlearn: 65.1486239\ttotal: 1m 24s\tremaining: 35s\n",
            "559:\tlearn: 65.1471704\ttotal: 1m 24s\tremaining: 34.9s\n",
            "560:\tlearn: 65.0928283\ttotal: 1m 24s\tremaining: 34.7s\n",
            "561:\tlearn: 64.9895317\ttotal: 1m 24s\tremaining: 34.6s\n",
            "562:\tlearn: 64.9352753\ttotal: 1m 24s\tremaining: 34.4s\n",
            "563:\tlearn: 64.8755634\ttotal: 1m 24s\tremaining: 34.3s\n",
            "564:\tlearn: 64.8674961\ttotal: 1m 24s\tremaining: 34.1s\n",
            "565:\tlearn: 64.7773536\ttotal: 1m 25s\tremaining: 34s\n",
            "566:\tlearn: 64.7479228\ttotal: 1m 25s\tremaining: 33.8s\n",
            "567:\tlearn: 64.6393295\ttotal: 1m 25s\tremaining: 33.7s\n",
            "568:\tlearn: 64.5280432\ttotal: 1m 25s\tremaining: 33.5s\n",
            "569:\tlearn: 64.4139549\ttotal: 1m 25s\tremaining: 33.4s\n",
            "570:\tlearn: 64.3353139\ttotal: 1m 25s\tremaining: 33.2s\n",
            "571:\tlearn: 64.2960970\ttotal: 1m 26s\tremaining: 33.1s\n",
            "572:\tlearn: 64.2488933\ttotal: 1m 26s\tremaining: 32.9s\n",
            "573:\tlearn: 64.1204391\ttotal: 1m 26s\tremaining: 32.8s\n",
            "574:\tlearn: 64.0822894\ttotal: 1m 26s\tremaining: 32.6s\n",
            "575:\tlearn: 63.9746225\ttotal: 1m 26s\tremaining: 32.5s\n",
            "576:\tlearn: 63.9328335\ttotal: 1m 26s\tremaining: 32.3s\n",
            "577:\tlearn: 63.8925741\ttotal: 1m 26s\tremaining: 32.2s\n",
            "578:\tlearn: 63.8802931\ttotal: 1m 27s\tremaining: 32s\n",
            "579:\tlearn: 63.7626148\ttotal: 1m 27s\tremaining: 31.9s\n",
            "580:\tlearn: 63.7448373\ttotal: 1m 27s\tremaining: 31.7s\n",
            "581:\tlearn: 63.5958646\ttotal: 1m 27s\tremaining: 31.6s\n",
            "582:\tlearn: 63.5408869\ttotal: 1m 27s\tremaining: 31.4s\n",
            "583:\tlearn: 63.4699391\ttotal: 1m 27s\tremaining: 31.3s\n",
            "584:\tlearn: 63.2255358\ttotal: 1m 27s\tremaining: 31.1s\n",
            "585:\tlearn: 63.1007753\ttotal: 1m 28s\tremaining: 31s\n",
            "586:\tlearn: 63.0752347\ttotal: 1m 28s\tremaining: 30.8s\n",
            "587:\tlearn: 62.9690318\ttotal: 1m 28s\tremaining: 30.7s\n",
            "588:\tlearn: 62.9083674\ttotal: 1m 28s\tremaining: 30.5s\n",
            "589:\tlearn: 62.8723160\ttotal: 1m 28s\tremaining: 30.4s\n",
            "590:\tlearn: 62.8472020\ttotal: 1m 28s\tremaining: 30.2s\n",
            "591:\tlearn: 62.8098079\ttotal: 1m 29s\tremaining: 30.1s\n",
            "592:\tlearn: 62.7093011\ttotal: 1m 29s\tremaining: 29.9s\n",
            "593:\tlearn: 62.6651596\ttotal: 1m 29s\tremaining: 29.8s\n",
            "594:\tlearn: 62.5738286\ttotal: 1m 29s\tremaining: 29.6s\n",
            "595:\tlearn: 62.3993952\ttotal: 1m 29s\tremaining: 29.5s\n",
            "596:\tlearn: 62.3730649\ttotal: 1m 29s\tremaining: 29.3s\n",
            "597:\tlearn: 62.3117323\ttotal: 1m 29s\tremaining: 29.2s\n",
            "598:\tlearn: 62.2963968\ttotal: 1m 30s\tremaining: 29s\n",
            "599:\tlearn: 62.2527585\ttotal: 1m 30s\tremaining: 28.9s\n",
            "600:\tlearn: 62.2145813\ttotal: 1m 30s\tremaining: 28.7s\n",
            "601:\tlearn: 62.1884513\ttotal: 1m 30s\tremaining: 28.6s\n",
            "602:\tlearn: 62.1501105\ttotal: 1m 30s\tremaining: 28.4s\n",
            "603:\tlearn: 62.0734607\ttotal: 1m 30s\tremaining: 28.3s\n",
            "604:\tlearn: 62.0433579\ttotal: 1m 30s\tremaining: 28.1s\n",
            "605:\tlearn: 61.9753397\ttotal: 1m 31s\tremaining: 28s\n",
            "606:\tlearn: 61.8624706\ttotal: 1m 31s\tremaining: 27.8s\n",
            "607:\tlearn: 61.7856003\ttotal: 1m 31s\tremaining: 27.7s\n",
            "608:\tlearn: 61.7327925\ttotal: 1m 31s\tremaining: 27.5s\n",
            "609:\tlearn: 61.6796728\ttotal: 1m 31s\tremaining: 27.4s\n",
            "610:\tlearn: 61.4924835\ttotal: 1m 31s\tremaining: 27.2s\n",
            "611:\tlearn: 61.4779055\ttotal: 1m 31s\tremaining: 27.1s\n",
            "612:\tlearn: 61.4052427\ttotal: 1m 32s\tremaining: 26.9s\n",
            "613:\tlearn: 61.3633354\ttotal: 1m 32s\tremaining: 26.8s\n",
            "614:\tlearn: 61.3123023\ttotal: 1m 32s\tremaining: 26.6s\n",
            "615:\tlearn: 61.2902841\ttotal: 1m 32s\tremaining: 26.4s\n",
            "616:\tlearn: 61.2519389\ttotal: 1m 32s\tremaining: 26.3s\n",
            "617:\tlearn: 61.1516715\ttotal: 1m 32s\tremaining: 26.1s\n",
            "618:\tlearn: 61.1133446\ttotal: 1m 33s\tremaining: 26s\n",
            "619:\tlearn: 61.0903203\ttotal: 1m 33s\tremaining: 25.8s\n",
            "620:\tlearn: 61.0120648\ttotal: 1m 33s\tremaining: 25.7s\n",
            "621:\tlearn: 60.9085990\ttotal: 1m 33s\tremaining: 25.5s\n",
            "622:\tlearn: 60.8158935\ttotal: 1m 33s\tremaining: 25.4s\n",
            "623:\tlearn: 60.7335184\ttotal: 1m 33s\tremaining: 25.2s\n",
            "624:\tlearn: 60.6860588\ttotal: 1m 33s\tremaining: 25.1s\n",
            "625:\tlearn: 60.6423713\ttotal: 1m 33s\tremaining: 24.9s\n",
            "626:\tlearn: 60.6026865\ttotal: 1m 34s\tremaining: 24.8s\n",
            "627:\tlearn: 60.5731953\ttotal: 1m 34s\tremaining: 24.6s\n",
            "628:\tlearn: 60.4761356\ttotal: 1m 34s\tremaining: 24.5s\n",
            "629:\tlearn: 60.4380868\ttotal: 1m 34s\tremaining: 24.3s\n",
            "630:\tlearn: 60.4126426\ttotal: 1m 34s\tremaining: 24.2s\n",
            "631:\tlearn: 60.2495225\ttotal: 1m 34s\tremaining: 24s\n",
            "632:\tlearn: 60.2031811\ttotal: 1m 35s\tremaining: 23.9s\n",
            "633:\tlearn: 60.1420447\ttotal: 1m 35s\tremaining: 23.7s\n",
            "634:\tlearn: 60.1172396\ttotal: 1m 35s\tremaining: 23.6s\n",
            "635:\tlearn: 60.0843716\ttotal: 1m 35s\tremaining: 23.4s\n",
            "636:\tlearn: 60.0821848\ttotal: 1m 35s\tremaining: 23.3s\n",
            "637:\tlearn: 60.0068713\ttotal: 1m 35s\tremaining: 23.1s\n",
            "638:\tlearn: 59.9788876\ttotal: 1m 36s\tremaining: 23s\n",
            "639:\tlearn: 59.9355286\ttotal: 1m 36s\tremaining: 22.8s\n",
            "640:\tlearn: 59.8534257\ttotal: 1m 36s\tremaining: 22.7s\n",
            "641:\tlearn: 59.8209096\ttotal: 1m 36s\tremaining: 22.5s\n",
            "642:\tlearn: 59.7806008\ttotal: 1m 36s\tremaining: 22.4s\n",
            "643:\tlearn: 59.7243420\ttotal: 1m 36s\tremaining: 22.2s\n",
            "644:\tlearn: 59.6473515\ttotal: 1m 36s\tremaining: 22.1s\n",
            "645:\tlearn: 59.6109413\ttotal: 1m 37s\tremaining: 21.9s\n",
            "646:\tlearn: 59.5777544\ttotal: 1m 37s\tremaining: 21.8s\n",
            "647:\tlearn: 59.5213040\ttotal: 1m 37s\tremaining: 21.6s\n",
            "648:\tlearn: 59.4935507\ttotal: 1m 37s\tremaining: 21.5s\n",
            "649:\tlearn: 59.4524741\ttotal: 1m 37s\tremaining: 21.3s\n",
            "650:\tlearn: 59.4225276\ttotal: 1m 37s\tremaining: 21.2s\n",
            "651:\tlearn: 59.3989956\ttotal: 1m 38s\tremaining: 21.1s\n",
            "652:\tlearn: 59.3361503\ttotal: 1m 38s\tremaining: 20.9s\n",
            "653:\tlearn: 59.2842942\ttotal: 1m 38s\tremaining: 20.8s\n",
            "654:\tlearn: 59.1815895\ttotal: 1m 38s\tremaining: 20.6s\n",
            "655:\tlearn: 59.1467799\ttotal: 1m 38s\tremaining: 20.5s\n",
            "656:\tlearn: 59.1139890\ttotal: 1m 38s\tremaining: 20.3s\n",
            "657:\tlearn: 58.9876651\ttotal: 1m 38s\tremaining: 20.2s\n",
            "658:\tlearn: 58.9068323\ttotal: 1m 39s\tremaining: 20s\n",
            "659:\tlearn: 58.8010102\ttotal: 1m 39s\tremaining: 19.8s\n",
            "660:\tlearn: 58.7192743\ttotal: 1m 39s\tremaining: 19.7s\n",
            "661:\tlearn: 58.6683270\ttotal: 1m 39s\tremaining: 19.5s\n",
            "662:\tlearn: 58.6205930\ttotal: 1m 39s\tremaining: 19.4s\n",
            "663:\tlearn: 58.6059041\ttotal: 1m 39s\tremaining: 19.3s\n",
            "664:\tlearn: 58.5882802\ttotal: 1m 40s\tremaining: 19.1s\n",
            "665:\tlearn: 58.5462704\ttotal: 1m 40s\tremaining: 19s\n",
            "666:\tlearn: 58.5449693\ttotal: 1m 40s\tremaining: 18.8s\n",
            "667:\tlearn: 58.4710488\ttotal: 1m 40s\tremaining: 18.7s\n",
            "668:\tlearn: 58.4195511\ttotal: 1m 40s\tremaining: 18.5s\n",
            "669:\tlearn: 58.3545128\ttotal: 1m 40s\tremaining: 18.4s\n",
            "670:\tlearn: 58.3215914\ttotal: 1m 40s\tremaining: 18.2s\n",
            "671:\tlearn: 58.2905127\ttotal: 1m 41s\tremaining: 18.1s\n",
            "672:\tlearn: 58.2545748\ttotal: 1m 41s\tremaining: 17.9s\n",
            "673:\tlearn: 58.2064067\ttotal: 1m 41s\tremaining: 17.8s\n",
            "674:\tlearn: 58.1645067\ttotal: 1m 41s\tremaining: 17.6s\n",
            "675:\tlearn: 58.1155077\ttotal: 1m 41s\tremaining: 17.5s\n",
            "676:\tlearn: 58.0880304\ttotal: 1m 41s\tremaining: 17.3s\n",
            "677:\tlearn: 58.0670630\ttotal: 1m 42s\tremaining: 17.2s\n",
            "678:\tlearn: 58.0068730\ttotal: 1m 42s\tremaining: 17s\n",
            "679:\tlearn: 57.9785130\ttotal: 1m 42s\tremaining: 16.9s\n",
            "680:\tlearn: 57.9628230\ttotal: 1m 42s\tremaining: 16.7s\n",
            "681:\tlearn: 57.9402357\ttotal: 1m 42s\tremaining: 16.6s\n",
            "682:\tlearn: 57.8943826\ttotal: 1m 42s\tremaining: 16.4s\n",
            "683:\tlearn: 57.8471344\ttotal: 1m 42s\tremaining: 16.3s\n",
            "684:\tlearn: 57.8107207\ttotal: 1m 43s\tremaining: 16.1s\n",
            "685:\tlearn: 57.7514963\ttotal: 1m 43s\tremaining: 16s\n",
            "686:\tlearn: 57.6856963\ttotal: 1m 43s\tremaining: 15.8s\n",
            "687:\tlearn: 57.6700490\ttotal: 1m 43s\tremaining: 15.7s\n",
            "688:\tlearn: 57.6288379\ttotal: 1m 43s\tremaining: 15.5s\n",
            "689:\tlearn: 57.4866857\ttotal: 1m 43s\tremaining: 15.4s\n",
            "690:\tlearn: 57.4591073\ttotal: 1m 44s\tremaining: 15.2s\n",
            "691:\tlearn: 57.4155018\ttotal: 1m 44s\tremaining: 15.1s\n",
            "692:\tlearn: 57.3978684\ttotal: 1m 44s\tremaining: 14.9s\n",
            "693:\tlearn: 57.3779160\ttotal: 1m 44s\tremaining: 14.8s\n",
            "694:\tlearn: 57.3322754\ttotal: 1m 44s\tremaining: 14.6s\n",
            "695:\tlearn: 57.3182991\ttotal: 1m 44s\tremaining: 14.5s\n",
            "696:\tlearn: 57.2872002\ttotal: 1m 45s\tremaining: 14.3s\n",
            "697:\tlearn: 57.2773492\ttotal: 1m 45s\tremaining: 14.2s\n",
            "698:\tlearn: 57.2209959\ttotal: 1m 45s\tremaining: 14s\n",
            "699:\tlearn: 57.1804815\ttotal: 1m 45s\tremaining: 13.9s\n",
            "700:\tlearn: 57.1044015\ttotal: 1m 45s\tremaining: 13.7s\n",
            "701:\tlearn: 57.0821913\ttotal: 1m 45s\tremaining: 13.6s\n",
            "702:\tlearn: 57.0507765\ttotal: 1m 45s\tremaining: 13.4s\n",
            "703:\tlearn: 57.0190706\ttotal: 1m 46s\tremaining: 13.3s\n",
            "704:\tlearn: 57.0172968\ttotal: 1m 46s\tremaining: 13.1s\n",
            "705:\tlearn: 56.9795759\ttotal: 1m 46s\tremaining: 13s\n",
            "706:\tlearn: 56.9583459\ttotal: 1m 46s\tremaining: 12.8s\n",
            "707:\tlearn: 56.9005922\ttotal: 1m 46s\tremaining: 12.7s\n",
            "708:\tlearn: 56.8473511\ttotal: 1m 46s\tremaining: 12.5s\n",
            "709:\tlearn: 56.8116289\ttotal: 1m 46s\tremaining: 12.4s\n",
            "710:\tlearn: 56.7741894\ttotal: 1m 47s\tremaining: 12.2s\n",
            "711:\tlearn: 56.7358238\ttotal: 1m 47s\tremaining: 12.1s\n",
            "712:\tlearn: 56.6560106\ttotal: 1m 47s\tremaining: 11.9s\n",
            "713:\tlearn: 56.5992626\ttotal: 1m 47s\tremaining: 11.7s\n",
            "714:\tlearn: 56.5077671\ttotal: 1m 47s\tremaining: 11.6s\n",
            "715:\tlearn: 56.4705614\ttotal: 1m 47s\tremaining: 11.4s\n",
            "716:\tlearn: 56.4417852\ttotal: 1m 47s\tremaining: 11.3s\n",
            "717:\tlearn: 56.4145759\ttotal: 1m 48s\tremaining: 11.1s\n",
            "718:\tlearn: 56.3687730\ttotal: 1m 48s\tremaining: 11s\n",
            "719:\tlearn: 56.2424644\ttotal: 1m 48s\tremaining: 10.8s\n",
            "720:\tlearn: 56.2213884\ttotal: 1m 48s\tremaining: 10.7s\n",
            "721:\tlearn: 56.1114587\ttotal: 1m 48s\tremaining: 10.5s\n",
            "722:\tlearn: 56.0335653\ttotal: 1m 48s\tremaining: 10.4s\n",
            "723:\tlearn: 55.9152012\ttotal: 1m 48s\tremaining: 10.2s\n",
            "724:\tlearn: 55.8821581\ttotal: 1m 49s\tremaining: 10.1s\n",
            "725:\tlearn: 55.8578533\ttotal: 1m 49s\tremaining: 9.94s\n",
            "726:\tlearn: 55.8290164\ttotal: 1m 49s\tremaining: 9.79s\n",
            "727:\tlearn: 55.7969141\ttotal: 1m 49s\tremaining: 9.64s\n",
            "728:\tlearn: 55.7611010\ttotal: 1m 49s\tremaining: 9.48s\n",
            "729:\tlearn: 55.6400077\ttotal: 1m 49s\tremaining: 9.33s\n",
            "730:\tlearn: 55.6192448\ttotal: 1m 50s\tremaining: 9.18s\n",
            "731:\tlearn: 55.5913750\ttotal: 1m 50s\tremaining: 9.03s\n",
            "732:\tlearn: 55.4717827\ttotal: 1m 50s\tremaining: 8.88s\n",
            "733:\tlearn: 55.4413314\ttotal: 1m 50s\tremaining: 8.73s\n",
            "734:\tlearn: 55.4023023\ttotal: 1m 50s\tremaining: 8.58s\n",
            "735:\tlearn: 55.3484237\ttotal: 1m 50s\tremaining: 8.43s\n",
            "736:\tlearn: 55.3147769\ttotal: 1m 50s\tremaining: 8.28s\n",
            "737:\tlearn: 55.3091649\ttotal: 1m 51s\tremaining: 8.13s\n",
            "738:\tlearn: 55.2323259\ttotal: 1m 51s\tremaining: 7.98s\n",
            "739:\tlearn: 55.2309599\ttotal: 1m 51s\tremaining: 7.83s\n",
            "740:\tlearn: 55.2236126\ttotal: 1m 51s\tremaining: 7.68s\n",
            "741:\tlearn: 55.1755755\ttotal: 1m 51s\tremaining: 7.53s\n",
            "742:\tlearn: 55.1429143\ttotal: 1m 51s\tremaining: 7.38s\n",
            "743:\tlearn: 55.1223539\ttotal: 1m 52s\tremaining: 7.23s\n",
            "744:\tlearn: 55.0577661\ttotal: 1m 52s\tremaining: 7.08s\n",
            "745:\tlearn: 55.0101098\ttotal: 1m 52s\tremaining: 6.93s\n",
            "746:\tlearn: 54.9474286\ttotal: 1m 52s\tremaining: 6.78s\n",
            "747:\tlearn: 54.8943380\ttotal: 1m 52s\tremaining: 6.63s\n",
            "748:\tlearn: 54.7357437\ttotal: 1m 52s\tremaining: 6.48s\n",
            "749:\tlearn: 54.7142745\ttotal: 1m 53s\tremaining: 6.33s\n",
            "750:\tlearn: 54.6827009\ttotal: 1m 53s\tremaining: 6.18s\n",
            "751:\tlearn: 54.6435019\ttotal: 1m 53s\tremaining: 6.03s\n",
            "752:\tlearn: 54.6197514\ttotal: 1m 53s\tremaining: 5.88s\n",
            "753:\tlearn: 54.5611196\ttotal: 1m 53s\tremaining: 5.72s\n",
            "754:\tlearn: 54.5576137\ttotal: 1m 53s\tremaining: 5.58s\n",
            "755:\tlearn: 54.5165090\ttotal: 1m 53s\tremaining: 5.42s\n",
            "756:\tlearn: 54.4642443\ttotal: 1m 54s\tremaining: 5.28s\n",
            "757:\tlearn: 54.4339027\ttotal: 1m 54s\tremaining: 5.13s\n",
            "758:\tlearn: 54.3957187\ttotal: 1m 54s\tremaining: 4.97s\n",
            "759:\tlearn: 54.3573833\ttotal: 1m 54s\tremaining: 4.82s\n",
            "760:\tlearn: 54.2590156\ttotal: 1m 54s\tremaining: 4.67s\n",
            "761:\tlearn: 54.2223249\ttotal: 1m 54s\tremaining: 4.52s\n",
            "762:\tlearn: 54.1865843\ttotal: 1m 54s\tremaining: 4.37s\n",
            "763:\tlearn: 54.1216611\ttotal: 1m 55s\tremaining: 4.22s\n",
            "764:\tlearn: 54.0731914\ttotal: 1m 55s\tremaining: 4.07s\n",
            "765:\tlearn: 53.9370095\ttotal: 1m 55s\tremaining: 3.92s\n",
            "766:\tlearn: 53.8880924\ttotal: 1m 55s\tremaining: 3.77s\n",
            "767:\tlearn: 53.7906889\ttotal: 1m 55s\tremaining: 3.62s\n",
            "768:\tlearn: 53.7561247\ttotal: 1m 55s\tremaining: 3.46s\n",
            "769:\tlearn: 53.6904364\ttotal: 1m 56s\tremaining: 3.31s\n",
            "770:\tlearn: 53.6303696\ttotal: 1m 56s\tremaining: 3.16s\n",
            "771:\tlearn: 53.5856239\ttotal: 1m 56s\tremaining: 3.01s\n",
            "772:\tlearn: 53.5409146\ttotal: 1m 56s\tremaining: 2.86s\n",
            "773:\tlearn: 53.5158382\ttotal: 1m 56s\tremaining: 2.71s\n",
            "774:\tlearn: 53.4813933\ttotal: 1m 56s\tremaining: 2.56s\n",
            "775:\tlearn: 53.4117044\ttotal: 1m 56s\tremaining: 2.41s\n",
            "776:\tlearn: 53.3816654\ttotal: 1m 57s\tremaining: 2.26s\n",
            "777:\tlearn: 53.3548192\ttotal: 1m 57s\tremaining: 2.11s\n",
            "778:\tlearn: 53.3374309\ttotal: 1m 57s\tremaining: 1.96s\n",
            "779:\tlearn: 53.3095298\ttotal: 1m 57s\tremaining: 1.81s\n",
            "780:\tlearn: 53.2510847\ttotal: 1m 57s\tremaining: 1.66s\n",
            "781:\tlearn: 53.2216396\ttotal: 1m 57s\tremaining: 1.51s\n",
            "782:\tlearn: 53.2071499\ttotal: 1m 57s\tremaining: 1.36s\n",
            "783:\tlearn: 53.1815504\ttotal: 1m 58s\tremaining: 1.21s\n",
            "784:\tlearn: 53.1617357\ttotal: 1m 58s\tremaining: 1.05s\n",
            "785:\tlearn: 53.1441068\ttotal: 1m 58s\tremaining: 904ms\n",
            "786:\tlearn: 53.0857113\ttotal: 1m 58s\tremaining: 753ms\n",
            "787:\tlearn: 53.0227823\ttotal: 1m 58s\tremaining: 603ms\n",
            "788:\tlearn: 53.0204284\ttotal: 1m 58s\tremaining: 452ms\n",
            "789:\tlearn: 52.9526208\ttotal: 1m 59s\tremaining: 301ms\n",
            "790:\tlearn: 52.9268508\ttotal: 1m 59s\tremaining: 151ms\n",
            "791:\tlearn: 52.9028114\ttotal: 1m 59s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 02:23:36,942]\u001b[0m A new study created in memory with name: no-name-5351219e-c4b3-43f8-8013-c84339c2e463\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 02:26:34,238]\u001b[0m Trial 0 finished with value: 57.310794461375295 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 57.310794461375295.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:28:51,072]\u001b[0m Trial 1 finished with value: 47.0897552436914 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 47.0897552436914.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:31:01,406]\u001b[0m Trial 2 finished with value: 47.05657757276818 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 2 with value: 47.05657757276818.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:47:16,135]\u001b[0m Trial 3 finished with value: 81.95072617976903 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 2 with value: 47.05657757276818.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 02:49:32,814]\u001b[0m Trial 4 finished with value: 54.42270842004076 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 2 with value: 47.05657757276818.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:03:02,452]\u001b[0m Trial 5 finished with value: 67.60950008855522 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 2 with value: 47.05657757276818.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:06:20,646]\u001b[0m Trial 6 finished with value: 45.34453529999927 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:09:27,294]\u001b[0m Trial 7 finished with value: 49.79700780661634 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:11:07,027]\u001b[0m Trial 8 finished with value: 67.60945391304371 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:30:37,914]\u001b[0m Trial 9 finished with value: 57.40686585917658 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:31:40,957]\u001b[0m Trial 10 finished with value: 79.97723508750317 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:33:20,226]\u001b[0m Trial 11 finished with value: 55.87401108749162 and parameters: {'iterations': 629, 'learning_rate': 0.2764615591885931, 'depth': 6, 'min_data_in_leaf': 6, 'reg_lambda': 98.40975370477312, 'subsample': 0.6137915450890752, 'random_strength': 13.607559365869914, 'od_wait': 12, 'leaf_estimation_iterations': 13, 'bagging_temperature': 99.66177975539146, 'colsample_bylevel': 0.6037888285524304}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:37:00,994]\u001b[0m Trial 12 finished with value: 47.7410907686109 and parameters: {'iterations': 312, 'learning_rate': 0.3353168481850217, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 83.3833616301794, 'subsample': 0.6018370841486229, 'random_strength': 19.02310110968608, 'od_wait': 11, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.2397968272557836, 'colsample_bylevel': 0.6798103370630707}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:39:29,033]\u001b[0m Trial 13 finished with value: 45.428184253183424 and parameters: {'iterations': 768, 'learning_rate': 0.39733087730033134, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 51.709612487085614, 'subsample': 0.8428965893650274, 'random_strength': 68.23293339853619, 'od_wait': 31, 'leaf_estimation_iterations': 10, 'bagging_temperature': 13.355307798299567, 'colsample_bylevel': 0.7317116575399958}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:41:33,668]\u001b[0m Trial 14 finished with value: 62.39304137708308 and parameters: {'iterations': 741, 'learning_rate': 0.12115442835598422, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 51.397128917456506, 'subsample': 0.879970061817357, 'random_strength': 74.51101113965332, 'od_wait': 94, 'leaf_estimation_iterations': 10, 'bagging_temperature': 39.38019209537576, 'colsample_bylevel': 0.5241786552111687}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:42:55,613]\u001b[0m Trial 15 finished with value: 65.63417833517829 and parameters: {'iterations': 815, 'learning_rate': 0.32308158949960997, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 49.37245683486181, 'subsample': 0.9930815447318664, 'random_strength': 66.99328267466255, 'od_wait': 41, 'leaf_estimation_iterations': 13, 'bagging_temperature': 14.566066853374414, 'colsample_bylevel': 0.061943926372203806}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:44:17,984]\u001b[0m Trial 16 finished with value: 64.89807632320789 and parameters: {'iterations': 592, 'learning_rate': 0.2407235903215652, 'depth': 5, 'min_data_in_leaf': 8, 'reg_lambda': 62.839769611979925, 'subsample': 0.8595044436039624, 'random_strength': 98.23074027293896, 'od_wait': 31, 'leaf_estimation_iterations': 8, 'bagging_temperature': 20.16763561176596, 'colsample_bylevel': 0.7715955209093116}. Best is trial 6 with value: 45.34453529999927.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:46:58,965]\u001b[0m Trial 17 finished with value: 42.615525740274286 and parameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}. Best is trial 17 with value: 42.615525740274286.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:55:23,514]\u001b[0m Trial 18 finished with value: 51.59795686679941 and parameters: {'iterations': 838, 'learning_rate': 0.4270062418190881, 'depth': 12, 'min_data_in_leaf': 13, 'reg_lambda': 40.65971705190806, 'subsample': 0.6934883378818296, 'random_strength': 87.60982739927447, 'od_wait': 86, 'leaf_estimation_iterations': 15, 'bagging_temperature': 55.57442205671075, 'colsample_bylevel': 0.22089840378700903}. Best is trial 17 with value: 42.615525740274286.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 03:58:51,789]\u001b[0m Trial 19 finished with value: 45.50729628393904 and parameters: {'iterations': 996, 'learning_rate': 0.18484933160405892, 'depth': 8, 'min_data_in_leaf': 20, 'reg_lambda': 42.564868866728915, 'subsample': 0.729625123932983, 'random_strength': 45.629127031175706, 'od_wait': 63, 'leaf_estimation_iterations': 17, 'bagging_temperature': 50.428094719702216, 'colsample_bylevel': 0.46751740730087066}. Best is trial 17 with value: 42.615525740274286.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 3: 42.615525740274286\n",
            "Best hyperparameters for fold 3: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "0:\tlearn: 206.8902255\ttotal: 156ms\tremaining: 2m 3s\n",
            "1:\tlearn: 205.6392514\ttotal: 241ms\tremaining: 1m 35s\n",
            "2:\tlearn: 205.2129312\ttotal: 411ms\tremaining: 1m 48s\n",
            "3:\tlearn: 205.0020045\ttotal: 483ms\tremaining: 1m 35s\n",
            "4:\tlearn: 205.0020045\ttotal: 531ms\tremaining: 1m 23s\n",
            "5:\tlearn: 204.6891131\ttotal: 685ms\tremaining: 1m 29s\n",
            "6:\tlearn: 202.8235990\ttotal: 827ms\tremaining: 1m 32s\n",
            "7:\tlearn: 201.9483903\ttotal: 992ms\tremaining: 1m 37s\n",
            "8:\tlearn: 201.7532797\ttotal: 1.16s\tremaining: 1m 41s\n",
            "9:\tlearn: 200.7952509\ttotal: 1.34s\tremaining: 1m 44s\n",
            "10:\tlearn: 200.6132614\ttotal: 1.51s\tremaining: 1m 47s\n",
            "11:\tlearn: 200.4544639\ttotal: 1.68s\tremaining: 1m 48s\n",
            "12:\tlearn: 200.1654063\ttotal: 1.86s\tremaining: 1m 51s\n",
            "13:\tlearn: 199.1776330\ttotal: 2.02s\tremaining: 1m 52s\n",
            "14:\tlearn: 198.8835189\ttotal: 2.16s\tremaining: 1m 51s\n",
            "15:\tlearn: 198.8639423\ttotal: 2.23s\tremaining: 1m 48s\n",
            "16:\tlearn: 198.8639423\ttotal: 2.28s\tremaining: 1m 44s\n",
            "17:\tlearn: 198.6969025\ttotal: 2.44s\tremaining: 1m 45s\n",
            "18:\tlearn: 198.4024319\ttotal: 2.61s\tremaining: 1m 46s\n",
            "19:\tlearn: 198.2202286\ttotal: 2.76s\tremaining: 1m 46s\n",
            "20:\tlearn: 198.2107069\ttotal: 2.85s\tremaining: 1m 44s\n",
            "21:\tlearn: 198.1207887\ttotal: 3.01s\tremaining: 1m 45s\n",
            "22:\tlearn: 198.1132433\ttotal: 3.17s\tremaining: 1m 46s\n",
            "23:\tlearn: 197.9495169\ttotal: 3.35s\tremaining: 1m 47s\n",
            "24:\tlearn: 197.9439679\ttotal: 3.43s\tremaining: 1m 45s\n",
            "25:\tlearn: 197.7103389\ttotal: 3.59s\tremaining: 1m 45s\n",
            "26:\tlearn: 197.6208727\ttotal: 3.75s\tremaining: 1m 46s\n",
            "27:\tlearn: 197.4794694\ttotal: 3.9s\tremaining: 1m 46s\n",
            "28:\tlearn: 197.1917484\ttotal: 4.06s\tremaining: 1m 46s\n",
            "29:\tlearn: 196.8383169\ttotal: 4.22s\tremaining: 1m 47s\n",
            "30:\tlearn: 196.4169178\ttotal: 4.38s\tremaining: 1m 47s\n",
            "31:\tlearn: 196.4063698\ttotal: 4.49s\tremaining: 1m 46s\n",
            "32:\tlearn: 196.4062202\ttotal: 4.55s\tremaining: 1m 44s\n",
            "33:\tlearn: 195.1546875\ttotal: 4.71s\tremaining: 1m 45s\n",
            "34:\tlearn: 195.0625592\ttotal: 4.8s\tremaining: 1m 43s\n",
            "35:\tlearn: 194.9681388\ttotal: 4.92s\tremaining: 1m 43s\n",
            "36:\tlearn: 194.1581317\ttotal: 5.11s\tremaining: 1m 44s\n",
            "37:\tlearn: 193.7497749\ttotal: 5.25s\tremaining: 1m 44s\n",
            "38:\tlearn: 192.5442887\ttotal: 5.42s\tremaining: 1m 44s\n",
            "39:\tlearn: 191.6649301\ttotal: 5.56s\tremaining: 1m 44s\n",
            "40:\tlearn: 191.0057641\ttotal: 5.73s\tremaining: 1m 44s\n",
            "41:\tlearn: 189.9348080\ttotal: 5.88s\tremaining: 1m 44s\n",
            "42:\tlearn: 188.8478074\ttotal: 6.03s\tremaining: 1m 44s\n",
            "43:\tlearn: 187.3441756\ttotal: 6.19s\tremaining: 1m 45s\n",
            "44:\tlearn: 186.7637561\ttotal: 6.37s\tremaining: 1m 45s\n",
            "45:\tlearn: 186.0066543\ttotal: 6.5s\tremaining: 1m 45s\n",
            "46:\tlearn: 185.3711297\ttotal: 6.66s\tremaining: 1m 45s\n",
            "47:\tlearn: 182.9434125\ttotal: 6.81s\tremaining: 1m 45s\n",
            "48:\tlearn: 182.6146454\ttotal: 6.97s\tremaining: 1m 45s\n",
            "49:\tlearn: 182.0242536\ttotal: 7.12s\tremaining: 1m 45s\n",
            "50:\tlearn: 181.3492257\ttotal: 7.26s\tremaining: 1m 45s\n",
            "51:\tlearn: 180.4482234\ttotal: 7.41s\tremaining: 1m 45s\n",
            "52:\tlearn: 176.6291158\ttotal: 7.55s\tremaining: 1m 45s\n",
            "53:\tlearn: 176.2144210\ttotal: 7.69s\tremaining: 1m 45s\n",
            "54:\tlearn: 175.1619255\ttotal: 7.87s\tremaining: 1m 45s\n",
            "55:\tlearn: 174.7826420\ttotal: 8.03s\tremaining: 1m 45s\n",
            "56:\tlearn: 172.7933818\ttotal: 8.18s\tremaining: 1m 45s\n",
            "57:\tlearn: 172.2798516\ttotal: 8.34s\tremaining: 1m 45s\n",
            "58:\tlearn: 171.3181269\ttotal: 8.48s\tremaining: 1m 45s\n",
            "59:\tlearn: 170.9609921\ttotal: 8.64s\tremaining: 1m 45s\n",
            "60:\tlearn: 169.3626367\ttotal: 8.78s\tremaining: 1m 45s\n",
            "61:\tlearn: 168.9950792\ttotal: 8.94s\tremaining: 1m 45s\n",
            "62:\tlearn: 168.0322078\ttotal: 9.09s\tremaining: 1m 45s\n",
            "63:\tlearn: 167.0369041\ttotal: 9.23s\tremaining: 1m 45s\n",
            "64:\tlearn: 165.9629175\ttotal: 9.38s\tremaining: 1m 44s\n",
            "65:\tlearn: 164.5177694\ttotal: 9.54s\tremaining: 1m 44s\n",
            "66:\tlearn: 162.8896455\ttotal: 9.69s\tremaining: 1m 44s\n",
            "67:\tlearn: 161.9993379\ttotal: 9.84s\tremaining: 1m 44s\n",
            "68:\tlearn: 161.8255491\ttotal: 10s\tremaining: 1m 44s\n",
            "69:\tlearn: 161.5904644\ttotal: 10.2s\tremaining: 1m 44s\n",
            "70:\tlearn: 160.2652759\ttotal: 10.3s\tremaining: 1m 44s\n",
            "71:\tlearn: 159.1745166\ttotal: 10.4s\tremaining: 1m 44s\n",
            "72:\tlearn: 158.8883445\ttotal: 10.6s\tremaining: 1m 44s\n",
            "73:\tlearn: 157.8994367\ttotal: 10.7s\tremaining: 1m 44s\n",
            "74:\tlearn: 157.3566433\ttotal: 10.9s\tremaining: 1m 43s\n",
            "75:\tlearn: 157.0205308\ttotal: 11s\tremaining: 1m 43s\n",
            "76:\tlearn: 156.7022287\ttotal: 11.2s\tremaining: 1m 43s\n",
            "77:\tlearn: 155.9631742\ttotal: 11.3s\tremaining: 1m 43s\n",
            "78:\tlearn: 155.5260561\ttotal: 11.5s\tremaining: 1m 44s\n",
            "79:\tlearn: 154.0386846\ttotal: 11.7s\tremaining: 1m 43s\n",
            "80:\tlearn: 153.3865299\ttotal: 11.8s\tremaining: 1m 43s\n",
            "81:\tlearn: 152.9774436\ttotal: 12s\tremaining: 1m 43s\n",
            "82:\tlearn: 152.6286780\ttotal: 12.1s\tremaining: 1m 43s\n",
            "83:\tlearn: 152.2429189\ttotal: 12.3s\tremaining: 1m 43s\n",
            "84:\tlearn: 152.1219204\ttotal: 12.4s\tremaining: 1m 43s\n",
            "85:\tlearn: 152.0783994\ttotal: 12.6s\tremaining: 1m 43s\n",
            "86:\tlearn: 151.9130701\ttotal: 12.7s\tremaining: 1m 43s\n",
            "87:\tlearn: 149.8393503\ttotal: 12.9s\tremaining: 1m 43s\n",
            "88:\tlearn: 148.9828375\ttotal: 13s\tremaining: 1m 42s\n",
            "89:\tlearn: 147.9167296\ttotal: 13.2s\tremaining: 1m 42s\n",
            "90:\tlearn: 146.9141285\ttotal: 13.3s\tremaining: 1m 42s\n",
            "91:\tlearn: 145.0581605\ttotal: 13.5s\tremaining: 1m 42s\n",
            "92:\tlearn: 143.9172940\ttotal: 13.6s\tremaining: 1m 42s\n",
            "93:\tlearn: 143.6588012\ttotal: 13.8s\tremaining: 1m 42s\n",
            "94:\tlearn: 142.7840739\ttotal: 13.9s\tremaining: 1m 41s\n",
            "95:\tlearn: 142.2205615\ttotal: 14s\tremaining: 1m 41s\n",
            "96:\tlearn: 141.4983687\ttotal: 14.2s\tremaining: 1m 41s\n",
            "97:\tlearn: 141.3187732\ttotal: 14.3s\tremaining: 1m 41s\n",
            "98:\tlearn: 140.7390184\ttotal: 14.5s\tremaining: 1m 41s\n",
            "99:\tlearn: 139.6305459\ttotal: 14.6s\tremaining: 1m 41s\n",
            "100:\tlearn: 139.4135681\ttotal: 14.8s\tremaining: 1m 41s\n",
            "101:\tlearn: 139.2241650\ttotal: 14.9s\tremaining: 1m 40s\n",
            "102:\tlearn: 138.5318584\ttotal: 15s\tremaining: 1m 40s\n",
            "103:\tlearn: 136.9513248\ttotal: 15.2s\tremaining: 1m 40s\n",
            "104:\tlearn: 136.1400065\ttotal: 15.3s\tremaining: 1m 40s\n",
            "105:\tlearn: 135.7086747\ttotal: 15.5s\tremaining: 1m 40s\n",
            "106:\tlearn: 135.4059110\ttotal: 15.6s\tremaining: 1m 39s\n",
            "107:\tlearn: 135.2556730\ttotal: 15.8s\tremaining: 1m 39s\n",
            "108:\tlearn: 133.6723308\ttotal: 15.9s\tremaining: 1m 39s\n",
            "109:\tlearn: 133.4531229\ttotal: 16.1s\tremaining: 1m 39s\n",
            "110:\tlearn: 132.7378460\ttotal: 16.2s\tremaining: 1m 39s\n",
            "111:\tlearn: 132.2354894\ttotal: 16.4s\tremaining: 1m 39s\n",
            "112:\tlearn: 132.0846901\ttotal: 16.5s\tremaining: 1m 39s\n",
            "113:\tlearn: 131.9443341\ttotal: 16.6s\tremaining: 1m 38s\n",
            "114:\tlearn: 131.5993670\ttotal: 16.8s\tremaining: 1m 38s\n",
            "115:\tlearn: 131.4557152\ttotal: 17s\tremaining: 1m 38s\n",
            "116:\tlearn: 131.4338894\ttotal: 17.1s\tremaining: 1m 38s\n",
            "117:\tlearn: 131.0955826\ttotal: 17.3s\tremaining: 1m 38s\n",
            "118:\tlearn: 129.8195342\ttotal: 17.4s\tremaining: 1m 38s\n",
            "119:\tlearn: 128.8877850\ttotal: 17.6s\tremaining: 1m 38s\n",
            "120:\tlearn: 128.2788655\ttotal: 17.7s\tremaining: 1m 38s\n",
            "121:\tlearn: 128.1538525\ttotal: 17.9s\tremaining: 1m 38s\n",
            "122:\tlearn: 127.8406776\ttotal: 18s\tremaining: 1m 37s\n",
            "123:\tlearn: 127.5245606\ttotal: 18.1s\tremaining: 1m 37s\n",
            "124:\tlearn: 126.6490246\ttotal: 18.3s\tremaining: 1m 37s\n",
            "125:\tlearn: 126.4362391\ttotal: 18.4s\tremaining: 1m 37s\n",
            "126:\tlearn: 125.9354467\ttotal: 18.5s\tremaining: 1m 37s\n",
            "127:\tlearn: 125.7521905\ttotal: 18.7s\tremaining: 1m 36s\n",
            "128:\tlearn: 125.6305745\ttotal: 18.9s\tremaining: 1m 36s\n",
            "129:\tlearn: 124.8457599\ttotal: 19s\tremaining: 1m 36s\n",
            "130:\tlearn: 124.5740362\ttotal: 19.1s\tremaining: 1m 36s\n",
            "131:\tlearn: 124.5088849\ttotal: 19.3s\tremaining: 1m 36s\n",
            "132:\tlearn: 124.2979371\ttotal: 19.4s\tremaining: 1m 36s\n",
            "133:\tlearn: 123.9403428\ttotal: 19.6s\tremaining: 1m 36s\n",
            "134:\tlearn: 123.9355680\ttotal: 19.7s\tremaining: 1m 36s\n",
            "135:\tlearn: 123.8466319\ttotal: 19.9s\tremaining: 1m 36s\n",
            "136:\tlearn: 123.0447056\ttotal: 20.1s\tremaining: 1m 35s\n",
            "137:\tlearn: 122.4589710\ttotal: 20.2s\tremaining: 1m 35s\n",
            "138:\tlearn: 122.3118803\ttotal: 20.4s\tremaining: 1m 35s\n",
            "139:\tlearn: 122.1532905\ttotal: 20.5s\tremaining: 1m 35s\n",
            "140:\tlearn: 121.8532331\ttotal: 20.7s\tremaining: 1m 35s\n",
            "141:\tlearn: 121.3418511\ttotal: 20.8s\tremaining: 1m 35s\n",
            "142:\tlearn: 120.9414413\ttotal: 21s\tremaining: 1m 35s\n",
            "143:\tlearn: 120.8075781\ttotal: 21.1s\tremaining: 1m 35s\n",
            "144:\tlearn: 120.6746304\ttotal: 21.3s\tremaining: 1m 35s\n",
            "145:\tlearn: 120.4726448\ttotal: 21.5s\tremaining: 1m 34s\n",
            "146:\tlearn: 120.2844815\ttotal: 21.6s\tremaining: 1m 34s\n",
            "147:\tlearn: 119.3997950\ttotal: 21.7s\tremaining: 1m 34s\n",
            "148:\tlearn: 118.9983348\ttotal: 21.9s\tremaining: 1m 34s\n",
            "149:\tlearn: 118.8427553\ttotal: 22s\tremaining: 1m 34s\n",
            "150:\tlearn: 118.4199098\ttotal: 22.2s\tremaining: 1m 34s\n",
            "151:\tlearn: 117.8925037\ttotal: 22.3s\tremaining: 1m 33s\n",
            "152:\tlearn: 117.7447950\ttotal: 22.5s\tremaining: 1m 33s\n",
            "153:\tlearn: 117.5399591\ttotal: 22.6s\tremaining: 1m 33s\n",
            "154:\tlearn: 117.3195936\ttotal: 22.8s\tremaining: 1m 33s\n",
            "155:\tlearn: 117.0729454\ttotal: 22.9s\tremaining: 1m 33s\n",
            "156:\tlearn: 116.4185154\ttotal: 23.1s\tremaining: 1m 33s\n",
            "157:\tlearn: 116.1724094\ttotal: 23.2s\tremaining: 1m 33s\n",
            "158:\tlearn: 115.9537737\ttotal: 23.4s\tremaining: 1m 32s\n",
            "159:\tlearn: 115.7480674\ttotal: 23.5s\tremaining: 1m 32s\n",
            "160:\tlearn: 115.5920091\ttotal: 23.6s\tremaining: 1m 32s\n",
            "161:\tlearn: 115.0825131\ttotal: 23.8s\tremaining: 1m 32s\n",
            "162:\tlearn: 114.6897783\ttotal: 23.9s\tremaining: 1m 32s\n",
            "163:\tlearn: 113.8460237\ttotal: 24.1s\tremaining: 1m 32s\n",
            "164:\tlearn: 113.7533845\ttotal: 24.2s\tremaining: 1m 31s\n",
            "165:\tlearn: 113.3287437\ttotal: 24.3s\tremaining: 1m 31s\n",
            "166:\tlearn: 113.0621873\ttotal: 24.5s\tremaining: 1m 31s\n",
            "167:\tlearn: 112.8737050\ttotal: 24.6s\tremaining: 1m 31s\n",
            "168:\tlearn: 112.4365597\ttotal: 24.8s\tremaining: 1m 31s\n",
            "169:\tlearn: 112.3869856\ttotal: 24.9s\tremaining: 1m 31s\n",
            "170:\tlearn: 112.2968278\ttotal: 25.1s\tremaining: 1m 31s\n",
            "171:\tlearn: 110.9704132\ttotal: 25.2s\tremaining: 1m 30s\n",
            "172:\tlearn: 110.9001029\ttotal: 25.4s\tremaining: 1m 30s\n",
            "173:\tlearn: 110.6970631\ttotal: 25.5s\tremaining: 1m 30s\n",
            "174:\tlearn: 110.5604118\ttotal: 25.7s\tremaining: 1m 30s\n",
            "175:\tlearn: 110.4187746\ttotal: 25.8s\tremaining: 1m 30s\n",
            "176:\tlearn: 109.9900680\ttotal: 26s\tremaining: 1m 30s\n",
            "177:\tlearn: 109.7996202\ttotal: 26.1s\tremaining: 1m 30s\n",
            "178:\tlearn: 109.4862103\ttotal: 26.3s\tremaining: 1m 29s\n",
            "179:\tlearn: 109.4126521\ttotal: 26.4s\tremaining: 1m 29s\n",
            "180:\tlearn: 109.3064310\ttotal: 26.6s\tremaining: 1m 29s\n",
            "181:\tlearn: 109.1845727\ttotal: 26.8s\tremaining: 1m 29s\n",
            "182:\tlearn: 108.8324980\ttotal: 26.9s\tremaining: 1m 29s\n",
            "183:\tlearn: 108.6887635\ttotal: 27.1s\tremaining: 1m 29s\n",
            "184:\tlearn: 108.3069292\ttotal: 27.2s\tremaining: 1m 29s\n",
            "185:\tlearn: 108.1182515\ttotal: 27.3s\tremaining: 1m 29s\n",
            "186:\tlearn: 107.6603677\ttotal: 27.5s\tremaining: 1m 28s\n",
            "187:\tlearn: 107.6027332\ttotal: 27.6s\tremaining: 1m 28s\n",
            "188:\tlearn: 107.4004566\ttotal: 27.8s\tremaining: 1m 28s\n",
            "189:\tlearn: 107.3283414\ttotal: 28s\tremaining: 1m 28s\n",
            "190:\tlearn: 107.1256179\ttotal: 28.1s\tremaining: 1m 28s\n",
            "191:\tlearn: 107.0241951\ttotal: 28.3s\tremaining: 1m 28s\n",
            "192:\tlearn: 106.8593720\ttotal: 28.4s\tremaining: 1m 28s\n",
            "193:\tlearn: 106.6719897\ttotal: 28.6s\tremaining: 1m 28s\n",
            "194:\tlearn: 106.6603439\ttotal: 28.8s\tremaining: 1m 28s\n",
            "195:\tlearn: 106.5898198\ttotal: 28.9s\tremaining: 1m 27s\n",
            "196:\tlearn: 106.4562045\ttotal: 29.1s\tremaining: 1m 27s\n",
            "197:\tlearn: 106.2760801\ttotal: 29.2s\tremaining: 1m 27s\n",
            "198:\tlearn: 106.0396063\ttotal: 29.4s\tremaining: 1m 27s\n",
            "199:\tlearn: 105.6650060\ttotal: 29.5s\tremaining: 1m 27s\n",
            "200:\tlearn: 105.2415395\ttotal: 29.7s\tremaining: 1m 27s\n",
            "201:\tlearn: 105.0150441\ttotal: 29.8s\tremaining: 1m 27s\n",
            "202:\tlearn: 104.8671962\ttotal: 30s\tremaining: 1m 26s\n",
            "203:\tlearn: 104.3872287\ttotal: 30.1s\tremaining: 1m 26s\n",
            "204:\tlearn: 104.2104913\ttotal: 30.3s\tremaining: 1m 26s\n",
            "205:\tlearn: 104.0020890\ttotal: 30.4s\tremaining: 1m 26s\n",
            "206:\tlearn: 103.8754676\ttotal: 30.5s\tremaining: 1m 26s\n",
            "207:\tlearn: 103.7770379\ttotal: 30.7s\tremaining: 1m 26s\n",
            "208:\tlearn: 103.1879959\ttotal: 30.8s\tremaining: 1m 26s\n",
            "209:\tlearn: 103.1112392\ttotal: 31s\tremaining: 1m 25s\n",
            "210:\tlearn: 102.9339125\ttotal: 31.2s\tremaining: 1m 25s\n",
            "211:\tlearn: 102.6373262\ttotal: 31.3s\tremaining: 1m 25s\n",
            "212:\tlearn: 102.5664684\ttotal: 31.4s\tremaining: 1m 25s\n",
            "213:\tlearn: 102.2543688\ttotal: 31.6s\tremaining: 1m 25s\n",
            "214:\tlearn: 101.9960037\ttotal: 31.7s\tremaining: 1m 25s\n",
            "215:\tlearn: 101.9085062\ttotal: 31.9s\tremaining: 1m 25s\n",
            "216:\tlearn: 101.7901424\ttotal: 32s\tremaining: 1m 24s\n",
            "217:\tlearn: 101.6792822\ttotal: 32.2s\tremaining: 1m 24s\n",
            "218:\tlearn: 101.5621810\ttotal: 32.3s\tremaining: 1m 24s\n",
            "219:\tlearn: 101.4909640\ttotal: 32.5s\tremaining: 1m 24s\n",
            "220:\tlearn: 101.1315158\ttotal: 32.6s\tremaining: 1m 24s\n",
            "221:\tlearn: 101.0755171\ttotal: 32.8s\tremaining: 1m 24s\n",
            "222:\tlearn: 100.6828619\ttotal: 32.9s\tremaining: 1m 23s\n",
            "223:\tlearn: 100.6353907\ttotal: 33.1s\tremaining: 1m 23s\n",
            "224:\tlearn: 100.5603639\ttotal: 33.2s\tremaining: 1m 23s\n",
            "225:\tlearn: 100.3847617\ttotal: 33.4s\tremaining: 1m 23s\n",
            "226:\tlearn: 100.1700490\ttotal: 33.5s\tremaining: 1m 23s\n",
            "227:\tlearn: 99.9989666\ttotal: 33.6s\tremaining: 1m 23s\n",
            "228:\tlearn: 99.9028703\ttotal: 33.8s\tremaining: 1m 23s\n",
            "229:\tlearn: 99.6638038\ttotal: 34s\tremaining: 1m 22s\n",
            "230:\tlearn: 99.5934120\ttotal: 34.1s\tremaining: 1m 22s\n",
            "231:\tlearn: 99.4851465\ttotal: 34.3s\tremaining: 1m 22s\n",
            "232:\tlearn: 99.3495668\ttotal: 34.4s\tremaining: 1m 22s\n",
            "233:\tlearn: 99.1003155\ttotal: 34.5s\tremaining: 1m 22s\n",
            "234:\tlearn: 98.9782578\ttotal: 34.7s\tremaining: 1m 22s\n",
            "235:\tlearn: 98.8709145\ttotal: 34.8s\tremaining: 1m 22s\n",
            "236:\tlearn: 98.5457967\ttotal: 35s\tremaining: 1m 21s\n",
            "237:\tlearn: 98.4373993\ttotal: 35.2s\tremaining: 1m 21s\n",
            "238:\tlearn: 98.4259425\ttotal: 35.3s\tremaining: 1m 21s\n",
            "239:\tlearn: 98.3510186\ttotal: 35.5s\tremaining: 1m 21s\n",
            "240:\tlearn: 98.1160668\ttotal: 35.6s\tremaining: 1m 21s\n",
            "241:\tlearn: 97.9282072\ttotal: 35.8s\tremaining: 1m 21s\n",
            "242:\tlearn: 97.6298342\ttotal: 35.9s\tremaining: 1m 21s\n",
            "243:\tlearn: 97.3904273\ttotal: 36s\tremaining: 1m 20s\n",
            "244:\tlearn: 97.1765267\ttotal: 36.2s\tremaining: 1m 20s\n",
            "245:\tlearn: 97.0328248\ttotal: 36.3s\tremaining: 1m 20s\n",
            "246:\tlearn: 96.8350516\ttotal: 36.5s\tremaining: 1m 20s\n",
            "247:\tlearn: 96.8349411\ttotal: 36.6s\tremaining: 1m 20s\n",
            "248:\tlearn: 96.6915457\ttotal: 36.8s\tremaining: 1m 20s\n",
            "249:\tlearn: 96.6333414\ttotal: 37s\tremaining: 1m 20s\n",
            "250:\tlearn: 96.4476953\ttotal: 37.1s\tremaining: 1m 19s\n",
            "251:\tlearn: 96.4221416\ttotal: 37.3s\tremaining: 1m 19s\n",
            "252:\tlearn: 96.4032524\ttotal: 37.4s\tremaining: 1m 19s\n",
            "253:\tlearn: 96.3103910\ttotal: 37.6s\tremaining: 1m 19s\n",
            "254:\tlearn: 96.2492256\ttotal: 37.7s\tremaining: 1m 19s\n",
            "255:\tlearn: 96.1786579\ttotal: 37.9s\tremaining: 1m 19s\n",
            "256:\tlearn: 96.0367422\ttotal: 38.1s\tremaining: 1m 19s\n",
            "257:\tlearn: 95.9994060\ttotal: 38.2s\tremaining: 1m 19s\n",
            "258:\tlearn: 95.5877056\ttotal: 38.4s\tremaining: 1m 18s\n",
            "259:\tlearn: 95.3188152\ttotal: 38.5s\tremaining: 1m 18s\n",
            "260:\tlearn: 95.2224080\ttotal: 38.7s\tremaining: 1m 18s\n",
            "261:\tlearn: 94.9860330\ttotal: 38.8s\tremaining: 1m 18s\n",
            "262:\tlearn: 94.9533163\ttotal: 39s\tremaining: 1m 18s\n",
            "263:\tlearn: 94.8695420\ttotal: 39.1s\tremaining: 1m 18s\n",
            "264:\tlearn: 94.5955161\ttotal: 39.2s\tremaining: 1m 18s\n",
            "265:\tlearn: 94.5172588\ttotal: 39.4s\tremaining: 1m 17s\n",
            "266:\tlearn: 94.4451206\ttotal: 39.5s\tremaining: 1m 17s\n",
            "267:\tlearn: 94.3672296\ttotal: 39.7s\tremaining: 1m 17s\n",
            "268:\tlearn: 94.2947628\ttotal: 39.8s\tremaining: 1m 17s\n",
            "269:\tlearn: 93.8631437\ttotal: 40s\tremaining: 1m 17s\n",
            "270:\tlearn: 93.6747945\ttotal: 40.1s\tremaining: 1m 17s\n",
            "271:\tlearn: 93.6486646\ttotal: 40.3s\tremaining: 1m 17s\n",
            "272:\tlearn: 93.5495657\ttotal: 40.5s\tremaining: 1m 16s\n",
            "273:\tlearn: 93.4405803\ttotal: 40.6s\tremaining: 1m 16s\n",
            "274:\tlearn: 93.2244617\ttotal: 40.8s\tremaining: 1m 16s\n",
            "275:\tlearn: 92.9501383\ttotal: 40.9s\tremaining: 1m 16s\n",
            "276:\tlearn: 92.7445063\ttotal: 41.1s\tremaining: 1m 16s\n",
            "277:\tlearn: 92.6116944\ttotal: 41.2s\tremaining: 1m 16s\n",
            "278:\tlearn: 92.5385921\ttotal: 41.4s\tremaining: 1m 16s\n",
            "279:\tlearn: 92.4703802\ttotal: 41.5s\tremaining: 1m 15s\n",
            "280:\tlearn: 92.2622655\ttotal: 41.6s\tremaining: 1m 15s\n",
            "281:\tlearn: 92.1360761\ttotal: 41.8s\tremaining: 1m 15s\n",
            "282:\tlearn: 92.0440380\ttotal: 41.9s\tremaining: 1m 15s\n",
            "283:\tlearn: 91.9528834\ttotal: 42.1s\tremaining: 1m 15s\n",
            "284:\tlearn: 91.8269017\ttotal: 42.2s\tremaining: 1m 15s\n",
            "285:\tlearn: 91.4202892\ttotal: 42.4s\tremaining: 1m 14s\n",
            "286:\tlearn: 91.2787726\ttotal: 42.5s\tremaining: 1m 14s\n",
            "287:\tlearn: 91.0195598\ttotal: 42.7s\tremaining: 1m 14s\n",
            "288:\tlearn: 90.6762080\ttotal: 42.8s\tremaining: 1m 14s\n",
            "289:\tlearn: 90.5829839\ttotal: 43s\tremaining: 1m 14s\n",
            "290:\tlearn: 90.4736520\ttotal: 43.1s\tremaining: 1m 14s\n",
            "291:\tlearn: 90.3576181\ttotal: 43.3s\tremaining: 1m 14s\n",
            "292:\tlearn: 90.1832392\ttotal: 43.4s\tremaining: 1m 13s\n",
            "293:\tlearn: 90.1808093\ttotal: 43.6s\tremaining: 1m 13s\n",
            "294:\tlearn: 90.1282222\ttotal: 43.7s\tremaining: 1m 13s\n",
            "295:\tlearn: 90.0964900\ttotal: 43.9s\tremaining: 1m 13s\n",
            "296:\tlearn: 89.8515917\ttotal: 44s\tremaining: 1m 13s\n",
            "297:\tlearn: 89.8388336\ttotal: 44.2s\tremaining: 1m 13s\n",
            "298:\tlearn: 89.7579490\ttotal: 44.4s\tremaining: 1m 13s\n",
            "299:\tlearn: 89.4822597\ttotal: 44.5s\tremaining: 1m 13s\n",
            "300:\tlearn: 89.4029721\ttotal: 44.7s\tremaining: 1m 12s\n",
            "301:\tlearn: 89.1732040\ttotal: 44.8s\tremaining: 1m 12s\n",
            "302:\tlearn: 89.0358059\ttotal: 45s\tremaining: 1m 12s\n",
            "303:\tlearn: 88.9064472\ttotal: 45.1s\tremaining: 1m 12s\n",
            "304:\tlearn: 88.7930382\ttotal: 45.3s\tremaining: 1m 12s\n",
            "305:\tlearn: 88.7003791\ttotal: 45.4s\tremaining: 1m 12s\n",
            "306:\tlearn: 88.4833207\ttotal: 45.5s\tremaining: 1m 11s\n",
            "307:\tlearn: 88.4164348\ttotal: 45.7s\tremaining: 1m 11s\n",
            "308:\tlearn: 88.3070279\ttotal: 45.8s\tremaining: 1m 11s\n",
            "309:\tlearn: 88.2083386\ttotal: 46s\tremaining: 1m 11s\n",
            "310:\tlearn: 88.1408269\ttotal: 46.2s\tremaining: 1m 11s\n",
            "311:\tlearn: 87.9885962\ttotal: 46.3s\tremaining: 1m 11s\n",
            "312:\tlearn: 87.6122118\ttotal: 46.4s\tremaining: 1m 11s\n",
            "313:\tlearn: 87.1820474\ttotal: 46.6s\tremaining: 1m 10s\n",
            "314:\tlearn: 87.0821084\ttotal: 46.7s\tremaining: 1m 10s\n",
            "315:\tlearn: 86.9892713\ttotal: 46.8s\tremaining: 1m 10s\n",
            "316:\tlearn: 86.7201655\ttotal: 47s\tremaining: 1m 10s\n",
            "317:\tlearn: 86.5903265\ttotal: 47.1s\tremaining: 1m 10s\n",
            "318:\tlearn: 86.4369695\ttotal: 47.3s\tremaining: 1m 10s\n",
            "319:\tlearn: 86.3715174\ttotal: 47.5s\tremaining: 1m 10s\n",
            "320:\tlearn: 86.2845246\ttotal: 47.6s\tremaining: 1m 9s\n",
            "321:\tlearn: 86.2205288\ttotal: 47.8s\tremaining: 1m 9s\n",
            "322:\tlearn: 86.0680748\ttotal: 47.9s\tremaining: 1m 9s\n",
            "323:\tlearn: 85.9473395\ttotal: 48.1s\tremaining: 1m 9s\n",
            "324:\tlearn: 85.8894794\ttotal: 48.2s\tremaining: 1m 9s\n",
            "325:\tlearn: 85.8611749\ttotal: 48.4s\tremaining: 1m 9s\n",
            "326:\tlearn: 85.5246435\ttotal: 48.5s\tremaining: 1m 8s\n",
            "327:\tlearn: 85.4325004\ttotal: 48.6s\tremaining: 1m 8s\n",
            "328:\tlearn: 85.3382732\ttotal: 48.8s\tremaining: 1m 8s\n",
            "329:\tlearn: 85.2414225\ttotal: 48.9s\tremaining: 1m 8s\n",
            "330:\tlearn: 85.1689274\ttotal: 49.1s\tremaining: 1m 8s\n",
            "331:\tlearn: 85.1173416\ttotal: 49.3s\tremaining: 1m 8s\n",
            "332:\tlearn: 84.8273293\ttotal: 49.4s\tremaining: 1m 8s\n",
            "333:\tlearn: 84.7085120\ttotal: 49.5s\tremaining: 1m 7s\n",
            "334:\tlearn: 84.6296846\ttotal: 49.7s\tremaining: 1m 7s\n",
            "335:\tlearn: 84.6104878\ttotal: 49.9s\tremaining: 1m 7s\n",
            "336:\tlearn: 84.5652283\ttotal: 50s\tremaining: 1m 7s\n",
            "337:\tlearn: 84.4154978\ttotal: 50.2s\tremaining: 1m 7s\n",
            "338:\tlearn: 84.3714545\ttotal: 50.4s\tremaining: 1m 7s\n",
            "339:\tlearn: 84.2995983\ttotal: 50.5s\tremaining: 1m 7s\n",
            "340:\tlearn: 84.0482510\ttotal: 50.7s\tremaining: 1m 7s\n",
            "341:\tlearn: 83.7654567\ttotal: 50.8s\tremaining: 1m 6s\n",
            "342:\tlearn: 83.6511698\ttotal: 51s\tremaining: 1m 6s\n",
            "343:\tlearn: 83.5600891\ttotal: 51.1s\tremaining: 1m 6s\n",
            "344:\tlearn: 83.4726972\ttotal: 51.2s\tremaining: 1m 6s\n",
            "345:\tlearn: 83.4170374\ttotal: 51.4s\tremaining: 1m 6s\n",
            "346:\tlearn: 83.3460747\ttotal: 51.6s\tremaining: 1m 6s\n",
            "347:\tlearn: 83.2518751\ttotal: 51.7s\tremaining: 1m 5s\n",
            "348:\tlearn: 83.1944207\ttotal: 51.9s\tremaining: 1m 5s\n",
            "349:\tlearn: 83.1297432\ttotal: 52s\tremaining: 1m 5s\n",
            "350:\tlearn: 82.9667466\ttotal: 52.1s\tremaining: 1m 5s\n",
            "351:\tlearn: 82.9601428\ttotal: 52.3s\tremaining: 1m 5s\n",
            "352:\tlearn: 82.8284653\ttotal: 52.4s\tremaining: 1m 5s\n",
            "353:\tlearn: 82.7473183\ttotal: 52.6s\tremaining: 1m 5s\n",
            "354:\tlearn: 82.6618049\ttotal: 52.7s\tremaining: 1m 4s\n",
            "355:\tlearn: 82.6598719\ttotal: 52.9s\tremaining: 1m 4s\n",
            "356:\tlearn: 82.5320074\ttotal: 53s\tremaining: 1m 4s\n",
            "357:\tlearn: 82.2322653\ttotal: 53.2s\tremaining: 1m 4s\n",
            "358:\tlearn: 82.1664055\ttotal: 53.3s\tremaining: 1m 4s\n",
            "359:\tlearn: 81.9807217\ttotal: 53.5s\tremaining: 1m 4s\n",
            "360:\tlearn: 81.9570556\ttotal: 53.6s\tremaining: 1m 4s\n",
            "361:\tlearn: 81.9394049\ttotal: 53.8s\tremaining: 1m 3s\n",
            "362:\tlearn: 81.7031851\ttotal: 53.9s\tremaining: 1m 3s\n",
            "363:\tlearn: 81.6599398\ttotal: 54.1s\tremaining: 1m 3s\n",
            "364:\tlearn: 81.1329738\ttotal: 54.2s\tremaining: 1m 3s\n",
            "365:\tlearn: 81.0117193\ttotal: 54.3s\tremaining: 1m 3s\n",
            "366:\tlearn: 80.9479129\ttotal: 54.5s\tremaining: 1m 3s\n",
            "367:\tlearn: 80.8073055\ttotal: 54.6s\tremaining: 1m 2s\n",
            "368:\tlearn: 80.5829305\ttotal: 54.8s\tremaining: 1m 2s\n",
            "369:\tlearn: 80.5032450\ttotal: 54.9s\tremaining: 1m 2s\n",
            "370:\tlearn: 80.4290947\ttotal: 55.1s\tremaining: 1m 2s\n",
            "371:\tlearn: 80.3566669\ttotal: 55.2s\tremaining: 1m 2s\n",
            "372:\tlearn: 80.3463676\ttotal: 55.4s\tremaining: 1m 2s\n",
            "373:\tlearn: 80.3391543\ttotal: 55.6s\tremaining: 1m 2s\n",
            "374:\tlearn: 80.3019665\ttotal: 55.7s\tremaining: 1m 1s\n",
            "375:\tlearn: 80.1415042\ttotal: 55.9s\tremaining: 1m 1s\n",
            "376:\tlearn: 80.0930675\ttotal: 56s\tremaining: 1m 1s\n",
            "377:\tlearn: 79.8635684\ttotal: 56.2s\tremaining: 1m 1s\n",
            "378:\tlearn: 79.7182401\ttotal: 56.3s\tremaining: 1m 1s\n",
            "379:\tlearn: 79.6701442\ttotal: 56.5s\tremaining: 1m 1s\n",
            "380:\tlearn: 79.6593125\ttotal: 56.7s\tremaining: 1m 1s\n",
            "381:\tlearn: 79.3711191\ttotal: 56.8s\tremaining: 1m\n",
            "382:\tlearn: 79.3708890\ttotal: 56.9s\tremaining: 1m\n",
            "383:\tlearn: 79.2212887\ttotal: 57.1s\tremaining: 1m\n",
            "384:\tlearn: 79.1572183\ttotal: 57.3s\tremaining: 1m\n",
            "385:\tlearn: 79.0290135\ttotal: 57.4s\tremaining: 1m\n",
            "386:\tlearn: 79.0021538\ttotal: 57.6s\tremaining: 1m\n",
            "387:\tlearn: 78.8316389\ttotal: 57.7s\tremaining: 1m\n",
            "388:\tlearn: 78.7531418\ttotal: 57.9s\tremaining: 59.9s\n",
            "389:\tlearn: 78.6769840\ttotal: 58s\tremaining: 59.8s\n",
            "390:\tlearn: 78.5908695\ttotal: 58.2s\tremaining: 59.7s\n",
            "391:\tlearn: 78.5001028\ttotal: 58.3s\tremaining: 59.5s\n",
            "392:\tlearn: 78.3106683\ttotal: 58.5s\tremaining: 59.4s\n",
            "393:\tlearn: 78.1674443\ttotal: 58.6s\tremaining: 59.2s\n",
            "394:\tlearn: 78.0879649\ttotal: 58.8s\tremaining: 59.1s\n",
            "395:\tlearn: 77.9472816\ttotal: 58.9s\tremaining: 58.9s\n",
            "396:\tlearn: 77.8945595\ttotal: 59s\tremaining: 58.7s\n",
            "397:\tlearn: 77.7389903\ttotal: 59.2s\tremaining: 58.6s\n",
            "398:\tlearn: 77.6912064\ttotal: 59.4s\tremaining: 58.5s\n",
            "399:\tlearn: 77.6081640\ttotal: 59.5s\tremaining: 58.3s\n",
            "400:\tlearn: 77.5504878\ttotal: 59.7s\tremaining: 58.2s\n",
            "401:\tlearn: 77.4109501\ttotal: 59.8s\tremaining: 58s\n",
            "402:\tlearn: 77.3196474\ttotal: 60s\tremaining: 57.9s\n",
            "403:\tlearn: 77.2699435\ttotal: 1m\tremaining: 57.8s\n",
            "404:\tlearn: 77.1134408\ttotal: 1m\tremaining: 57.6s\n",
            "405:\tlearn: 77.0955902\ttotal: 1m\tremaining: 57.5s\n",
            "406:\tlearn: 77.0023766\ttotal: 1m\tremaining: 57.3s\n",
            "407:\tlearn: 76.9323248\ttotal: 1m\tremaining: 57.2s\n",
            "408:\tlearn: 76.8662924\ttotal: 1m\tremaining: 57.1s\n",
            "409:\tlearn: 76.8179676\ttotal: 1m 1s\tremaining: 56.9s\n",
            "410:\tlearn: 76.7931142\ttotal: 1m 1s\tremaining: 56.8s\n",
            "411:\tlearn: 76.7353676\ttotal: 1m 1s\tremaining: 56.6s\n",
            "412:\tlearn: 76.4329389\ttotal: 1m 1s\tremaining: 56.5s\n",
            "413:\tlearn: 76.3542210\ttotal: 1m 1s\tremaining: 56.3s\n",
            "414:\tlearn: 76.1922303\ttotal: 1m 1s\tremaining: 56.2s\n",
            "415:\tlearn: 76.1032143\ttotal: 1m 1s\tremaining: 56s\n",
            "416:\tlearn: 75.9842247\ttotal: 1m 2s\tremaining: 55.9s\n",
            "417:\tlearn: 75.9338427\ttotal: 1m 2s\tremaining: 55.7s\n",
            "418:\tlearn: 75.8551632\ttotal: 1m 2s\tremaining: 55.6s\n",
            "419:\tlearn: 75.8545974\ttotal: 1m 2s\tremaining: 55.4s\n",
            "420:\tlearn: 75.6751904\ttotal: 1m 2s\tremaining: 55.3s\n",
            "421:\tlearn: 75.6226158\ttotal: 1m 2s\tremaining: 55.1s\n",
            "422:\tlearn: 75.4507935\ttotal: 1m 3s\tremaining: 55s\n",
            "423:\tlearn: 75.3535551\ttotal: 1m 3s\tremaining: 54.8s\n",
            "424:\tlearn: 75.2371487\ttotal: 1m 3s\tremaining: 54.7s\n",
            "425:\tlearn: 75.2038443\ttotal: 1m 3s\tremaining: 54.5s\n",
            "426:\tlearn: 75.1470321\ttotal: 1m 3s\tremaining: 54.4s\n",
            "427:\tlearn: 75.1426272\ttotal: 1m 3s\tremaining: 54.3s\n",
            "428:\tlearn: 74.9048615\ttotal: 1m 3s\tremaining: 54.1s\n",
            "429:\tlearn: 74.6985741\ttotal: 1m 4s\tremaining: 54s\n",
            "430:\tlearn: 74.6744885\ttotal: 1m 4s\tremaining: 53.8s\n",
            "431:\tlearn: 74.6253760\ttotal: 1m 4s\tremaining: 53.7s\n",
            "432:\tlearn: 74.4992442\ttotal: 1m 4s\tremaining: 53.5s\n",
            "433:\tlearn: 74.3327805\ttotal: 1m 4s\tremaining: 53.4s\n",
            "434:\tlearn: 74.2764786\ttotal: 1m 4s\tremaining: 53.2s\n",
            "435:\tlearn: 74.2079247\ttotal: 1m 4s\tremaining: 53.1s\n",
            "436:\tlearn: 73.9176289\ttotal: 1m 5s\tremaining: 52.9s\n",
            "437:\tlearn: 73.7254108\ttotal: 1m 5s\tremaining: 52.8s\n",
            "438:\tlearn: 73.6783372\ttotal: 1m 5s\tremaining: 52.6s\n",
            "439:\tlearn: 73.6329615\ttotal: 1m 5s\tremaining: 52.5s\n",
            "440:\tlearn: 73.6121525\ttotal: 1m 5s\tremaining: 52.3s\n",
            "441:\tlearn: 73.5657866\ttotal: 1m 5s\tremaining: 52.2s\n",
            "442:\tlearn: 73.4946116\ttotal: 1m 6s\tremaining: 52.1s\n",
            "443:\tlearn: 73.4621464\ttotal: 1m 6s\tremaining: 51.9s\n",
            "444:\tlearn: 73.3976671\ttotal: 1m 6s\tremaining: 51.8s\n",
            "445:\tlearn: 73.3572239\ttotal: 1m 6s\tremaining: 51.6s\n",
            "446:\tlearn: 73.1903999\ttotal: 1m 6s\tremaining: 51.5s\n",
            "447:\tlearn: 73.0182414\ttotal: 1m 6s\tremaining: 51.3s\n",
            "448:\tlearn: 72.8647781\ttotal: 1m 6s\tremaining: 51.2s\n",
            "449:\tlearn: 72.7852901\ttotal: 1m 7s\tremaining: 51s\n",
            "450:\tlearn: 72.7841354\ttotal: 1m 7s\tremaining: 50.9s\n",
            "451:\tlearn: 72.6675969\ttotal: 1m 7s\tremaining: 50.7s\n",
            "452:\tlearn: 72.5646887\ttotal: 1m 7s\tremaining: 50.6s\n",
            "453:\tlearn: 72.5087819\ttotal: 1m 7s\tremaining: 50.4s\n",
            "454:\tlearn: 72.4322609\ttotal: 1m 7s\tremaining: 50.3s\n",
            "455:\tlearn: 72.4047128\ttotal: 1m 8s\tremaining: 50.1s\n",
            "456:\tlearn: 72.2911469\ttotal: 1m 8s\tremaining: 50s\n",
            "457:\tlearn: 72.2341047\ttotal: 1m 8s\tremaining: 49.9s\n",
            "458:\tlearn: 72.1604322\ttotal: 1m 8s\tremaining: 49.7s\n",
            "459:\tlearn: 72.0860090\ttotal: 1m 8s\tremaining: 49.6s\n",
            "460:\tlearn: 72.0653624\ttotal: 1m 8s\tremaining: 49.4s\n",
            "461:\tlearn: 72.0014998\ttotal: 1m 9s\tremaining: 49.3s\n",
            "462:\tlearn: 71.8892406\ttotal: 1m 9s\tremaining: 49.1s\n",
            "463:\tlearn: 71.8674452\ttotal: 1m 9s\tremaining: 49s\n",
            "464:\tlearn: 71.8291036\ttotal: 1m 9s\tremaining: 48.9s\n",
            "465:\tlearn: 71.7623627\ttotal: 1m 9s\tremaining: 48.7s\n",
            "466:\tlearn: 71.7237926\ttotal: 1m 9s\tremaining: 48.6s\n",
            "467:\tlearn: 71.6226351\ttotal: 1m 9s\tremaining: 48.4s\n",
            "468:\tlearn: 71.4020699\ttotal: 1m 10s\tremaining: 48.3s\n",
            "469:\tlearn: 71.3622927\ttotal: 1m 10s\tremaining: 48.1s\n",
            "470:\tlearn: 71.3223796\ttotal: 1m 10s\tremaining: 48s\n",
            "471:\tlearn: 71.2002544\ttotal: 1m 10s\tremaining: 47.8s\n",
            "472:\tlearn: 71.1760813\ttotal: 1m 10s\tremaining: 47.7s\n",
            "473:\tlearn: 71.1384927\ttotal: 1m 10s\tremaining: 47.6s\n",
            "474:\tlearn: 71.0521628\ttotal: 1m 11s\tremaining: 47.4s\n",
            "475:\tlearn: 71.0121390\ttotal: 1m 11s\tremaining: 47.3s\n",
            "476:\tlearn: 70.9718612\ttotal: 1m 11s\tremaining: 47.1s\n",
            "477:\tlearn: 70.8114934\ttotal: 1m 11s\tremaining: 47s\n",
            "478:\tlearn: 70.8107858\ttotal: 1m 11s\tremaining: 46.8s\n",
            "479:\tlearn: 70.7801866\ttotal: 1m 11s\tremaining: 46.7s\n",
            "480:\tlearn: 70.7079399\ttotal: 1m 11s\tremaining: 46.5s\n",
            "481:\tlearn: 70.4784133\ttotal: 1m 12s\tremaining: 46.4s\n",
            "482:\tlearn: 70.3629719\ttotal: 1m 12s\tremaining: 46.2s\n",
            "483:\tlearn: 70.3251947\ttotal: 1m 12s\tremaining: 46.1s\n",
            "484:\tlearn: 70.2525605\ttotal: 1m 12s\tremaining: 45.9s\n",
            "485:\tlearn: 70.0142244\ttotal: 1m 12s\tremaining: 45.8s\n",
            "486:\tlearn: 69.9709311\ttotal: 1m 12s\tremaining: 45.6s\n",
            "487:\tlearn: 69.9471180\ttotal: 1m 13s\tremaining: 45.5s\n",
            "488:\tlearn: 69.8211232\ttotal: 1m 13s\tremaining: 45.3s\n",
            "489:\tlearn: 69.7963942\ttotal: 1m 13s\tremaining: 45.2s\n",
            "490:\tlearn: 69.7956150\ttotal: 1m 13s\tremaining: 45.1s\n",
            "491:\tlearn: 69.6925226\ttotal: 1m 13s\tremaining: 44.9s\n",
            "492:\tlearn: 69.6748408\ttotal: 1m 13s\tremaining: 44.8s\n",
            "493:\tlearn: 69.5886053\ttotal: 1m 13s\tremaining: 44.6s\n",
            "494:\tlearn: 69.5415871\ttotal: 1m 14s\tremaining: 44.5s\n",
            "495:\tlearn: 69.4973343\ttotal: 1m 14s\tremaining: 44.3s\n",
            "496:\tlearn: 69.2762945\ttotal: 1m 14s\tremaining: 44.2s\n",
            "497:\tlearn: 69.2289063\ttotal: 1m 14s\tremaining: 44s\n",
            "498:\tlearn: 69.1990859\ttotal: 1m 14s\tremaining: 43.9s\n",
            "499:\tlearn: 69.1750306\ttotal: 1m 14s\tremaining: 43.7s\n",
            "500:\tlearn: 69.1501525\ttotal: 1m 15s\tremaining: 43.6s\n",
            "501:\tlearn: 69.1120847\ttotal: 1m 15s\tremaining: 43.4s\n",
            "502:\tlearn: 69.0548332\ttotal: 1m 15s\tremaining: 43.3s\n",
            "503:\tlearn: 68.9978371\ttotal: 1m 15s\tremaining: 43.1s\n",
            "504:\tlearn: 68.8088272\ttotal: 1m 15s\tremaining: 43s\n",
            "505:\tlearn: 68.7669479\ttotal: 1m 15s\tremaining: 42.8s\n",
            "506:\tlearn: 68.6804046\ttotal: 1m 15s\tremaining: 42.7s\n",
            "507:\tlearn: 68.6326751\ttotal: 1m 16s\tremaining: 42.5s\n",
            "508:\tlearn: 68.6157660\ttotal: 1m 16s\tremaining: 42.4s\n",
            "509:\tlearn: 68.5609704\ttotal: 1m 16s\tremaining: 42.3s\n",
            "510:\tlearn: 68.5382284\ttotal: 1m 16s\tremaining: 42.1s\n",
            "511:\tlearn: 68.5138307\ttotal: 1m 16s\tremaining: 42s\n",
            "512:\tlearn: 68.4585237\ttotal: 1m 16s\tremaining: 41.8s\n",
            "513:\tlearn: 68.2404133\ttotal: 1m 17s\tremaining: 41.7s\n",
            "514:\tlearn: 68.1493098\ttotal: 1m 17s\tremaining: 41.5s\n",
            "515:\tlearn: 68.1059037\ttotal: 1m 17s\tremaining: 41.4s\n",
            "516:\tlearn: 68.0831977\ttotal: 1m 17s\tremaining: 41.2s\n",
            "517:\tlearn: 68.0181853\ttotal: 1m 17s\tremaining: 41.1s\n",
            "518:\tlearn: 67.9821123\ttotal: 1m 17s\tremaining: 40.9s\n",
            "519:\tlearn: 67.9538091\ttotal: 1m 17s\tremaining: 40.8s\n",
            "520:\tlearn: 67.8025979\ttotal: 1m 18s\tremaining: 40.6s\n",
            "521:\tlearn: 67.7297634\ttotal: 1m 18s\tremaining: 40.5s\n",
            "522:\tlearn: 67.6875207\ttotal: 1m 18s\tremaining: 40.3s\n",
            "523:\tlearn: 67.6360405\ttotal: 1m 18s\tremaining: 40.2s\n",
            "524:\tlearn: 67.5343274\ttotal: 1m 18s\tremaining: 40s\n",
            "525:\tlearn: 67.4953206\ttotal: 1m 18s\tremaining: 39.9s\n",
            "526:\tlearn: 67.3462740\ttotal: 1m 19s\tremaining: 39.7s\n",
            "527:\tlearn: 67.3003277\ttotal: 1m 19s\tremaining: 39.6s\n",
            "528:\tlearn: 67.2293263\ttotal: 1m 19s\tremaining: 39.4s\n",
            "529:\tlearn: 67.1608158\ttotal: 1m 19s\tremaining: 39.3s\n",
            "530:\tlearn: 67.1358010\ttotal: 1m 19s\tremaining: 39.1s\n",
            "531:\tlearn: 66.9947506\ttotal: 1m 19s\tremaining: 39s\n",
            "532:\tlearn: 66.9768343\ttotal: 1m 19s\tremaining: 38.8s\n",
            "533:\tlearn: 66.8466582\ttotal: 1m 20s\tremaining: 38.7s\n",
            "534:\tlearn: 66.7778280\ttotal: 1m 20s\tremaining: 38.5s\n",
            "535:\tlearn: 66.6881391\ttotal: 1m 20s\tremaining: 38.4s\n",
            "536:\tlearn: 66.6752683\ttotal: 1m 20s\tremaining: 38.2s\n",
            "537:\tlearn: 66.5858307\ttotal: 1m 20s\tremaining: 38.1s\n",
            "538:\tlearn: 66.5665373\ttotal: 1m 20s\tremaining: 37.9s\n",
            "539:\tlearn: 66.4877652\ttotal: 1m 20s\tremaining: 37.8s\n",
            "540:\tlearn: 66.3927651\ttotal: 1m 21s\tremaining: 37.6s\n",
            "541:\tlearn: 66.2973214\ttotal: 1m 21s\tremaining: 37.5s\n",
            "542:\tlearn: 66.2325578\ttotal: 1m 21s\tremaining: 37.3s\n",
            "543:\tlearn: 66.2064142\ttotal: 1m 21s\tremaining: 37.2s\n",
            "544:\tlearn: 66.1100224\ttotal: 1m 21s\tremaining: 37s\n",
            "545:\tlearn: 66.1084607\ttotal: 1m 21s\tremaining: 36.9s\n",
            "546:\tlearn: 66.0704686\ttotal: 1m 22s\tremaining: 36.8s\n",
            "547:\tlearn: 66.0433948\ttotal: 1m 22s\tremaining: 36.6s\n",
            "548:\tlearn: 66.0077423\ttotal: 1m 22s\tremaining: 36.5s\n",
            "549:\tlearn: 65.9539043\ttotal: 1m 22s\tremaining: 36.3s\n",
            "550:\tlearn: 65.8838248\ttotal: 1m 22s\tremaining: 36.2s\n",
            "551:\tlearn: 65.8095263\ttotal: 1m 22s\tremaining: 36s\n",
            "552:\tlearn: 65.5912784\ttotal: 1m 22s\tremaining: 35.9s\n",
            "553:\tlearn: 65.5381376\ttotal: 1m 23s\tremaining: 35.7s\n",
            "554:\tlearn: 65.5152868\ttotal: 1m 23s\tremaining: 35.6s\n",
            "555:\tlearn: 65.4591637\ttotal: 1m 23s\tremaining: 35.4s\n",
            "556:\tlearn: 65.3470208\ttotal: 1m 23s\tremaining: 35.3s\n",
            "557:\tlearn: 65.2728688\ttotal: 1m 23s\tremaining: 35.1s\n",
            "558:\tlearn: 65.2152219\ttotal: 1m 23s\tremaining: 34.9s\n",
            "559:\tlearn: 65.0983516\ttotal: 1m 23s\tremaining: 34.8s\n",
            "560:\tlearn: 64.9557175\ttotal: 1m 24s\tremaining: 34.6s\n",
            "561:\tlearn: 64.9277968\ttotal: 1m 24s\tremaining: 34.5s\n",
            "562:\tlearn: 64.9080708\ttotal: 1m 24s\tremaining: 34.4s\n",
            "563:\tlearn: 64.8584024\ttotal: 1m 24s\tremaining: 34.2s\n",
            "564:\tlearn: 64.8337468\ttotal: 1m 24s\tremaining: 34.1s\n",
            "565:\tlearn: 64.8145597\ttotal: 1m 24s\tremaining: 33.9s\n",
            "566:\tlearn: 64.7642416\ttotal: 1m 25s\tremaining: 33.8s\n",
            "567:\tlearn: 64.7009259\ttotal: 1m 25s\tremaining: 33.6s\n",
            "568:\tlearn: 64.6328490\ttotal: 1m 25s\tremaining: 33.5s\n",
            "569:\tlearn: 64.3723389\ttotal: 1m 25s\tremaining: 33.3s\n",
            "570:\tlearn: 64.3171092\ttotal: 1m 25s\tremaining: 33.2s\n",
            "571:\tlearn: 64.2669726\ttotal: 1m 25s\tremaining: 33s\n",
            "572:\tlearn: 64.2313185\ttotal: 1m 26s\tremaining: 32.9s\n",
            "573:\tlearn: 64.1143079\ttotal: 1m 26s\tremaining: 32.7s\n",
            "574:\tlearn: 64.0750872\ttotal: 1m 26s\tremaining: 32.6s\n",
            "575:\tlearn: 64.0739909\ttotal: 1m 26s\tremaining: 32.4s\n",
            "576:\tlearn: 63.9882226\ttotal: 1m 26s\tremaining: 32.3s\n",
            "577:\tlearn: 63.8951252\ttotal: 1m 26s\tremaining: 32.1s\n",
            "578:\tlearn: 63.8231264\ttotal: 1m 26s\tremaining: 32s\n",
            "579:\tlearn: 63.6509290\ttotal: 1m 27s\tremaining: 31.8s\n",
            "580:\tlearn: 63.5597414\ttotal: 1m 27s\tremaining: 31.7s\n",
            "581:\tlearn: 63.5146243\ttotal: 1m 27s\tremaining: 31.5s\n",
            "582:\tlearn: 63.4488495\ttotal: 1m 27s\tremaining: 31.4s\n",
            "583:\tlearn: 63.3821708\ttotal: 1m 27s\tremaining: 31.2s\n",
            "584:\tlearn: 63.3289882\ttotal: 1m 27s\tremaining: 31.1s\n",
            "585:\tlearn: 63.2949480\ttotal: 1m 27s\tremaining: 30.9s\n",
            "586:\tlearn: 63.1947783\ttotal: 1m 28s\tremaining: 30.8s\n",
            "587:\tlearn: 63.1648244\ttotal: 1m 28s\tremaining: 30.6s\n",
            "588:\tlearn: 63.0680305\ttotal: 1m 28s\tremaining: 30.5s\n",
            "589:\tlearn: 62.9403196\ttotal: 1m 28s\tremaining: 30.3s\n",
            "590:\tlearn: 62.9207603\ttotal: 1m 28s\tremaining: 30.2s\n",
            "591:\tlearn: 62.8695981\ttotal: 1m 28s\tremaining: 30s\n",
            "592:\tlearn: 62.8188905\ttotal: 1m 29s\tremaining: 29.9s\n",
            "593:\tlearn: 62.7720614\ttotal: 1m 29s\tremaining: 29.7s\n",
            "594:\tlearn: 62.7337453\ttotal: 1m 29s\tremaining: 29.6s\n",
            "595:\tlearn: 62.7223078\ttotal: 1m 29s\tremaining: 29.4s\n",
            "596:\tlearn: 62.6894447\ttotal: 1m 29s\tremaining: 29.3s\n",
            "597:\tlearn: 62.6726844\ttotal: 1m 29s\tremaining: 29.1s\n",
            "598:\tlearn: 62.6250489\ttotal: 1m 30s\tremaining: 29s\n",
            "599:\tlearn: 62.5303688\ttotal: 1m 30s\tremaining: 28.9s\n",
            "600:\tlearn: 62.5193631\ttotal: 1m 30s\tremaining: 28.7s\n",
            "601:\tlearn: 62.4888299\ttotal: 1m 30s\tremaining: 28.6s\n",
            "602:\tlearn: 62.4657141\ttotal: 1m 30s\tremaining: 28.4s\n",
            "603:\tlearn: 62.4318199\ttotal: 1m 30s\tremaining: 28.3s\n",
            "604:\tlearn: 62.4146547\ttotal: 1m 30s\tremaining: 28.1s\n",
            "605:\tlearn: 62.2002559\ttotal: 1m 31s\tremaining: 28s\n",
            "606:\tlearn: 62.1538250\ttotal: 1m 31s\tremaining: 27.8s\n",
            "607:\tlearn: 62.1348974\ttotal: 1m 31s\tremaining: 27.7s\n",
            "608:\tlearn: 62.1164705\ttotal: 1m 31s\tremaining: 27.5s\n",
            "609:\tlearn: 62.0453240\ttotal: 1m 31s\tremaining: 27.4s\n",
            "610:\tlearn: 62.0035492\ttotal: 1m 31s\tremaining: 27.2s\n",
            "611:\tlearn: 61.9656195\ttotal: 1m 32s\tremaining: 27.1s\n",
            "612:\tlearn: 61.9228726\ttotal: 1m 32s\tremaining: 26.9s\n",
            "613:\tlearn: 61.8584799\ttotal: 1m 32s\tremaining: 26.8s\n",
            "614:\tlearn: 61.7683886\ttotal: 1m 32s\tremaining: 26.6s\n",
            "615:\tlearn: 61.7050312\ttotal: 1m 32s\tremaining: 26.5s\n",
            "616:\tlearn: 61.6664991\ttotal: 1m 32s\tremaining: 26.3s\n",
            "617:\tlearn: 61.6361800\ttotal: 1m 32s\tremaining: 26.2s\n",
            "618:\tlearn: 61.5395575\ttotal: 1m 33s\tremaining: 26s\n",
            "619:\tlearn: 61.5241631\ttotal: 1m 33s\tremaining: 25.9s\n",
            "620:\tlearn: 61.4825976\ttotal: 1m 33s\tremaining: 25.7s\n",
            "621:\tlearn: 61.4804631\ttotal: 1m 33s\tremaining: 25.6s\n",
            "622:\tlearn: 61.4426674\ttotal: 1m 33s\tremaining: 25.4s\n",
            "623:\tlearn: 61.3288137\ttotal: 1m 33s\tremaining: 25.3s\n",
            "624:\tlearn: 61.2098345\ttotal: 1m 33s\tremaining: 25.1s\n",
            "625:\tlearn: 61.1115458\ttotal: 1m 34s\tremaining: 25s\n",
            "626:\tlearn: 61.0797924\ttotal: 1m 34s\tremaining: 24.8s\n",
            "627:\tlearn: 61.0465758\ttotal: 1m 34s\tremaining: 24.7s\n",
            "628:\tlearn: 60.9643590\ttotal: 1m 34s\tremaining: 24.5s\n",
            "629:\tlearn: 60.9146124\ttotal: 1m 34s\tremaining: 24.4s\n",
            "630:\tlearn: 60.8774540\ttotal: 1m 34s\tremaining: 24.2s\n",
            "631:\tlearn: 60.7684066\ttotal: 1m 35s\tremaining: 24.1s\n",
            "632:\tlearn: 60.7284830\ttotal: 1m 35s\tremaining: 23.9s\n",
            "633:\tlearn: 60.6961272\ttotal: 1m 35s\tremaining: 23.8s\n",
            "634:\tlearn: 60.5773853\ttotal: 1m 35s\tremaining: 23.6s\n",
            "635:\tlearn: 60.5408047\ttotal: 1m 35s\tremaining: 23.5s\n",
            "636:\tlearn: 60.5269213\ttotal: 1m 35s\tremaining: 23.3s\n",
            "637:\tlearn: 60.4812002\ttotal: 1m 35s\tremaining: 23.2s\n",
            "638:\tlearn: 60.4308167\ttotal: 1m 36s\tremaining: 23s\n",
            "639:\tlearn: 60.4109097\ttotal: 1m 36s\tremaining: 22.9s\n",
            "640:\tlearn: 60.3829123\ttotal: 1m 36s\tremaining: 22.7s\n",
            "641:\tlearn: 60.3528786\ttotal: 1m 36s\tremaining: 22.6s\n",
            "642:\tlearn: 60.3250400\ttotal: 1m 36s\tremaining: 22.4s\n",
            "643:\tlearn: 60.2824621\ttotal: 1m 36s\tremaining: 22.3s\n",
            "644:\tlearn: 60.2631550\ttotal: 1m 37s\tremaining: 22.1s\n",
            "645:\tlearn: 60.1860704\ttotal: 1m 37s\tremaining: 22s\n",
            "646:\tlearn: 60.1703842\ttotal: 1m 37s\tremaining: 21.8s\n",
            "647:\tlearn: 60.1299593\ttotal: 1m 37s\tremaining: 21.7s\n",
            "648:\tlearn: 60.0608565\ttotal: 1m 37s\tremaining: 21.5s\n",
            "649:\tlearn: 60.0355031\ttotal: 1m 37s\tremaining: 21.4s\n",
            "650:\tlearn: 59.9838783\ttotal: 1m 37s\tremaining: 21.2s\n",
            "651:\tlearn: 59.9415225\ttotal: 1m 38s\tremaining: 21.1s\n",
            "652:\tlearn: 59.9274380\ttotal: 1m 38s\tremaining: 20.9s\n",
            "653:\tlearn: 59.8672531\ttotal: 1m 38s\tremaining: 20.8s\n",
            "654:\tlearn: 59.8667785\ttotal: 1m 38s\tremaining: 20.6s\n",
            "655:\tlearn: 59.8512061\ttotal: 1m 38s\tremaining: 20.5s\n",
            "656:\tlearn: 59.8253108\ttotal: 1m 38s\tremaining: 20.3s\n",
            "657:\tlearn: 59.8015867\ttotal: 1m 39s\tremaining: 20.2s\n",
            "658:\tlearn: 59.7194894\ttotal: 1m 39s\tremaining: 20s\n",
            "659:\tlearn: 59.6772114\ttotal: 1m 39s\tremaining: 19.9s\n",
            "660:\tlearn: 59.6466313\ttotal: 1m 39s\tremaining: 19.7s\n",
            "661:\tlearn: 59.5997519\ttotal: 1m 39s\tremaining: 19.6s\n",
            "662:\tlearn: 59.5823948\ttotal: 1m 39s\tremaining: 19.4s\n",
            "663:\tlearn: 59.5064813\ttotal: 1m 39s\tremaining: 19.3s\n",
            "664:\tlearn: 59.4929095\ttotal: 1m 40s\tremaining: 19.1s\n",
            "665:\tlearn: 59.3843621\ttotal: 1m 40s\tremaining: 19s\n",
            "666:\tlearn: 59.3661954\ttotal: 1m 40s\tremaining: 18.8s\n",
            "667:\tlearn: 59.2796795\ttotal: 1m 40s\tremaining: 18.7s\n",
            "668:\tlearn: 59.2505295\ttotal: 1m 40s\tremaining: 18.5s\n",
            "669:\tlearn: 59.1856755\ttotal: 1m 40s\tremaining: 18.3s\n",
            "670:\tlearn: 59.1663057\ttotal: 1m 40s\tremaining: 18.2s\n",
            "671:\tlearn: 59.1379062\ttotal: 1m 41s\tremaining: 18s\n",
            "672:\tlearn: 59.0857343\ttotal: 1m 41s\tremaining: 17.9s\n",
            "673:\tlearn: 59.0345739\ttotal: 1m 41s\tremaining: 17.7s\n",
            "674:\tlearn: 59.0165295\ttotal: 1m 41s\tremaining: 17.6s\n",
            "675:\tlearn: 59.0042328\ttotal: 1m 41s\tremaining: 17.4s\n",
            "676:\tlearn: 58.9784965\ttotal: 1m 41s\tremaining: 17.3s\n",
            "677:\tlearn: 58.8972944\ttotal: 1m 41s\tremaining: 17.1s\n",
            "678:\tlearn: 58.8544425\ttotal: 1m 42s\tremaining: 17s\n",
            "679:\tlearn: 58.8089180\ttotal: 1m 42s\tremaining: 16.8s\n",
            "680:\tlearn: 58.7817486\ttotal: 1m 42s\tremaining: 16.7s\n",
            "681:\tlearn: 58.7475033\ttotal: 1m 42s\tremaining: 16.5s\n",
            "682:\tlearn: 58.6780441\ttotal: 1m 42s\tremaining: 16.4s\n",
            "683:\tlearn: 58.6518705\ttotal: 1m 42s\tremaining: 16.2s\n",
            "684:\tlearn: 58.6119065\ttotal: 1m 43s\tremaining: 16.1s\n",
            "685:\tlearn: 58.5327653\ttotal: 1m 43s\tremaining: 15.9s\n",
            "686:\tlearn: 58.4302607\ttotal: 1m 43s\tremaining: 15.8s\n",
            "687:\tlearn: 58.4063456\ttotal: 1m 43s\tremaining: 15.6s\n",
            "688:\tlearn: 58.3571522\ttotal: 1m 43s\tremaining: 15.5s\n",
            "689:\tlearn: 58.2978310\ttotal: 1m 43s\tremaining: 15.3s\n",
            "690:\tlearn: 58.2384520\ttotal: 1m 43s\tremaining: 15.2s\n",
            "691:\tlearn: 58.1591973\ttotal: 1m 44s\tremaining: 15s\n",
            "692:\tlearn: 57.9539144\ttotal: 1m 44s\tremaining: 14.9s\n",
            "693:\tlearn: 57.9229674\ttotal: 1m 44s\tremaining: 14.7s\n",
            "694:\tlearn: 57.8920378\ttotal: 1m 44s\tremaining: 14.6s\n",
            "695:\tlearn: 57.8450812\ttotal: 1m 44s\tremaining: 14.4s\n",
            "696:\tlearn: 57.8120298\ttotal: 1m 44s\tremaining: 14.3s\n",
            "697:\tlearn: 57.7484224\ttotal: 1m 44s\tremaining: 14.1s\n",
            "698:\tlearn: 57.7360015\ttotal: 1m 45s\tremaining: 14s\n",
            "699:\tlearn: 57.7126198\ttotal: 1m 45s\tremaining: 13.8s\n",
            "700:\tlearn: 57.6792203\ttotal: 1m 45s\tremaining: 13.7s\n",
            "701:\tlearn: 57.6347351\ttotal: 1m 45s\tremaining: 13.5s\n",
            "702:\tlearn: 57.5832070\ttotal: 1m 45s\tremaining: 13.4s\n",
            "703:\tlearn: 57.5406867\ttotal: 1m 45s\tremaining: 13.2s\n",
            "704:\tlearn: 57.5156240\ttotal: 1m 46s\tremaining: 13.1s\n",
            "705:\tlearn: 57.4346694\ttotal: 1m 46s\tremaining: 12.9s\n",
            "706:\tlearn: 57.3538091\ttotal: 1m 46s\tremaining: 12.8s\n",
            "707:\tlearn: 57.3362635\ttotal: 1m 46s\tremaining: 12.6s\n",
            "708:\tlearn: 57.2890571\ttotal: 1m 46s\tremaining: 12.5s\n",
            "709:\tlearn: 57.2560761\ttotal: 1m 46s\tremaining: 12.3s\n",
            "710:\tlearn: 57.2147830\ttotal: 1m 46s\tremaining: 12.2s\n",
            "711:\tlearn: 57.1581585\ttotal: 1m 47s\tremaining: 12s\n",
            "712:\tlearn: 57.0934106\ttotal: 1m 47s\tremaining: 11.9s\n",
            "713:\tlearn: 57.0696193\ttotal: 1m 47s\tremaining: 11.7s\n",
            "714:\tlearn: 57.0346079\ttotal: 1m 47s\tremaining: 11.6s\n",
            "715:\tlearn: 56.9472049\ttotal: 1m 47s\tremaining: 11.4s\n",
            "716:\tlearn: 56.9355978\ttotal: 1m 47s\tremaining: 11.3s\n",
            "717:\tlearn: 56.8498456\ttotal: 1m 47s\tremaining: 11.1s\n",
            "718:\tlearn: 56.8210261\ttotal: 1m 48s\tremaining: 11s\n",
            "719:\tlearn: 56.7870798\ttotal: 1m 48s\tremaining: 10.8s\n",
            "720:\tlearn: 56.7223674\ttotal: 1m 48s\tremaining: 10.7s\n",
            "721:\tlearn: 56.6808196\ttotal: 1m 48s\tremaining: 10.5s\n",
            "722:\tlearn: 56.6589115\ttotal: 1m 48s\tremaining: 10.4s\n",
            "723:\tlearn: 56.6210889\ttotal: 1m 48s\tremaining: 10.2s\n",
            "724:\tlearn: 56.5844718\ttotal: 1m 49s\tremaining: 10.1s\n",
            "725:\tlearn: 56.4734088\ttotal: 1m 49s\tremaining: 9.92s\n",
            "726:\tlearn: 56.4530539\ttotal: 1m 49s\tremaining: 9.78s\n",
            "727:\tlearn: 56.4070583\ttotal: 1m 49s\tremaining: 9.63s\n",
            "728:\tlearn: 56.3197862\ttotal: 1m 49s\tremaining: 9.47s\n",
            "729:\tlearn: 56.2780683\ttotal: 1m 49s\tremaining: 9.32s\n",
            "730:\tlearn: 56.2763798\ttotal: 1m 49s\tremaining: 9.17s\n",
            "731:\tlearn: 56.0948157\ttotal: 1m 50s\tremaining: 9.02s\n",
            "732:\tlearn: 56.0788425\ttotal: 1m 50s\tremaining: 8.87s\n",
            "733:\tlearn: 56.0396974\ttotal: 1m 50s\tremaining: 8.72s\n",
            "734:\tlearn: 56.0076756\ttotal: 1m 50s\tremaining: 8.57s\n",
            "735:\tlearn: 55.9901515\ttotal: 1m 50s\tremaining: 8.42s\n",
            "736:\tlearn: 55.9548496\ttotal: 1m 50s\tremaining: 8.27s\n",
            "737:\tlearn: 55.9366282\ttotal: 1m 51s\tremaining: 8.13s\n",
            "738:\tlearn: 55.9112734\ttotal: 1m 51s\tremaining: 7.98s\n",
            "739:\tlearn: 55.8253201\ttotal: 1m 51s\tremaining: 7.83s\n",
            "740:\tlearn: 55.7318036\ttotal: 1m 51s\tremaining: 7.67s\n",
            "741:\tlearn: 55.6360196\ttotal: 1m 51s\tremaining: 7.53s\n",
            "742:\tlearn: 55.5928491\ttotal: 1m 51s\tremaining: 7.37s\n",
            "743:\tlearn: 55.5759645\ttotal: 1m 51s\tremaining: 7.22s\n",
            "744:\tlearn: 55.5419556\ttotal: 1m 52s\tremaining: 7.07s\n",
            "745:\tlearn: 55.4849999\ttotal: 1m 52s\tremaining: 6.92s\n",
            "746:\tlearn: 55.4593454\ttotal: 1m 52s\tremaining: 6.77s\n",
            "747:\tlearn: 55.4344739\ttotal: 1m 52s\tremaining: 6.62s\n",
            "748:\tlearn: 55.3462059\ttotal: 1m 52s\tremaining: 6.47s\n",
            "749:\tlearn: 55.3060255\ttotal: 1m 52s\tremaining: 6.32s\n",
            "750:\tlearn: 55.2683837\ttotal: 1m 53s\tremaining: 6.17s\n",
            "751:\tlearn: 55.2370368\ttotal: 1m 53s\tremaining: 6.02s\n",
            "752:\tlearn: 55.2037808\ttotal: 1m 53s\tremaining: 5.87s\n",
            "753:\tlearn: 55.1671962\ttotal: 1m 53s\tremaining: 5.72s\n",
            "754:\tlearn: 55.1344308\ttotal: 1m 53s\tremaining: 5.57s\n",
            "755:\tlearn: 55.0434004\ttotal: 1m 53s\tremaining: 5.42s\n",
            "756:\tlearn: 54.9528650\ttotal: 1m 53s\tremaining: 5.27s\n",
            "757:\tlearn: 54.9247260\ttotal: 1m 54s\tremaining: 5.12s\n",
            "758:\tlearn: 54.8892476\ttotal: 1m 54s\tremaining: 4.96s\n",
            "759:\tlearn: 54.8480123\ttotal: 1m 54s\tremaining: 4.81s\n",
            "760:\tlearn: 54.7989190\ttotal: 1m 54s\tremaining: 4.66s\n",
            "761:\tlearn: 54.7250160\ttotal: 1m 54s\tremaining: 4.51s\n",
            "762:\tlearn: 54.7005795\ttotal: 1m 54s\tremaining: 4.36s\n",
            "763:\tlearn: 54.6726342\ttotal: 1m 54s\tremaining: 4.21s\n",
            "764:\tlearn: 54.5991405\ttotal: 1m 55s\tremaining: 4.06s\n",
            "765:\tlearn: 54.5874012\ttotal: 1m 55s\tremaining: 3.91s\n",
            "766:\tlearn: 54.5433206\ttotal: 1m 55s\tremaining: 3.76s\n",
            "767:\tlearn: 54.4812856\ttotal: 1m 55s\tremaining: 3.61s\n",
            "768:\tlearn: 54.4168004\ttotal: 1m 55s\tremaining: 3.46s\n",
            "769:\tlearn: 54.3710377\ttotal: 1m 55s\tremaining: 3.31s\n",
            "770:\tlearn: 54.3277668\ttotal: 1m 55s\tremaining: 3.16s\n",
            "771:\tlearn: 54.3048673\ttotal: 1m 56s\tremaining: 3.01s\n",
            "772:\tlearn: 54.2577509\ttotal: 1m 56s\tremaining: 2.86s\n",
            "773:\tlearn: 54.1802684\ttotal: 1m 56s\tremaining: 2.71s\n",
            "774:\tlearn: 54.1326787\ttotal: 1m 56s\tremaining: 2.56s\n",
            "775:\tlearn: 54.0830190\ttotal: 1m 56s\tremaining: 2.41s\n",
            "776:\tlearn: 54.0552940\ttotal: 1m 56s\tremaining: 2.26s\n",
            "777:\tlearn: 54.0304038\ttotal: 1m 57s\tremaining: 2.11s\n",
            "778:\tlearn: 53.9917166\ttotal: 1m 57s\tremaining: 1.96s\n",
            "779:\tlearn: 53.9722685\ttotal: 1m 57s\tremaining: 1.8s\n",
            "780:\tlearn: 53.9033459\ttotal: 1m 57s\tremaining: 1.66s\n",
            "781:\tlearn: 53.8664348\ttotal: 1m 57s\tremaining: 1.5s\n",
            "782:\tlearn: 53.8228746\ttotal: 1m 57s\tremaining: 1.35s\n",
            "783:\tlearn: 53.7858978\ttotal: 1m 58s\tremaining: 1.2s\n",
            "784:\tlearn: 53.7710718\ttotal: 1m 58s\tremaining: 1.05s\n",
            "785:\tlearn: 53.6461301\ttotal: 1m 58s\tremaining: 903ms\n",
            "786:\tlearn: 53.6058188\ttotal: 1m 58s\tremaining: 753ms\n",
            "787:\tlearn: 53.5699031\ttotal: 1m 58s\tremaining: 602ms\n",
            "788:\tlearn: 53.5381477\ttotal: 1m 58s\tremaining: 452ms\n",
            "789:\tlearn: 53.4968932\ttotal: 1m 58s\tremaining: 301ms\n",
            "790:\tlearn: 53.4747725\ttotal: 1m 59s\tremaining: 151ms\n",
            "791:\tlearn: 53.4572627\ttotal: 1m 59s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 04:00:53,005]\u001b[0m A new study created in memory with name: no-name-52a19541-ec62-42c8-8ecc-c0e0a26c940c\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 04:02:52,475]\u001b[0m Trial 0 finished with value: 51.7893636644697 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 51.7893636644697.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:04:56,646]\u001b[0m Trial 1 finished with value: 47.890309680958346 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 47.890309680958346.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:06:59,321]\u001b[0m Trial 2 finished with value: 46.47266476434371 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 2 with value: 46.47266476434371.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:20:05,794]\u001b[0m Trial 3 finished with value: 76.818324755595 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 2 with value: 46.47266476434371.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:22:10,930]\u001b[0m Trial 4 finished with value: 55.521883975037014 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 2 with value: 46.47266476434371.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:39:10,617]\u001b[0m Trial 5 finished with value: 66.89882463877937 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 2 with value: 46.47266476434371.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:42:16,969]\u001b[0m Trial 6 finished with value: 44.76931509209572 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:45:16,453]\u001b[0m Trial 7 finished with value: 49.37052039590518 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 04:46:52,092]\u001b[0m Trial 8 finished with value: 68.84913884673526 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:06:49,587]\u001b[0m Trial 9 finished with value: 56.34296597184787 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:07:51,594]\u001b[0m Trial 10 finished with value: 80.2135405424639 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:09:23,997]\u001b[0m Trial 11 finished with value: 57.11747038357349 and parameters: {'iterations': 629, 'learning_rate': 0.2764615591885931, 'depth': 6, 'min_data_in_leaf': 6, 'reg_lambda': 98.40975370477312, 'subsample': 0.6137915450890752, 'random_strength': 13.607559365869914, 'od_wait': 12, 'leaf_estimation_iterations': 13, 'bagging_temperature': 99.66177975539146, 'colsample_bylevel': 0.6037888285524304}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:13:05,127]\u001b[0m Trial 12 finished with value: 48.80378894553733 and parameters: {'iterations': 312, 'learning_rate': 0.3353168481850217, 'depth': 12, 'min_data_in_leaf': 1, 'reg_lambda': 83.3833616301794, 'subsample': 0.6018370841486229, 'random_strength': 19.02310110968608, 'od_wait': 11, 'leaf_estimation_iterations': 15, 'bagging_temperature': 2.2397968272557836, 'colsample_bylevel': 0.6798103370630707}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:15:26,415]\u001b[0m Trial 13 finished with value: 45.73475134979331 and parameters: {'iterations': 768, 'learning_rate': 0.39733087730033134, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 51.709612487085614, 'subsample': 0.8428965893650274, 'random_strength': 68.23293339853619, 'od_wait': 31, 'leaf_estimation_iterations': 10, 'bagging_temperature': 13.355307798299567, 'colsample_bylevel': 0.7317116575399958}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:17:25,048]\u001b[0m Trial 14 finished with value: 63.4674509634417 and parameters: {'iterations': 741, 'learning_rate': 0.12115442835598422, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 51.397128917456506, 'subsample': 0.879970061817357, 'random_strength': 74.51101113965332, 'od_wait': 94, 'leaf_estimation_iterations': 10, 'bagging_temperature': 39.38019209537576, 'colsample_bylevel': 0.5241786552111687}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:18:38,817]\u001b[0m Trial 15 finished with value: 63.7197746479203 and parameters: {'iterations': 815, 'learning_rate': 0.32308158949960997, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 49.37245683486181, 'subsample': 0.9930815447318664, 'random_strength': 66.99328267466255, 'od_wait': 41, 'leaf_estimation_iterations': 13, 'bagging_temperature': 14.566066853374414, 'colsample_bylevel': 0.061943926372203806}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:19:56,582]\u001b[0m Trial 16 finished with value: 66.69283825760108 and parameters: {'iterations': 592, 'learning_rate': 0.2407235903215652, 'depth': 5, 'min_data_in_leaf': 8, 'reg_lambda': 62.839769611979925, 'subsample': 0.8595044436039624, 'random_strength': 98.23074027293896, 'od_wait': 31, 'leaf_estimation_iterations': 8, 'bagging_temperature': 20.16763561176596, 'colsample_bylevel': 0.7715955209093116}. Best is trial 6 with value: 44.76931509209572.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:22:25,795]\u001b[0m Trial 17 finished with value: 42.99479569369386 and parameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}. Best is trial 17 with value: 42.99479569369386.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:30:39,371]\u001b[0m Trial 18 finished with value: 51.47277960627142 and parameters: {'iterations': 838, 'learning_rate': 0.4270062418190881, 'depth': 12, 'min_data_in_leaf': 13, 'reg_lambda': 40.65971705190806, 'subsample': 0.6934883378818296, 'random_strength': 87.60982739927447, 'od_wait': 86, 'leaf_estimation_iterations': 15, 'bagging_temperature': 55.57442205671075, 'colsample_bylevel': 0.22089840378700903}. Best is trial 17 with value: 42.99479569369386.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:33:53,286]\u001b[0m Trial 19 finished with value: 44.85393140095243 and parameters: {'iterations': 996, 'learning_rate': 0.18484933160405892, 'depth': 8, 'min_data_in_leaf': 20, 'reg_lambda': 42.564868866728915, 'subsample': 0.729625123932983, 'random_strength': 45.629127031175706, 'od_wait': 63, 'leaf_estimation_iterations': 17, 'bagging_temperature': 50.428094719702216, 'colsample_bylevel': 0.46751740730087066}. Best is trial 17 with value: 42.99479569369386.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 4: 42.99479569369386\n",
            "Best hyperparameters for fold 4: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "0:\tlearn: 207.5462183\ttotal: 148ms\tremaining: 1m 56s\n",
            "1:\tlearn: 206.2793843\ttotal: 235ms\tremaining: 1m 32s\n",
            "2:\tlearn: 206.2774985\ttotal: 287ms\tremaining: 1m 15s\n",
            "3:\tlearn: 204.7584634\ttotal: 371ms\tremaining: 1m 13s\n",
            "4:\tlearn: 204.1595926\ttotal: 541ms\tremaining: 1m 25s\n",
            "5:\tlearn: 204.1418121\ttotal: 604ms\tremaining: 1m 19s\n",
            "6:\tlearn: 204.1093265\ttotal: 700ms\tremaining: 1m 18s\n",
            "7:\tlearn: 202.6380865\ttotal: 845ms\tremaining: 1m 22s\n",
            "8:\tlearn: 202.1132612\ttotal: 1.02s\tremaining: 1m 28s\n",
            "9:\tlearn: 201.9492653\ttotal: 1.1s\tremaining: 1m 26s\n",
            "10:\tlearn: 201.9431037\ttotal: 1.16s\tremaining: 1m 22s\n",
            "11:\tlearn: 201.9431037\ttotal: 1.21s\tremaining: 1m 18s\n",
            "12:\tlearn: 201.7805569\ttotal: 1.38s\tremaining: 1m 22s\n",
            "13:\tlearn: 201.7276653\ttotal: 1.45s\tremaining: 1m 20s\n",
            "14:\tlearn: 201.6003051\ttotal: 1.61s\tremaining: 1m 23s\n",
            "15:\tlearn: 201.0473364\ttotal: 1.76s\tremaining: 1m 25s\n",
            "16:\tlearn: 200.8695627\ttotal: 1.94s\tremaining: 1m 28s\n",
            "17:\tlearn: 200.7557947\ttotal: 2.02s\tremaining: 1m 27s\n",
            "18:\tlearn: 200.7245038\ttotal: 2.19s\tremaining: 1m 29s\n",
            "19:\tlearn: 200.6383023\ttotal: 2.33s\tremaining: 1m 29s\n",
            "20:\tlearn: 200.4611226\ttotal: 2.5s\tremaining: 1m 31s\n",
            "21:\tlearn: 200.1483520\ttotal: 2.67s\tremaining: 1m 33s\n",
            "22:\tlearn: 200.1354411\ttotal: 2.77s\tremaining: 1m 32s\n",
            "23:\tlearn: 199.8170020\ttotal: 2.92s\tremaining: 1m 33s\n",
            "24:\tlearn: 199.3498327\ttotal: 3.09s\tremaining: 1m 34s\n",
            "25:\tlearn: 199.2248663\ttotal: 3.26s\tremaining: 1m 36s\n",
            "26:\tlearn: 199.2248663\ttotal: 3.31s\tremaining: 1m 33s\n",
            "27:\tlearn: 198.9408717\ttotal: 3.47s\tremaining: 1m 34s\n",
            "28:\tlearn: 198.8351729\ttotal: 3.63s\tremaining: 1m 35s\n",
            "29:\tlearn: 198.8351695\ttotal: 3.68s\tremaining: 1m 33s\n",
            "30:\tlearn: 198.7721750\ttotal: 3.81s\tremaining: 1m 33s\n",
            "31:\tlearn: 198.6799692\ttotal: 3.98s\tremaining: 1m 34s\n",
            "32:\tlearn: 198.2863211\ttotal: 4.14s\tremaining: 1m 35s\n",
            "33:\tlearn: 198.2052423\ttotal: 4.23s\tremaining: 1m 34s\n",
            "34:\tlearn: 198.0718448\ttotal: 4.33s\tremaining: 1m 33s\n",
            "35:\tlearn: 197.3348876\ttotal: 4.4s\tremaining: 1m 32s\n",
            "36:\tlearn: 196.6597088\ttotal: 4.51s\tremaining: 1m 32s\n",
            "37:\tlearn: 194.8084120\ttotal: 4.67s\tremaining: 1m 32s\n",
            "38:\tlearn: 193.6421588\ttotal: 4.83s\tremaining: 1m 33s\n",
            "39:\tlearn: 192.8883522\ttotal: 4.96s\tremaining: 1m 33s\n",
            "40:\tlearn: 192.0891796\ttotal: 5.12s\tremaining: 1m 33s\n",
            "41:\tlearn: 190.5702000\ttotal: 5.26s\tremaining: 1m 33s\n",
            "42:\tlearn: 189.1541167\ttotal: 5.4s\tremaining: 1m 34s\n",
            "43:\tlearn: 188.5088820\ttotal: 5.55s\tremaining: 1m 34s\n",
            "44:\tlearn: 185.7162432\ttotal: 5.71s\tremaining: 1m 34s\n",
            "45:\tlearn: 185.0355572\ttotal: 5.85s\tremaining: 1m 34s\n",
            "46:\tlearn: 184.3849653\ttotal: 6.01s\tremaining: 1m 35s\n",
            "47:\tlearn: 183.9684108\ttotal: 6.17s\tremaining: 1m 35s\n",
            "48:\tlearn: 182.4375774\ttotal: 6.31s\tremaining: 1m 35s\n",
            "49:\tlearn: 181.1160130\ttotal: 6.48s\tremaining: 1m 36s\n",
            "50:\tlearn: 180.4135666\ttotal: 6.63s\tremaining: 1m 36s\n",
            "51:\tlearn: 179.9923688\ttotal: 6.78s\tremaining: 1m 36s\n",
            "52:\tlearn: 178.8088927\ttotal: 6.93s\tremaining: 1m 36s\n",
            "53:\tlearn: 175.8334264\ttotal: 7.08s\tremaining: 1m 36s\n",
            "54:\tlearn: 175.4143278\ttotal: 7.23s\tremaining: 1m 36s\n",
            "55:\tlearn: 174.1932464\ttotal: 7.37s\tremaining: 1m 36s\n",
            "56:\tlearn: 172.5426550\ttotal: 7.5s\tremaining: 1m 36s\n",
            "57:\tlearn: 170.3574276\ttotal: 7.66s\tremaining: 1m 36s\n",
            "58:\tlearn: 169.3762980\ttotal: 7.8s\tremaining: 1m 36s\n",
            "59:\tlearn: 167.1019145\ttotal: 7.94s\tremaining: 1m 36s\n",
            "60:\tlearn: 166.7321771\ttotal: 8.1s\tremaining: 1m 37s\n",
            "61:\tlearn: 164.1894266\ttotal: 8.23s\tremaining: 1m 36s\n",
            "62:\tlearn: 162.1737440\ttotal: 8.37s\tremaining: 1m 36s\n",
            "63:\tlearn: 161.6068300\ttotal: 8.53s\tremaining: 1m 36s\n",
            "64:\tlearn: 161.2873708\ttotal: 8.7s\tremaining: 1m 37s\n",
            "65:\tlearn: 159.7644790\ttotal: 8.87s\tremaining: 1m 37s\n",
            "66:\tlearn: 159.4411468\ttotal: 9.03s\tremaining: 1m 37s\n",
            "67:\tlearn: 157.3687971\ttotal: 9.17s\tremaining: 1m 37s\n",
            "68:\tlearn: 156.9700727\ttotal: 9.29s\tremaining: 1m 37s\n",
            "69:\tlearn: 156.7597518\ttotal: 9.45s\tremaining: 1m 37s\n",
            "70:\tlearn: 155.8871650\ttotal: 9.6s\tremaining: 1m 37s\n",
            "71:\tlearn: 154.4040132\ttotal: 9.75s\tremaining: 1m 37s\n",
            "72:\tlearn: 153.8230585\ttotal: 9.91s\tremaining: 1m 37s\n",
            "73:\tlearn: 153.1784035\ttotal: 10.1s\tremaining: 1m 37s\n",
            "74:\tlearn: 152.8869159\ttotal: 10.2s\tremaining: 1m 37s\n",
            "75:\tlearn: 151.3246873\ttotal: 10.4s\tremaining: 1m 37s\n",
            "76:\tlearn: 150.4802671\ttotal: 10.5s\tremaining: 1m 37s\n",
            "77:\tlearn: 150.2805027\ttotal: 10.7s\tremaining: 1m 37s\n",
            "78:\tlearn: 150.1486663\ttotal: 10.8s\tremaining: 1m 37s\n",
            "79:\tlearn: 150.0656522\ttotal: 11s\tremaining: 1m 37s\n",
            "80:\tlearn: 149.7706518\ttotal: 11.1s\tremaining: 1m 37s\n",
            "81:\tlearn: 149.6609064\ttotal: 11.3s\tremaining: 1m 37s\n",
            "82:\tlearn: 149.2600367\ttotal: 11.4s\tremaining: 1m 37s\n",
            "83:\tlearn: 148.0045997\ttotal: 11.6s\tremaining: 1m 37s\n",
            "84:\tlearn: 147.7883968\ttotal: 11.8s\tremaining: 1m 37s\n",
            "85:\tlearn: 147.4656667\ttotal: 11.9s\tremaining: 1m 38s\n",
            "86:\tlearn: 147.2195432\ttotal: 12.1s\tremaining: 1m 38s\n",
            "87:\tlearn: 145.7592730\ttotal: 12.3s\tremaining: 1m 38s\n",
            "88:\tlearn: 145.4749549\ttotal: 12.4s\tremaining: 1m 38s\n",
            "89:\tlearn: 144.9949157\ttotal: 12.6s\tremaining: 1m 38s\n",
            "90:\tlearn: 144.9570511\ttotal: 12.7s\tremaining: 1m 38s\n",
            "91:\tlearn: 144.1834087\ttotal: 12.9s\tremaining: 1m 38s\n",
            "92:\tlearn: 143.4052038\ttotal: 13s\tremaining: 1m 37s\n",
            "93:\tlearn: 143.1618193\ttotal: 13.2s\tremaining: 1m 37s\n",
            "94:\tlearn: 142.6011885\ttotal: 13.3s\tremaining: 1m 37s\n",
            "95:\tlearn: 141.0298550\ttotal: 13.5s\tremaining: 1m 37s\n",
            "96:\tlearn: 140.0791656\ttotal: 13.6s\tremaining: 1m 37s\n",
            "97:\tlearn: 139.8654670\ttotal: 13.8s\tremaining: 1m 37s\n",
            "98:\tlearn: 139.6526450\ttotal: 13.9s\tremaining: 1m 37s\n",
            "99:\tlearn: 138.6596989\ttotal: 14.1s\tremaining: 1m 37s\n",
            "100:\tlearn: 136.7394058\ttotal: 14.2s\tremaining: 1m 37s\n",
            "101:\tlearn: 136.2401101\ttotal: 14.4s\tremaining: 1m 37s\n",
            "102:\tlearn: 135.9783857\ttotal: 14.5s\tremaining: 1m 37s\n",
            "103:\tlearn: 135.4440864\ttotal: 14.7s\tremaining: 1m 37s\n",
            "104:\tlearn: 135.2442035\ttotal: 14.9s\tremaining: 1m 37s\n",
            "105:\tlearn: 134.5833829\ttotal: 15s\tremaining: 1m 37s\n",
            "106:\tlearn: 133.9280251\ttotal: 15.1s\tremaining: 1m 36s\n",
            "107:\tlearn: 133.1157807\ttotal: 15.3s\tremaining: 1m 36s\n",
            "108:\tlearn: 132.8651380\ttotal: 15.4s\tremaining: 1m 36s\n",
            "109:\tlearn: 132.6927822\ttotal: 15.6s\tremaining: 1m 36s\n",
            "110:\tlearn: 132.3454401\ttotal: 15.7s\tremaining: 1m 36s\n",
            "111:\tlearn: 131.1321992\ttotal: 15.9s\tremaining: 1m 36s\n",
            "112:\tlearn: 130.9107680\ttotal: 16s\tremaining: 1m 36s\n",
            "113:\tlearn: 130.3050587\ttotal: 16.2s\tremaining: 1m 36s\n",
            "114:\tlearn: 130.0297226\ttotal: 16.3s\tremaining: 1m 36s\n",
            "115:\tlearn: 129.3432868\ttotal: 16.5s\tremaining: 1m 35s\n",
            "116:\tlearn: 129.0524138\ttotal: 16.6s\tremaining: 1m 35s\n",
            "117:\tlearn: 128.7877637\ttotal: 16.8s\tremaining: 1m 35s\n",
            "118:\tlearn: 128.5784514\ttotal: 16.9s\tremaining: 1m 35s\n",
            "119:\tlearn: 128.2769540\ttotal: 17.1s\tremaining: 1m 35s\n",
            "120:\tlearn: 128.1230852\ttotal: 17.2s\tremaining: 1m 35s\n",
            "121:\tlearn: 128.1098680\ttotal: 17.4s\tremaining: 1m 35s\n",
            "122:\tlearn: 126.9225046\ttotal: 17.6s\tremaining: 1m 35s\n",
            "123:\tlearn: 126.3890721\ttotal: 17.7s\tremaining: 1m 35s\n",
            "124:\tlearn: 126.2838809\ttotal: 17.9s\tremaining: 1m 35s\n",
            "125:\tlearn: 125.6187679\ttotal: 18s\tremaining: 1m 35s\n",
            "126:\tlearn: 125.0039309\ttotal: 18.2s\tremaining: 1m 35s\n",
            "127:\tlearn: 123.8834791\ttotal: 18.3s\tremaining: 1m 34s\n",
            "128:\tlearn: 123.7233662\ttotal: 18.5s\tremaining: 1m 34s\n",
            "129:\tlearn: 123.4302312\ttotal: 18.6s\tremaining: 1m 34s\n",
            "130:\tlearn: 122.5667072\ttotal: 18.7s\tremaining: 1m 34s\n",
            "131:\tlearn: 122.3135098\ttotal: 18.9s\tremaining: 1m 34s\n",
            "132:\tlearn: 121.9941735\ttotal: 19.1s\tremaining: 1m 34s\n",
            "133:\tlearn: 121.7417432\ttotal: 19.2s\tremaining: 1m 34s\n",
            "134:\tlearn: 121.6541706\ttotal: 19.4s\tremaining: 1m 34s\n",
            "135:\tlearn: 121.3811288\ttotal: 19.5s\tremaining: 1m 34s\n",
            "136:\tlearn: 120.9895294\ttotal: 19.7s\tremaining: 1m 34s\n",
            "137:\tlearn: 119.3650058\ttotal: 19.8s\tremaining: 1m 34s\n",
            "138:\tlearn: 118.7639624\ttotal: 20s\tremaining: 1m 33s\n",
            "139:\tlearn: 118.5530425\ttotal: 20.1s\tremaining: 1m 33s\n",
            "140:\tlearn: 118.3409872\ttotal: 20.3s\tremaining: 1m 33s\n",
            "141:\tlearn: 117.9661261\ttotal: 20.4s\tremaining: 1m 33s\n",
            "142:\tlearn: 117.7865426\ttotal: 20.6s\tremaining: 1m 33s\n",
            "143:\tlearn: 117.6776834\ttotal: 20.7s\tremaining: 1m 33s\n",
            "144:\tlearn: 117.5357194\ttotal: 20.9s\tremaining: 1m 33s\n",
            "145:\tlearn: 117.3692188\ttotal: 21s\tremaining: 1m 33s\n",
            "146:\tlearn: 117.2501206\ttotal: 21.2s\tremaining: 1m 33s\n",
            "147:\tlearn: 116.8905241\ttotal: 21.3s\tremaining: 1m 32s\n",
            "148:\tlearn: 116.2355697\ttotal: 21.5s\tremaining: 1m 32s\n",
            "149:\tlearn: 115.9807208\ttotal: 21.6s\tremaining: 1m 32s\n",
            "150:\tlearn: 115.8451299\ttotal: 21.8s\tremaining: 1m 32s\n",
            "151:\tlearn: 115.3066585\ttotal: 21.9s\tremaining: 1m 32s\n",
            "152:\tlearn: 115.1997879\ttotal: 22.1s\tremaining: 1m 32s\n",
            "153:\tlearn: 114.8580534\ttotal: 22.2s\tremaining: 1m 32s\n",
            "154:\tlearn: 114.5550005\ttotal: 22.4s\tremaining: 1m 31s\n",
            "155:\tlearn: 114.1907969\ttotal: 22.5s\tremaining: 1m 31s\n",
            "156:\tlearn: 113.6580586\ttotal: 22.7s\tremaining: 1m 31s\n",
            "157:\tlearn: 113.5520017\ttotal: 22.8s\tremaining: 1m 31s\n",
            "158:\tlearn: 113.2242917\ttotal: 22.9s\tremaining: 1m 31s\n",
            "159:\tlearn: 113.0721903\ttotal: 23.1s\tremaining: 1m 31s\n",
            "160:\tlearn: 112.9043039\ttotal: 23.2s\tremaining: 1m 31s\n",
            "161:\tlearn: 112.7784037\ttotal: 23.4s\tremaining: 1m 30s\n",
            "162:\tlearn: 112.6992728\ttotal: 23.5s\tremaining: 1m 30s\n",
            "163:\tlearn: 112.5497374\ttotal: 23.7s\tremaining: 1m 30s\n",
            "164:\tlearn: 112.4707878\ttotal: 23.8s\tremaining: 1m 30s\n",
            "165:\tlearn: 112.3814107\ttotal: 24s\tremaining: 1m 30s\n",
            "166:\tlearn: 112.3550513\ttotal: 24.2s\tremaining: 1m 30s\n",
            "167:\tlearn: 112.1413659\ttotal: 24.3s\tremaining: 1m 30s\n",
            "168:\tlearn: 111.6700568\ttotal: 24.4s\tremaining: 1m 30s\n",
            "169:\tlearn: 111.4415039\ttotal: 24.6s\tremaining: 1m 29s\n",
            "170:\tlearn: 110.6233081\ttotal: 24.7s\tremaining: 1m 29s\n",
            "171:\tlearn: 110.1612568\ttotal: 24.8s\tremaining: 1m 29s\n",
            "172:\tlearn: 109.6891887\ttotal: 25s\tremaining: 1m 29s\n",
            "173:\tlearn: 109.3229355\ttotal: 25.1s\tremaining: 1m 29s\n",
            "174:\tlearn: 109.2250350\ttotal: 25.3s\tremaining: 1m 29s\n",
            "175:\tlearn: 109.1183335\ttotal: 25.5s\tremaining: 1m 29s\n",
            "176:\tlearn: 108.8102626\ttotal: 25.6s\tremaining: 1m 28s\n",
            "177:\tlearn: 108.5511565\ttotal: 25.7s\tremaining: 1m 28s\n",
            "178:\tlearn: 108.4425326\ttotal: 25.9s\tremaining: 1m 28s\n",
            "179:\tlearn: 108.2340045\ttotal: 26.1s\tremaining: 1m 28s\n",
            "180:\tlearn: 108.0553137\ttotal: 26.2s\tremaining: 1m 28s\n",
            "181:\tlearn: 107.8980852\ttotal: 26.3s\tremaining: 1m 28s\n",
            "182:\tlearn: 107.7791274\ttotal: 26.5s\tremaining: 1m 28s\n",
            "183:\tlearn: 107.6612676\ttotal: 26.6s\tremaining: 1m 28s\n",
            "184:\tlearn: 107.5268236\ttotal: 26.8s\tremaining: 1m 27s\n",
            "185:\tlearn: 107.4181724\ttotal: 26.9s\tremaining: 1m 27s\n",
            "186:\tlearn: 107.1446460\ttotal: 27.1s\tremaining: 1m 27s\n",
            "187:\tlearn: 106.6475482\ttotal: 27.2s\tremaining: 1m 27s\n",
            "188:\tlearn: 106.4213769\ttotal: 27.3s\tremaining: 1m 27s\n",
            "189:\tlearn: 106.0390756\ttotal: 27.5s\tremaining: 1m 27s\n",
            "190:\tlearn: 105.9280120\ttotal: 27.6s\tremaining: 1m 26s\n",
            "191:\tlearn: 105.8173518\ttotal: 27.8s\tremaining: 1m 26s\n",
            "192:\tlearn: 105.5341316\ttotal: 27.9s\tremaining: 1m 26s\n",
            "193:\tlearn: 105.4153958\ttotal: 28.1s\tremaining: 1m 26s\n",
            "194:\tlearn: 105.3976284\ttotal: 28.2s\tremaining: 1m 26s\n",
            "195:\tlearn: 105.3288950\ttotal: 28.4s\tremaining: 1m 26s\n",
            "196:\tlearn: 105.2083885\ttotal: 28.5s\tremaining: 1m 26s\n",
            "197:\tlearn: 105.0733091\ttotal: 28.6s\tremaining: 1m 25s\n",
            "198:\tlearn: 104.6532081\ttotal: 28.8s\tremaining: 1m 25s\n",
            "199:\tlearn: 103.9183688\ttotal: 28.9s\tremaining: 1m 25s\n",
            "200:\tlearn: 103.7494385\ttotal: 29.1s\tremaining: 1m 25s\n",
            "201:\tlearn: 103.6688238\ttotal: 29.2s\tremaining: 1m 25s\n",
            "202:\tlearn: 103.4469529\ttotal: 29.4s\tremaining: 1m 25s\n",
            "203:\tlearn: 103.3514962\ttotal: 29.5s\tremaining: 1m 25s\n",
            "204:\tlearn: 103.2598669\ttotal: 29.6s\tremaining: 1m 24s\n",
            "205:\tlearn: 103.1501362\ttotal: 29.9s\tremaining: 1m 24s\n",
            "206:\tlearn: 103.0125458\ttotal: 30s\tremaining: 1m 24s\n",
            "207:\tlearn: 102.8792501\ttotal: 30.2s\tremaining: 1m 24s\n",
            "208:\tlearn: 102.7264122\ttotal: 30.3s\tremaining: 1m 24s\n",
            "209:\tlearn: 102.6034505\ttotal: 30.5s\tremaining: 1m 24s\n",
            "210:\tlearn: 102.6001783\ttotal: 30.6s\tremaining: 1m 24s\n",
            "211:\tlearn: 102.1856078\ttotal: 30.8s\tremaining: 1m 24s\n",
            "212:\tlearn: 102.0161231\ttotal: 30.9s\tremaining: 1m 24s\n",
            "213:\tlearn: 101.9591320\ttotal: 31.1s\tremaining: 1m 23s\n",
            "214:\tlearn: 101.7622583\ttotal: 31.3s\tremaining: 1m 23s\n",
            "215:\tlearn: 101.6955291\ttotal: 31.4s\tremaining: 1m 23s\n",
            "216:\tlearn: 101.4224817\ttotal: 31.6s\tremaining: 1m 23s\n",
            "217:\tlearn: 101.3366631\ttotal: 31.7s\tremaining: 1m 23s\n",
            "218:\tlearn: 101.0271836\ttotal: 31.9s\tremaining: 1m 23s\n",
            "219:\tlearn: 100.9970916\ttotal: 32s\tremaining: 1m 23s\n",
            "220:\tlearn: 100.7655248\ttotal: 32.2s\tremaining: 1m 23s\n",
            "221:\tlearn: 100.5035228\ttotal: 32.3s\tremaining: 1m 22s\n",
            "222:\tlearn: 100.0303629\ttotal: 32.5s\tremaining: 1m 22s\n",
            "223:\tlearn: 99.9497806\ttotal: 32.6s\tremaining: 1m 22s\n",
            "224:\tlearn: 99.8096755\ttotal: 32.8s\tremaining: 1m 22s\n",
            "225:\tlearn: 99.7529767\ttotal: 32.9s\tremaining: 1m 22s\n",
            "226:\tlearn: 99.5786169\ttotal: 33.1s\tremaining: 1m 22s\n",
            "227:\tlearn: 99.5225290\ttotal: 33.3s\tremaining: 1m 22s\n",
            "228:\tlearn: 99.4541021\ttotal: 33.4s\tremaining: 1m 22s\n",
            "229:\tlearn: 99.4019049\ttotal: 33.6s\tremaining: 1m 22s\n",
            "230:\tlearn: 99.3090813\ttotal: 33.8s\tremaining: 1m 21s\n",
            "231:\tlearn: 99.2820987\ttotal: 33.9s\tremaining: 1m 21s\n",
            "232:\tlearn: 98.5182953\ttotal: 34.1s\tremaining: 1m 21s\n",
            "233:\tlearn: 98.1868660\ttotal: 34.2s\tremaining: 1m 21s\n",
            "234:\tlearn: 98.1774112\ttotal: 34.4s\tremaining: 1m 21s\n",
            "235:\tlearn: 97.9827569\ttotal: 34.5s\tremaining: 1m 21s\n",
            "236:\tlearn: 97.9005777\ttotal: 34.7s\tremaining: 1m 21s\n",
            "237:\tlearn: 97.8024721\ttotal: 34.8s\tremaining: 1m 21s\n",
            "238:\tlearn: 97.7659922\ttotal: 35s\tremaining: 1m 20s\n",
            "239:\tlearn: 97.6875199\ttotal: 35.1s\tremaining: 1m 20s\n",
            "240:\tlearn: 97.5870813\ttotal: 35.3s\tremaining: 1m 20s\n",
            "241:\tlearn: 97.3840073\ttotal: 35.4s\tremaining: 1m 20s\n",
            "242:\tlearn: 97.3833427\ttotal: 35.6s\tremaining: 1m 20s\n",
            "243:\tlearn: 97.2759730\ttotal: 35.8s\tremaining: 1m 20s\n",
            "244:\tlearn: 97.2662257\ttotal: 35.9s\tremaining: 1m 20s\n",
            "245:\tlearn: 97.2312778\ttotal: 36.1s\tremaining: 1m 20s\n",
            "246:\tlearn: 96.8922012\ttotal: 36.2s\tremaining: 1m 19s\n",
            "247:\tlearn: 96.5098756\ttotal: 36.4s\tremaining: 1m 19s\n",
            "248:\tlearn: 96.2857730\ttotal: 36.5s\tremaining: 1m 19s\n",
            "249:\tlearn: 96.2187482\ttotal: 36.7s\tremaining: 1m 19s\n",
            "250:\tlearn: 96.1752168\ttotal: 36.8s\tremaining: 1m 19s\n",
            "251:\tlearn: 96.0379858\ttotal: 37s\tremaining: 1m 19s\n",
            "252:\tlearn: 95.6623032\ttotal: 37.1s\tremaining: 1m 19s\n",
            "253:\tlearn: 95.6011590\ttotal: 37.3s\tremaining: 1m 18s\n",
            "254:\tlearn: 95.5185529\ttotal: 37.4s\tremaining: 1m 18s\n",
            "255:\tlearn: 95.2733452\ttotal: 37.5s\tremaining: 1m 18s\n",
            "256:\tlearn: 95.2204321\ttotal: 37.7s\tremaining: 1m 18s\n",
            "257:\tlearn: 95.1367229\ttotal: 37.9s\tremaining: 1m 18s\n",
            "258:\tlearn: 94.8141703\ttotal: 38s\tremaining: 1m 18s\n",
            "259:\tlearn: 94.7336568\ttotal: 38.1s\tremaining: 1m 18s\n",
            "260:\tlearn: 94.7107507\ttotal: 38.3s\tremaining: 1m 17s\n",
            "261:\tlearn: 94.5726808\ttotal: 38.5s\tremaining: 1m 17s\n",
            "262:\tlearn: 94.4733066\ttotal: 38.6s\tremaining: 1m 17s\n",
            "263:\tlearn: 94.4722590\ttotal: 38.8s\tremaining: 1m 17s\n",
            "264:\tlearn: 94.2394499\ttotal: 38.9s\tremaining: 1m 17s\n",
            "265:\tlearn: 94.0215413\ttotal: 39.1s\tremaining: 1m 17s\n",
            "266:\tlearn: 93.9808368\ttotal: 39.2s\tremaining: 1m 17s\n",
            "267:\tlearn: 93.9219478\ttotal: 39.4s\tremaining: 1m 17s\n",
            "268:\tlearn: 93.5489909\ttotal: 39.5s\tremaining: 1m 16s\n",
            "269:\tlearn: 93.1211267\ttotal: 39.7s\tremaining: 1m 16s\n",
            "270:\tlearn: 92.9734187\ttotal: 39.8s\tremaining: 1m 16s\n",
            "271:\tlearn: 92.7984594\ttotal: 40s\tremaining: 1m 16s\n",
            "272:\tlearn: 92.6790243\ttotal: 40.1s\tremaining: 1m 16s\n",
            "273:\tlearn: 92.5715995\ttotal: 40.3s\tremaining: 1m 16s\n",
            "274:\tlearn: 92.1278996\ttotal: 40.4s\tremaining: 1m 15s\n",
            "275:\tlearn: 92.0377010\ttotal: 40.6s\tremaining: 1m 15s\n",
            "276:\tlearn: 91.9218361\ttotal: 40.7s\tremaining: 1m 15s\n",
            "277:\tlearn: 91.7591616\ttotal: 40.8s\tremaining: 1m 15s\n",
            "278:\tlearn: 91.7027624\ttotal: 41s\tremaining: 1m 15s\n",
            "279:\tlearn: 91.6278989\ttotal: 41.1s\tremaining: 1m 15s\n",
            "280:\tlearn: 91.4998372\ttotal: 41.3s\tremaining: 1m 15s\n",
            "281:\tlearn: 91.4957249\ttotal: 41.4s\tremaining: 1m 14s\n",
            "282:\tlearn: 91.3946178\ttotal: 41.6s\tremaining: 1m 14s\n",
            "283:\tlearn: 91.1303788\ttotal: 41.8s\tremaining: 1m 14s\n",
            "284:\tlearn: 91.0527536\ttotal: 41.9s\tremaining: 1m 14s\n",
            "285:\tlearn: 90.9883530\ttotal: 42.1s\tremaining: 1m 14s\n",
            "286:\tlearn: 90.8781647\ttotal: 42.2s\tremaining: 1m 14s\n",
            "287:\tlearn: 90.5300847\ttotal: 42.4s\tremaining: 1m 14s\n",
            "288:\tlearn: 90.3227689\ttotal: 42.5s\tremaining: 1m 14s\n",
            "289:\tlearn: 90.2400772\ttotal: 42.7s\tremaining: 1m 13s\n",
            "290:\tlearn: 90.1156682\ttotal: 42.8s\tremaining: 1m 13s\n",
            "291:\tlearn: 90.0371552\ttotal: 43s\tremaining: 1m 13s\n",
            "292:\tlearn: 89.9611059\ttotal: 43.1s\tremaining: 1m 13s\n",
            "293:\tlearn: 89.9220410\ttotal: 43.3s\tremaining: 1m 13s\n",
            "294:\tlearn: 89.7944521\ttotal: 43.5s\tremaining: 1m 13s\n",
            "295:\tlearn: 89.7201598\ttotal: 43.6s\tremaining: 1m 13s\n",
            "296:\tlearn: 89.7153449\ttotal: 43.7s\tremaining: 1m 12s\n",
            "297:\tlearn: 89.7081029\ttotal: 43.9s\tremaining: 1m 12s\n",
            "298:\tlearn: 89.6205231\ttotal: 44s\tremaining: 1m 12s\n",
            "299:\tlearn: 89.4644154\ttotal: 44.2s\tremaining: 1m 12s\n",
            "300:\tlearn: 89.1605594\ttotal: 44.3s\tremaining: 1m 12s\n",
            "301:\tlearn: 88.9144520\ttotal: 44.4s\tremaining: 1m 12s\n",
            "302:\tlearn: 88.8344896\ttotal: 44.6s\tremaining: 1m 11s\n",
            "303:\tlearn: 88.7181794\ttotal: 44.7s\tremaining: 1m 11s\n",
            "304:\tlearn: 88.6593371\ttotal: 44.9s\tremaining: 1m 11s\n",
            "305:\tlearn: 88.5551673\ttotal: 45s\tremaining: 1m 11s\n",
            "306:\tlearn: 88.4759103\ttotal: 45.2s\tremaining: 1m 11s\n",
            "307:\tlearn: 88.3881344\ttotal: 45.3s\tremaining: 1m 11s\n",
            "308:\tlearn: 88.2785865\ttotal: 45.5s\tremaining: 1m 11s\n",
            "309:\tlearn: 88.1978618\ttotal: 45.6s\tremaining: 1m 10s\n",
            "310:\tlearn: 87.9505495\ttotal: 45.8s\tremaining: 1m 10s\n",
            "311:\tlearn: 87.8138048\ttotal: 45.9s\tremaining: 1m 10s\n",
            "312:\tlearn: 87.3225870\ttotal: 46.1s\tremaining: 1m 10s\n",
            "313:\tlearn: 86.9643405\ttotal: 46.2s\tremaining: 1m 10s\n",
            "314:\tlearn: 86.7350507\ttotal: 46.4s\tremaining: 1m 10s\n",
            "315:\tlearn: 86.6680960\ttotal: 46.5s\tremaining: 1m 10s\n",
            "316:\tlearn: 86.5588177\ttotal: 46.7s\tremaining: 1m 9s\n",
            "317:\tlearn: 86.5194651\ttotal: 46.8s\tremaining: 1m 9s\n",
            "318:\tlearn: 86.2786990\ttotal: 47s\tremaining: 1m 9s\n",
            "319:\tlearn: 86.1395707\ttotal: 47.1s\tremaining: 1m 9s\n",
            "320:\tlearn: 86.0552651\ttotal: 47.3s\tremaining: 1m 9s\n",
            "321:\tlearn: 85.9877862\ttotal: 47.5s\tremaining: 1m 9s\n",
            "322:\tlearn: 85.8531654\ttotal: 47.6s\tremaining: 1m 9s\n",
            "323:\tlearn: 85.8073612\ttotal: 47.8s\tremaining: 1m 9s\n",
            "324:\tlearn: 85.2200180\ttotal: 47.9s\tremaining: 1m 8s\n",
            "325:\tlearn: 84.8169301\ttotal: 48s\tremaining: 1m 8s\n",
            "326:\tlearn: 84.6717821\ttotal: 48.2s\tremaining: 1m 8s\n",
            "327:\tlearn: 84.4787832\ttotal: 48.3s\tremaining: 1m 8s\n",
            "328:\tlearn: 84.3814415\ttotal: 48.5s\tremaining: 1m 8s\n",
            "329:\tlearn: 84.2142512\ttotal: 48.6s\tremaining: 1m 8s\n",
            "330:\tlearn: 84.0738631\ttotal: 48.7s\tremaining: 1m 7s\n",
            "331:\tlearn: 84.0154443\ttotal: 48.9s\tremaining: 1m 7s\n",
            "332:\tlearn: 83.9493090\ttotal: 49s\tremaining: 1m 7s\n",
            "333:\tlearn: 83.5991540\ttotal: 49.2s\tremaining: 1m 7s\n",
            "334:\tlearn: 83.4888919\ttotal: 49.3s\tremaining: 1m 7s\n",
            "335:\tlearn: 83.4451540\ttotal: 49.5s\tremaining: 1m 7s\n",
            "336:\tlearn: 83.3221104\ttotal: 49.6s\tremaining: 1m 7s\n",
            "337:\tlearn: 83.2347889\ttotal: 49.8s\tremaining: 1m 6s\n",
            "338:\tlearn: 83.1075018\ttotal: 49.9s\tremaining: 1m 6s\n",
            "339:\tlearn: 82.9701239\ttotal: 50.1s\tremaining: 1m 6s\n",
            "340:\tlearn: 82.8683366\ttotal: 50.2s\tremaining: 1m 6s\n",
            "341:\tlearn: 82.7934609\ttotal: 50.4s\tremaining: 1m 6s\n",
            "342:\tlearn: 82.5624680\ttotal: 50.5s\tremaining: 1m 6s\n",
            "343:\tlearn: 82.5602853\ttotal: 50.7s\tremaining: 1m 6s\n",
            "344:\tlearn: 82.4501056\ttotal: 50.8s\tremaining: 1m 5s\n",
            "345:\tlearn: 82.2985234\ttotal: 51s\tremaining: 1m 5s\n",
            "346:\tlearn: 82.2646524\ttotal: 51.2s\tremaining: 1m 5s\n",
            "347:\tlearn: 82.2643207\ttotal: 51.3s\tremaining: 1m 5s\n",
            "348:\tlearn: 82.2031719\ttotal: 51.5s\tremaining: 1m 5s\n",
            "349:\tlearn: 82.1095009\ttotal: 51.7s\tremaining: 1m 5s\n",
            "350:\tlearn: 82.0535290\ttotal: 51.8s\tremaining: 1m 5s\n",
            "351:\tlearn: 81.8780015\ttotal: 52s\tremaining: 1m 4s\n",
            "352:\tlearn: 81.6623355\ttotal: 52.1s\tremaining: 1m 4s\n",
            "353:\tlearn: 81.5962824\ttotal: 52.3s\tremaining: 1m 4s\n",
            "354:\tlearn: 81.3504247\ttotal: 52.4s\tremaining: 1m 4s\n",
            "355:\tlearn: 80.9910031\ttotal: 52.6s\tremaining: 1m 4s\n",
            "356:\tlearn: 80.9509632\ttotal: 52.7s\tremaining: 1m 4s\n",
            "357:\tlearn: 80.8558068\ttotal: 52.9s\tremaining: 1m 4s\n",
            "358:\tlearn: 80.8201945\ttotal: 53s\tremaining: 1m 3s\n",
            "359:\tlearn: 80.7716081\ttotal: 53.2s\tremaining: 1m 3s\n",
            "360:\tlearn: 80.6976144\ttotal: 53.4s\tremaining: 1m 3s\n",
            "361:\tlearn: 80.5930859\ttotal: 53.5s\tremaining: 1m 3s\n",
            "362:\tlearn: 80.5214214\ttotal: 53.6s\tremaining: 1m 3s\n",
            "363:\tlearn: 80.4206540\ttotal: 53.8s\tremaining: 1m 3s\n",
            "364:\tlearn: 80.3682529\ttotal: 53.9s\tremaining: 1m 3s\n",
            "365:\tlearn: 80.2300677\ttotal: 54.1s\tremaining: 1m 2s\n",
            "366:\tlearn: 80.0939804\ttotal: 54.2s\tremaining: 1m 2s\n",
            "367:\tlearn: 79.9605811\ttotal: 54.4s\tremaining: 1m 2s\n",
            "368:\tlearn: 79.9157891\ttotal: 54.5s\tremaining: 1m 2s\n",
            "369:\tlearn: 79.8712330\ttotal: 54.7s\tremaining: 1m 2s\n",
            "370:\tlearn: 79.7932362\ttotal: 54.8s\tremaining: 1m 2s\n",
            "371:\tlearn: 79.7061807\ttotal: 55s\tremaining: 1m 2s\n",
            "372:\tlearn: 79.6609997\ttotal: 55.1s\tremaining: 1m 1s\n",
            "373:\tlearn: 79.6545303\ttotal: 55.3s\tremaining: 1m 1s\n",
            "374:\tlearn: 79.6125368\ttotal: 55.5s\tremaining: 1m 1s\n",
            "375:\tlearn: 79.4284897\ttotal: 55.6s\tremaining: 1m 1s\n",
            "376:\tlearn: 79.2145146\ttotal: 55.7s\tremaining: 1m 1s\n",
            "377:\tlearn: 79.1963148\ttotal: 55.9s\tremaining: 1m 1s\n",
            "378:\tlearn: 79.1031524\ttotal: 56s\tremaining: 1m 1s\n",
            "379:\tlearn: 79.0250670\ttotal: 56.2s\tremaining: 1m\n",
            "380:\tlearn: 78.9749864\ttotal: 56.3s\tremaining: 1m\n",
            "381:\tlearn: 78.7598383\ttotal: 56.5s\tremaining: 1m\n",
            "382:\tlearn: 78.7431097\ttotal: 56.6s\tremaining: 1m\n",
            "383:\tlearn: 78.5703953\ttotal: 56.8s\tremaining: 1m\n",
            "384:\tlearn: 78.2338917\ttotal: 56.9s\tremaining: 1m\n",
            "385:\tlearn: 78.1267990\ttotal: 57s\tremaining: 60s\n",
            "386:\tlearn: 78.0699812\ttotal: 57.2s\tremaining: 59.8s\n",
            "387:\tlearn: 78.0013917\ttotal: 57.3s\tremaining: 59.7s\n",
            "388:\tlearn: 77.7607793\ttotal: 57.5s\tremaining: 59.5s\n",
            "389:\tlearn: 77.5311864\ttotal: 57.6s\tremaining: 59.4s\n",
            "390:\tlearn: 77.4445444\ttotal: 57.8s\tremaining: 59.2s\n",
            "391:\tlearn: 77.3698900\ttotal: 57.9s\tremaining: 59.1s\n",
            "392:\tlearn: 77.2252431\ttotal: 58.1s\tremaining: 58.9s\n",
            "393:\tlearn: 77.0408563\ttotal: 58.2s\tremaining: 58.8s\n",
            "394:\tlearn: 76.9365635\ttotal: 58.3s\tremaining: 58.6s\n",
            "395:\tlearn: 76.8410904\ttotal: 58.5s\tremaining: 58.5s\n",
            "396:\tlearn: 76.7776930\ttotal: 58.6s\tremaining: 58.3s\n",
            "397:\tlearn: 76.6675940\ttotal: 58.8s\tremaining: 58.2s\n",
            "398:\tlearn: 76.3435188\ttotal: 58.9s\tremaining: 58s\n",
            "399:\tlearn: 76.2718991\ttotal: 59.1s\tremaining: 57.9s\n",
            "400:\tlearn: 76.0154116\ttotal: 59.2s\tremaining: 57.7s\n",
            "401:\tlearn: 75.9480849\ttotal: 59.4s\tremaining: 57.6s\n",
            "402:\tlearn: 75.8376442\ttotal: 59.5s\tremaining: 57.4s\n",
            "403:\tlearn: 75.7536804\ttotal: 59.6s\tremaining: 57.3s\n",
            "404:\tlearn: 75.6539004\ttotal: 59.8s\tremaining: 57.1s\n",
            "405:\tlearn: 75.5671498\ttotal: 59.9s\tremaining: 57s\n",
            "406:\tlearn: 75.4614396\ttotal: 1m\tremaining: 56.8s\n",
            "407:\tlearn: 75.4188711\ttotal: 1m\tremaining: 56.6s\n",
            "408:\tlearn: 75.2535326\ttotal: 1m\tremaining: 56.5s\n",
            "409:\tlearn: 75.1746248\ttotal: 1m\tremaining: 56.3s\n",
            "410:\tlearn: 74.9660638\ttotal: 1m\tremaining: 56.2s\n",
            "411:\tlearn: 74.9054766\ttotal: 1m\tremaining: 56s\n",
            "412:\tlearn: 74.8481025\ttotal: 1m\tremaining: 55.9s\n",
            "413:\tlearn: 74.8338738\ttotal: 1m 1s\tremaining: 55.8s\n",
            "414:\tlearn: 74.7994456\ttotal: 1m 1s\tremaining: 55.7s\n",
            "415:\tlearn: 74.5536699\ttotal: 1m 1s\tremaining: 55.5s\n",
            "416:\tlearn: 74.4834023\ttotal: 1m 1s\tremaining: 55.4s\n",
            "417:\tlearn: 74.3634477\ttotal: 1m 1s\tremaining: 55.2s\n",
            "418:\tlearn: 74.2566037\ttotal: 1m 1s\tremaining: 55.1s\n",
            "419:\tlearn: 74.2156916\ttotal: 1m 2s\tremaining: 54.9s\n",
            "420:\tlearn: 74.1598726\ttotal: 1m 2s\tremaining: 54.8s\n",
            "421:\tlearn: 74.0919724\ttotal: 1m 2s\tremaining: 54.6s\n",
            "422:\tlearn: 73.9867116\ttotal: 1m 2s\tremaining: 54.5s\n",
            "423:\tlearn: 73.8950888\ttotal: 1m 2s\tremaining: 54.3s\n",
            "424:\tlearn: 73.8514669\ttotal: 1m 2s\tremaining: 54.2s\n",
            "425:\tlearn: 73.8004503\ttotal: 1m 2s\tremaining: 54s\n",
            "426:\tlearn: 73.6933193\ttotal: 1m 3s\tremaining: 53.9s\n",
            "427:\tlearn: 73.6635239\ttotal: 1m 3s\tremaining: 53.7s\n",
            "428:\tlearn: 73.6239831\ttotal: 1m 3s\tremaining: 53.6s\n",
            "429:\tlearn: 73.5027602\ttotal: 1m 3s\tremaining: 53.4s\n",
            "430:\tlearn: 73.3926216\ttotal: 1m 3s\tremaining: 53.3s\n",
            "431:\tlearn: 73.2745490\ttotal: 1m 3s\tremaining: 53.1s\n",
            "432:\tlearn: 73.2692059\ttotal: 1m 3s\tremaining: 53s\n",
            "433:\tlearn: 73.2285452\ttotal: 1m 4s\tremaining: 52.8s\n",
            "434:\tlearn: 73.1516571\ttotal: 1m 4s\tremaining: 52.7s\n",
            "435:\tlearn: 73.1034384\ttotal: 1m 4s\tremaining: 52.6s\n",
            "436:\tlearn: 72.8619408\ttotal: 1m 4s\tremaining: 52.4s\n",
            "437:\tlearn: 72.7188531\ttotal: 1m 4s\tremaining: 52.3s\n",
            "438:\tlearn: 72.6315453\ttotal: 1m 4s\tremaining: 52.1s\n",
            "439:\tlearn: 72.5371088\ttotal: 1m 4s\tremaining: 52s\n",
            "440:\tlearn: 72.4377213\ttotal: 1m 5s\tremaining: 51.8s\n",
            "441:\tlearn: 72.3519042\ttotal: 1m 5s\tremaining: 51.7s\n",
            "442:\tlearn: 72.3283601\ttotal: 1m 5s\tremaining: 51.5s\n",
            "443:\tlearn: 72.2726373\ttotal: 1m 5s\tremaining: 51.4s\n",
            "444:\tlearn: 72.2264974\ttotal: 1m 5s\tremaining: 51.2s\n",
            "445:\tlearn: 72.2026619\ttotal: 1m 5s\tremaining: 51.1s\n",
            "446:\tlearn: 72.1807682\ttotal: 1m 6s\tremaining: 51s\n",
            "447:\tlearn: 72.0598561\ttotal: 1m 6s\tremaining: 50.8s\n",
            "448:\tlearn: 72.0013809\ttotal: 1m 6s\tremaining: 50.7s\n",
            "449:\tlearn: 71.9592538\ttotal: 1m 6s\tremaining: 50.5s\n",
            "450:\tlearn: 71.8062549\ttotal: 1m 6s\tremaining: 50.4s\n",
            "451:\tlearn: 71.7951190\ttotal: 1m 6s\tremaining: 50.2s\n",
            "452:\tlearn: 71.7303330\ttotal: 1m 6s\tremaining: 50.1s\n",
            "453:\tlearn: 71.5896824\ttotal: 1m 7s\tremaining: 49.9s\n",
            "454:\tlearn: 71.5032083\ttotal: 1m 7s\tremaining: 49.8s\n",
            "455:\tlearn: 71.4495080\ttotal: 1m 7s\tremaining: 49.7s\n",
            "456:\tlearn: 71.4230684\ttotal: 1m 7s\tremaining: 49.5s\n",
            "457:\tlearn: 71.3921374\ttotal: 1m 7s\tremaining: 49.4s\n",
            "458:\tlearn: 71.3536959\ttotal: 1m 7s\tremaining: 49.2s\n",
            "459:\tlearn: 71.2867868\ttotal: 1m 8s\tremaining: 49.1s\n",
            "460:\tlearn: 71.2149531\ttotal: 1m 8s\tremaining: 48.9s\n",
            "461:\tlearn: 71.1410140\ttotal: 1m 8s\tremaining: 48.8s\n",
            "462:\tlearn: 71.0592109\ttotal: 1m 8s\tremaining: 48.6s\n",
            "463:\tlearn: 71.0344593\ttotal: 1m 8s\tremaining: 48.5s\n",
            "464:\tlearn: 70.9035884\ttotal: 1m 8s\tremaining: 48.3s\n",
            "465:\tlearn: 70.8767380\ttotal: 1m 8s\tremaining: 48.2s\n",
            "466:\tlearn: 70.8019216\ttotal: 1m 9s\tremaining: 48.1s\n",
            "467:\tlearn: 70.7626708\ttotal: 1m 9s\tremaining: 47.9s\n",
            "468:\tlearn: 70.6733548\ttotal: 1m 9s\tremaining: 47.8s\n",
            "469:\tlearn: 70.5920555\ttotal: 1m 9s\tremaining: 47.6s\n",
            "470:\tlearn: 70.5579926\ttotal: 1m 9s\tremaining: 47.5s\n",
            "471:\tlearn: 70.4864047\ttotal: 1m 9s\tremaining: 47.3s\n",
            "472:\tlearn: 70.4610804\ttotal: 1m 9s\tremaining: 47.2s\n",
            "473:\tlearn: 70.4585730\ttotal: 1m 10s\tremaining: 47.1s\n",
            "474:\tlearn: 70.4390434\ttotal: 1m 10s\tremaining: 46.9s\n",
            "475:\tlearn: 70.3598302\ttotal: 1m 10s\tremaining: 46.8s\n",
            "476:\tlearn: 70.3553867\ttotal: 1m 10s\tremaining: 46.6s\n",
            "477:\tlearn: 70.3241701\ttotal: 1m 10s\tremaining: 46.5s\n",
            "478:\tlearn: 70.2596619\ttotal: 1m 10s\tremaining: 46.4s\n",
            "479:\tlearn: 70.1860849\ttotal: 1m 11s\tremaining: 46.2s\n",
            "480:\tlearn: 70.1623578\ttotal: 1m 11s\tremaining: 46.1s\n",
            "481:\tlearn: 70.1226877\ttotal: 1m 11s\tremaining: 45.9s\n",
            "482:\tlearn: 70.0164097\ttotal: 1m 11s\tremaining: 45.8s\n",
            "483:\tlearn: 69.8274784\ttotal: 1m 11s\tremaining: 45.6s\n",
            "484:\tlearn: 69.7527069\ttotal: 1m 11s\tremaining: 45.5s\n",
            "485:\tlearn: 69.7198176\ttotal: 1m 11s\tremaining: 45.3s\n",
            "486:\tlearn: 69.6550516\ttotal: 1m 12s\tremaining: 45.2s\n",
            "487:\tlearn: 69.4767169\ttotal: 1m 12s\tremaining: 45s\n",
            "488:\tlearn: 69.4657775\ttotal: 1m 12s\tremaining: 44.9s\n",
            "489:\tlearn: 69.4225524\ttotal: 1m 12s\tremaining: 44.7s\n",
            "490:\tlearn: 69.3389459\ttotal: 1m 12s\tremaining: 44.6s\n",
            "491:\tlearn: 69.2342304\ttotal: 1m 12s\tremaining: 44.4s\n",
            "492:\tlearn: 69.2006648\ttotal: 1m 13s\tremaining: 44.3s\n",
            "493:\tlearn: 69.1583164\ttotal: 1m 13s\tremaining: 44.1s\n",
            "494:\tlearn: 69.1141141\ttotal: 1m 13s\tremaining: 44s\n",
            "495:\tlearn: 69.0603610\ttotal: 1m 13s\tremaining: 43.9s\n",
            "496:\tlearn: 69.0277419\ttotal: 1m 13s\tremaining: 43.7s\n",
            "497:\tlearn: 68.9719266\ttotal: 1m 13s\tremaining: 43.6s\n",
            "498:\tlearn: 68.9459739\ttotal: 1m 13s\tremaining: 43.4s\n",
            "499:\tlearn: 68.7877781\ttotal: 1m 14s\tremaining: 43.3s\n",
            "500:\tlearn: 68.7096118\ttotal: 1m 14s\tremaining: 43.1s\n",
            "501:\tlearn: 68.6664141\ttotal: 1m 14s\tremaining: 43s\n",
            "502:\tlearn: 68.6030400\ttotal: 1m 14s\tremaining: 42.8s\n",
            "503:\tlearn: 68.4512683\ttotal: 1m 14s\tremaining: 42.7s\n",
            "504:\tlearn: 68.2762145\ttotal: 1m 14s\tremaining: 42.5s\n",
            "505:\tlearn: 68.2533832\ttotal: 1m 15s\tremaining: 42.4s\n",
            "506:\tlearn: 68.2065689\ttotal: 1m 15s\tremaining: 42.3s\n",
            "507:\tlearn: 68.1286978\ttotal: 1m 15s\tremaining: 42.1s\n",
            "508:\tlearn: 68.0783187\ttotal: 1m 15s\tremaining: 42s\n",
            "509:\tlearn: 68.0302660\ttotal: 1m 15s\tremaining: 41.8s\n",
            "510:\tlearn: 67.9854472\ttotal: 1m 15s\tremaining: 41.7s\n",
            "511:\tlearn: 67.9273126\ttotal: 1m 15s\tremaining: 41.5s\n",
            "512:\tlearn: 67.8816565\ttotal: 1m 16s\tremaining: 41.4s\n",
            "513:\tlearn: 67.6378817\ttotal: 1m 16s\tremaining: 41.3s\n",
            "514:\tlearn: 67.5718734\ttotal: 1m 16s\tremaining: 41.1s\n",
            "515:\tlearn: 67.5025786\ttotal: 1m 16s\tremaining: 41s\n",
            "516:\tlearn: 67.3178973\ttotal: 1m 16s\tremaining: 40.8s\n",
            "517:\tlearn: 67.1964662\ttotal: 1m 16s\tremaining: 40.7s\n",
            "518:\tlearn: 67.1497239\ttotal: 1m 17s\tremaining: 40.5s\n",
            "519:\tlearn: 67.0915397\ttotal: 1m 17s\tremaining: 40.4s\n",
            "520:\tlearn: 66.9491171\ttotal: 1m 17s\tremaining: 40.2s\n",
            "521:\tlearn: 66.9091444\ttotal: 1m 17s\tremaining: 40.1s\n",
            "522:\tlearn: 66.8469012\ttotal: 1m 17s\tremaining: 39.9s\n",
            "523:\tlearn: 66.8251090\ttotal: 1m 17s\tremaining: 39.8s\n",
            "524:\tlearn: 66.7223228\ttotal: 1m 17s\tremaining: 39.6s\n",
            "525:\tlearn: 66.6536012\ttotal: 1m 18s\tremaining: 39.5s\n",
            "526:\tlearn: 66.6255925\ttotal: 1m 18s\tremaining: 39.3s\n",
            "527:\tlearn: 66.5466775\ttotal: 1m 18s\tremaining: 39.2s\n",
            "528:\tlearn: 66.4776888\ttotal: 1m 18s\tremaining: 39s\n",
            "529:\tlearn: 66.4586140\ttotal: 1m 18s\tremaining: 38.9s\n",
            "530:\tlearn: 66.2744884\ttotal: 1m 18s\tremaining: 38.7s\n",
            "531:\tlearn: 66.2465725\ttotal: 1m 18s\tremaining: 38.6s\n",
            "532:\tlearn: 66.2090583\ttotal: 1m 19s\tremaining: 38.4s\n",
            "533:\tlearn: 66.1304943\ttotal: 1m 19s\tremaining: 38.3s\n",
            "534:\tlearn: 66.1029954\ttotal: 1m 19s\tremaining: 38.1s\n",
            "535:\tlearn: 66.0175031\ttotal: 1m 19s\tremaining: 38s\n",
            "536:\tlearn: 65.9075623\ttotal: 1m 19s\tremaining: 37.8s\n",
            "537:\tlearn: 65.8700067\ttotal: 1m 19s\tremaining: 37.7s\n",
            "538:\tlearn: 65.7783342\ttotal: 1m 20s\tremaining: 37.6s\n",
            "539:\tlearn: 65.6888557\ttotal: 1m 20s\tremaining: 37.4s\n",
            "540:\tlearn: 65.6291920\ttotal: 1m 20s\tremaining: 37.2s\n",
            "541:\tlearn: 65.5814645\ttotal: 1m 20s\tremaining: 37.1s\n",
            "542:\tlearn: 65.4981758\ttotal: 1m 20s\tremaining: 36.9s\n",
            "543:\tlearn: 65.4324779\ttotal: 1m 20s\tremaining: 36.8s\n",
            "544:\tlearn: 65.4029371\ttotal: 1m 20s\tremaining: 36.7s\n",
            "545:\tlearn: 65.3206767\ttotal: 1m 21s\tremaining: 36.5s\n",
            "546:\tlearn: 65.3108467\ttotal: 1m 21s\tremaining: 36.4s\n",
            "547:\tlearn: 65.2219104\ttotal: 1m 21s\tremaining: 36.2s\n",
            "548:\tlearn: 65.1438860\ttotal: 1m 21s\tremaining: 36.1s\n",
            "549:\tlearn: 65.1395359\ttotal: 1m 21s\tremaining: 35.9s\n",
            "550:\tlearn: 65.0942185\ttotal: 1m 21s\tremaining: 35.8s\n",
            "551:\tlearn: 65.0776708\ttotal: 1m 21s\tremaining: 35.6s\n",
            "552:\tlearn: 64.9968008\ttotal: 1m 22s\tremaining: 35.5s\n",
            "553:\tlearn: 64.9346967\ttotal: 1m 22s\tremaining: 35.3s\n",
            "554:\tlearn: 64.9061095\ttotal: 1m 22s\tremaining: 35.2s\n",
            "555:\tlearn: 64.8662403\ttotal: 1m 22s\tremaining: 35s\n",
            "556:\tlearn: 64.7759368\ttotal: 1m 22s\tremaining: 34.9s\n",
            "557:\tlearn: 64.6889921\ttotal: 1m 22s\tremaining: 34.7s\n",
            "558:\tlearn: 64.6301909\ttotal: 1m 23s\tremaining: 34.6s\n",
            "559:\tlearn: 64.6070756\ttotal: 1m 23s\tremaining: 34.5s\n",
            "560:\tlearn: 64.5794081\ttotal: 1m 23s\tremaining: 34.3s\n",
            "561:\tlearn: 64.5211880\ttotal: 1m 23s\tremaining: 34.2s\n",
            "562:\tlearn: 64.4887046\ttotal: 1m 23s\tremaining: 34s\n",
            "563:\tlearn: 64.3602404\ttotal: 1m 23s\tremaining: 33.9s\n",
            "564:\tlearn: 64.3400158\ttotal: 1m 23s\tremaining: 33.7s\n",
            "565:\tlearn: 64.1834471\ttotal: 1m 24s\tremaining: 33.6s\n",
            "566:\tlearn: 64.0633410\ttotal: 1m 24s\tremaining: 33.4s\n",
            "567:\tlearn: 64.0346771\ttotal: 1m 24s\tremaining: 33.3s\n",
            "568:\tlearn: 63.9148379\ttotal: 1m 24s\tremaining: 33.1s\n",
            "569:\tlearn: 63.8685417\ttotal: 1m 24s\tremaining: 33s\n",
            "570:\tlearn: 63.8440863\ttotal: 1m 24s\tremaining: 32.8s\n",
            "571:\tlearn: 63.7781777\ttotal: 1m 24s\tremaining: 32.7s\n",
            "572:\tlearn: 63.7399446\ttotal: 1m 25s\tremaining: 32.5s\n",
            "573:\tlearn: 63.7201994\ttotal: 1m 25s\tremaining: 32.4s\n",
            "574:\tlearn: 63.7084472\ttotal: 1m 25s\tremaining: 32.3s\n",
            "575:\tlearn: 63.6980183\ttotal: 1m 25s\tremaining: 32.1s\n",
            "576:\tlearn: 63.6689791\ttotal: 1m 25s\tremaining: 32s\n",
            "577:\tlearn: 63.6206598\ttotal: 1m 25s\tremaining: 31.8s\n",
            "578:\tlearn: 63.5757026\ttotal: 1m 26s\tremaining: 31.7s\n",
            "579:\tlearn: 63.5144786\ttotal: 1m 26s\tremaining: 31.5s\n",
            "580:\tlearn: 63.4713557\ttotal: 1m 26s\tremaining: 31.4s\n",
            "581:\tlearn: 63.3563683\ttotal: 1m 26s\tremaining: 31.2s\n",
            "582:\tlearn: 63.3308914\ttotal: 1m 26s\tremaining: 31.1s\n",
            "583:\tlearn: 63.2621929\ttotal: 1m 26s\tremaining: 30.9s\n",
            "584:\tlearn: 63.2243384\ttotal: 1m 27s\tremaining: 30.8s\n",
            "585:\tlearn: 63.1940088\ttotal: 1m 27s\tremaining: 30.6s\n",
            "586:\tlearn: 62.9947282\ttotal: 1m 27s\tremaining: 30.5s\n",
            "587:\tlearn: 62.9591346\ttotal: 1m 27s\tremaining: 30.3s\n",
            "588:\tlearn: 62.8777838\ttotal: 1m 27s\tremaining: 30.2s\n",
            "589:\tlearn: 62.8406268\ttotal: 1m 27s\tremaining: 30.1s\n",
            "590:\tlearn: 62.7999800\ttotal: 1m 27s\tremaining: 29.9s\n",
            "591:\tlearn: 62.7214077\ttotal: 1m 28s\tremaining: 29.8s\n",
            "592:\tlearn: 62.5903120\ttotal: 1m 28s\tremaining: 29.6s\n",
            "593:\tlearn: 62.5614568\ttotal: 1m 28s\tremaining: 29.5s\n",
            "594:\tlearn: 62.4988769\ttotal: 1m 28s\tremaining: 29.3s\n",
            "595:\tlearn: 62.4123307\ttotal: 1m 28s\tremaining: 29.2s\n",
            "596:\tlearn: 62.3077849\ttotal: 1m 28s\tremaining: 29s\n",
            "597:\tlearn: 62.2278472\ttotal: 1m 28s\tremaining: 28.9s\n",
            "598:\tlearn: 62.1957753\ttotal: 1m 29s\tremaining: 28.7s\n",
            "599:\tlearn: 62.1794356\ttotal: 1m 29s\tremaining: 28.6s\n",
            "600:\tlearn: 62.1046427\ttotal: 1m 29s\tremaining: 28.4s\n",
            "601:\tlearn: 62.0795769\ttotal: 1m 29s\tremaining: 28.3s\n",
            "602:\tlearn: 62.0601662\ttotal: 1m 29s\tremaining: 28.1s\n",
            "603:\tlearn: 62.0582183\ttotal: 1m 29s\tremaining: 28s\n",
            "604:\tlearn: 61.8797658\ttotal: 1m 30s\tremaining: 27.8s\n",
            "605:\tlearn: 61.8287951\ttotal: 1m 30s\tremaining: 27.7s\n",
            "606:\tlearn: 61.8130084\ttotal: 1m 30s\tremaining: 27.5s\n",
            "607:\tlearn: 61.7944204\ttotal: 1m 30s\tremaining: 27.4s\n",
            "608:\tlearn: 61.7583825\ttotal: 1m 30s\tremaining: 27.2s\n",
            "609:\tlearn: 61.6638244\ttotal: 1m 30s\tremaining: 27.1s\n",
            "610:\tlearn: 61.6011462\ttotal: 1m 30s\tremaining: 26.9s\n",
            "611:\tlearn: 61.5613113\ttotal: 1m 31s\tremaining: 26.8s\n",
            "612:\tlearn: 61.5299703\ttotal: 1m 31s\tremaining: 26.6s\n",
            "613:\tlearn: 61.3909922\ttotal: 1m 31s\tremaining: 26.5s\n",
            "614:\tlearn: 61.3434607\ttotal: 1m 31s\tremaining: 26.3s\n",
            "615:\tlearn: 61.2792539\ttotal: 1m 31s\tremaining: 26.2s\n",
            "616:\tlearn: 61.2322313\ttotal: 1m 31s\tremaining: 26s\n",
            "617:\tlearn: 61.2116314\ttotal: 1m 31s\tremaining: 25.9s\n",
            "618:\tlearn: 61.1902456\ttotal: 1m 32s\tremaining: 25.7s\n",
            "619:\tlearn: 61.1632611\ttotal: 1m 32s\tremaining: 25.6s\n",
            "620:\tlearn: 61.1292888\ttotal: 1m 32s\tremaining: 25.4s\n",
            "621:\tlearn: 61.0710687\ttotal: 1m 32s\tremaining: 25.3s\n",
            "622:\tlearn: 61.0443849\ttotal: 1m 32s\tremaining: 25.1s\n",
            "623:\tlearn: 61.0397385\ttotal: 1m 32s\tremaining: 25s\n",
            "624:\tlearn: 60.8960374\ttotal: 1m 33s\tremaining: 24.9s\n",
            "625:\tlearn: 60.8588697\ttotal: 1m 33s\tremaining: 24.7s\n",
            "626:\tlearn: 60.8214775\ttotal: 1m 33s\tremaining: 24.6s\n",
            "627:\tlearn: 60.7441189\ttotal: 1m 33s\tremaining: 24.4s\n",
            "628:\tlearn: 60.7230199\ttotal: 1m 33s\tremaining: 24.3s\n",
            "629:\tlearn: 60.6268540\ttotal: 1m 33s\tremaining: 24.1s\n",
            "630:\tlearn: 60.5808943\ttotal: 1m 33s\tremaining: 24s\n",
            "631:\tlearn: 60.5486918\ttotal: 1m 34s\tremaining: 23.8s\n",
            "632:\tlearn: 60.4270401\ttotal: 1m 34s\tremaining: 23.7s\n",
            "633:\tlearn: 60.4090657\ttotal: 1m 34s\tremaining: 23.5s\n",
            "634:\tlearn: 60.3918415\ttotal: 1m 34s\tremaining: 23.4s\n",
            "635:\tlearn: 60.3412845\ttotal: 1m 34s\tremaining: 23.2s\n",
            "636:\tlearn: 60.3094091\ttotal: 1m 34s\tremaining: 23.1s\n",
            "637:\tlearn: 60.3034504\ttotal: 1m 35s\tremaining: 23s\n",
            "638:\tlearn: 60.2727142\ttotal: 1m 35s\tremaining: 22.8s\n",
            "639:\tlearn: 60.2044322\ttotal: 1m 35s\tremaining: 22.7s\n",
            "640:\tlearn: 60.1681822\ttotal: 1m 35s\tremaining: 22.5s\n",
            "641:\tlearn: 60.1244851\ttotal: 1m 35s\tremaining: 22.4s\n",
            "642:\tlearn: 60.0686040\ttotal: 1m 35s\tremaining: 22.2s\n",
            "643:\tlearn: 60.0364844\ttotal: 1m 36s\tremaining: 22.1s\n",
            "644:\tlearn: 60.0123012\ttotal: 1m 36s\tremaining: 21.9s\n",
            "645:\tlearn: 59.9916877\ttotal: 1m 36s\tremaining: 21.8s\n",
            "646:\tlearn: 59.8946089\ttotal: 1m 36s\tremaining: 21.6s\n",
            "647:\tlearn: 59.7995603\ttotal: 1m 36s\tremaining: 21.5s\n",
            "648:\tlearn: 59.7900228\ttotal: 1m 36s\tremaining: 21.3s\n",
            "649:\tlearn: 59.7536172\ttotal: 1m 36s\tremaining: 21.2s\n",
            "650:\tlearn: 59.7166573\ttotal: 1m 37s\tremaining: 21s\n",
            "651:\tlearn: 59.6882594\ttotal: 1m 37s\tremaining: 20.9s\n",
            "652:\tlearn: 59.6570391\ttotal: 1m 37s\tremaining: 20.7s\n",
            "653:\tlearn: 59.6308631\ttotal: 1m 37s\tremaining: 20.6s\n",
            "654:\tlearn: 59.6251519\ttotal: 1m 37s\tremaining: 20.4s\n",
            "655:\tlearn: 59.5998671\ttotal: 1m 37s\tremaining: 20.3s\n",
            "656:\tlearn: 59.5805994\ttotal: 1m 38s\tremaining: 20.2s\n",
            "657:\tlearn: 59.4975588\ttotal: 1m 38s\tremaining: 20s\n",
            "658:\tlearn: 59.4813758\ttotal: 1m 38s\tremaining: 19.9s\n",
            "659:\tlearn: 59.4463439\ttotal: 1m 38s\tremaining: 19.7s\n",
            "660:\tlearn: 59.4296842\ttotal: 1m 38s\tremaining: 19.6s\n",
            "661:\tlearn: 59.4055939\ttotal: 1m 38s\tremaining: 19.4s\n",
            "662:\tlearn: 59.3951530\ttotal: 1m 39s\tremaining: 19.3s\n",
            "663:\tlearn: 59.3584002\ttotal: 1m 39s\tremaining: 19.1s\n",
            "664:\tlearn: 59.3418952\ttotal: 1m 39s\tremaining: 19s\n",
            "665:\tlearn: 59.3217863\ttotal: 1m 39s\tremaining: 18.8s\n",
            "666:\tlearn: 59.3082222\ttotal: 1m 39s\tremaining: 18.7s\n",
            "667:\tlearn: 59.2790028\ttotal: 1m 39s\tremaining: 18.6s\n",
            "668:\tlearn: 59.2776910\ttotal: 1m 40s\tremaining: 18.4s\n",
            "669:\tlearn: 59.2606786\ttotal: 1m 40s\tremaining: 18.3s\n",
            "670:\tlearn: 59.2486485\ttotal: 1m 40s\tremaining: 18.1s\n",
            "671:\tlearn: 59.2038633\ttotal: 1m 40s\tremaining: 18s\n",
            "672:\tlearn: 59.1792847\ttotal: 1m 40s\tremaining: 17.8s\n",
            "673:\tlearn: 59.1541128\ttotal: 1m 40s\tremaining: 17.7s\n",
            "674:\tlearn: 59.0526889\ttotal: 1m 41s\tremaining: 17.5s\n",
            "675:\tlearn: 59.0147337\ttotal: 1m 41s\tremaining: 17.4s\n",
            "676:\tlearn: 58.9474677\ttotal: 1m 41s\tremaining: 17.2s\n",
            "677:\tlearn: 58.9014940\ttotal: 1m 41s\tremaining: 17.1s\n",
            "678:\tlearn: 58.8452067\ttotal: 1m 41s\tremaining: 16.9s\n",
            "679:\tlearn: 58.7357298\ttotal: 1m 41s\tremaining: 16.8s\n",
            "680:\tlearn: 58.6996121\ttotal: 1m 41s\tremaining: 16.6s\n",
            "681:\tlearn: 58.6773022\ttotal: 1m 42s\tremaining: 16.5s\n",
            "682:\tlearn: 58.6125615\ttotal: 1m 42s\tremaining: 16.3s\n",
            "683:\tlearn: 58.6021368\ttotal: 1m 42s\tremaining: 16.2s\n",
            "684:\tlearn: 58.5863307\ttotal: 1m 42s\tremaining: 16s\n",
            "685:\tlearn: 58.5724407\ttotal: 1m 42s\tremaining: 15.9s\n",
            "686:\tlearn: 58.5525507\ttotal: 1m 42s\tremaining: 15.7s\n",
            "687:\tlearn: 58.5230510\ttotal: 1m 43s\tremaining: 15.6s\n",
            "688:\tlearn: 58.4465654\ttotal: 1m 43s\tremaining: 15.4s\n",
            "689:\tlearn: 58.4038435\ttotal: 1m 43s\tremaining: 15.3s\n",
            "690:\tlearn: 58.3766375\ttotal: 1m 43s\tremaining: 15.1s\n",
            "691:\tlearn: 58.3104966\ttotal: 1m 43s\tremaining: 15s\n",
            "692:\tlearn: 58.2868385\ttotal: 1m 43s\tremaining: 14.8s\n",
            "693:\tlearn: 58.2705460\ttotal: 1m 43s\tremaining: 14.7s\n",
            "694:\tlearn: 58.1082391\ttotal: 1m 44s\tremaining: 14.5s\n",
            "695:\tlearn: 58.0790495\ttotal: 1m 44s\tremaining: 14.4s\n",
            "696:\tlearn: 57.9835542\ttotal: 1m 44s\tremaining: 14.2s\n",
            "697:\tlearn: 57.9623443\ttotal: 1m 44s\tremaining: 14.1s\n",
            "698:\tlearn: 57.9125582\ttotal: 1m 44s\tremaining: 13.9s\n",
            "699:\tlearn: 57.8910236\ttotal: 1m 44s\tremaining: 13.8s\n",
            "700:\tlearn: 57.8052398\ttotal: 1m 44s\tremaining: 13.6s\n",
            "701:\tlearn: 57.7920934\ttotal: 1m 45s\tremaining: 13.5s\n",
            "702:\tlearn: 57.6990277\ttotal: 1m 45s\tremaining: 13.3s\n",
            "703:\tlearn: 57.6801961\ttotal: 1m 45s\tremaining: 13.2s\n",
            "704:\tlearn: 57.6302206\ttotal: 1m 45s\tremaining: 13s\n",
            "705:\tlearn: 57.5792360\ttotal: 1m 45s\tremaining: 12.9s\n",
            "706:\tlearn: 57.5522718\ttotal: 1m 45s\tremaining: 12.7s\n",
            "707:\tlearn: 57.4624285\ttotal: 1m 46s\tremaining: 12.6s\n",
            "708:\tlearn: 57.4377976\ttotal: 1m 46s\tremaining: 12.4s\n",
            "709:\tlearn: 57.4124290\ttotal: 1m 46s\tremaining: 12.3s\n",
            "710:\tlearn: 57.2957364\ttotal: 1m 46s\tremaining: 12.1s\n",
            "711:\tlearn: 57.1937603\ttotal: 1m 46s\tremaining: 12s\n",
            "712:\tlearn: 57.1840419\ttotal: 1m 46s\tremaining: 11.8s\n",
            "713:\tlearn: 57.1207024\ttotal: 1m 46s\tremaining: 11.7s\n",
            "714:\tlearn: 57.0831535\ttotal: 1m 47s\tremaining: 11.5s\n",
            "715:\tlearn: 57.0134849\ttotal: 1m 47s\tremaining: 11.4s\n",
            "716:\tlearn: 56.9586341\ttotal: 1m 47s\tremaining: 11.2s\n",
            "717:\tlearn: 56.9307213\ttotal: 1m 47s\tremaining: 11.1s\n",
            "718:\tlearn: 56.8856564\ttotal: 1m 47s\tremaining: 10.9s\n",
            "719:\tlearn: 56.8555472\ttotal: 1m 47s\tremaining: 10.8s\n",
            "720:\tlearn: 56.8120241\ttotal: 1m 47s\tremaining: 10.6s\n",
            "721:\tlearn: 56.7917828\ttotal: 1m 48s\tremaining: 10.5s\n",
            "722:\tlearn: 56.7134307\ttotal: 1m 48s\tremaining: 10.3s\n",
            "723:\tlearn: 56.6502765\ttotal: 1m 48s\tremaining: 10.2s\n",
            "724:\tlearn: 56.5747488\ttotal: 1m 48s\tremaining: 10s\n",
            "725:\tlearn: 56.5153960\ttotal: 1m 48s\tremaining: 9.89s\n",
            "726:\tlearn: 56.4099682\ttotal: 1m 48s\tremaining: 9.74s\n",
            "727:\tlearn: 56.3040485\ttotal: 1m 49s\tremaining: 9.59s\n",
            "728:\tlearn: 56.2795497\ttotal: 1m 49s\tremaining: 9.44s\n",
            "729:\tlearn: 56.2583124\ttotal: 1m 49s\tremaining: 9.29s\n",
            "730:\tlearn: 56.2424874\ttotal: 1m 49s\tremaining: 9.14s\n",
            "731:\tlearn: 56.2092594\ttotal: 1m 49s\tremaining: 8.99s\n",
            "732:\tlearn: 56.1746238\ttotal: 1m 49s\tremaining: 8.84s\n",
            "733:\tlearn: 56.1563317\ttotal: 1m 50s\tremaining: 8.69s\n",
            "734:\tlearn: 56.0732016\ttotal: 1m 50s\tremaining: 8.54s\n",
            "735:\tlearn: 56.0677290\ttotal: 1m 50s\tremaining: 8.39s\n",
            "736:\tlearn: 55.9763308\ttotal: 1m 50s\tremaining: 8.24s\n",
            "737:\tlearn: 55.9407115\ttotal: 1m 50s\tremaining: 8.09s\n",
            "738:\tlearn: 55.8878520\ttotal: 1m 50s\tremaining: 7.94s\n",
            "739:\tlearn: 55.8842910\ttotal: 1m 50s\tremaining: 7.79s\n",
            "740:\tlearn: 55.8487898\ttotal: 1m 51s\tremaining: 7.65s\n",
            "741:\tlearn: 55.8206457\ttotal: 1m 51s\tremaining: 7.5s\n",
            "742:\tlearn: 55.7847898\ttotal: 1m 51s\tremaining: 7.35s\n",
            "743:\tlearn: 55.7499340\ttotal: 1m 51s\tremaining: 7.2s\n",
            "744:\tlearn: 55.7076257\ttotal: 1m 51s\tremaining: 7.05s\n",
            "745:\tlearn: 55.6911726\ttotal: 1m 51s\tremaining: 6.9s\n",
            "746:\tlearn: 55.6439496\ttotal: 1m 52s\tremaining: 6.75s\n",
            "747:\tlearn: 55.6166661\ttotal: 1m 52s\tremaining: 6.6s\n",
            "748:\tlearn: 55.5100894\ttotal: 1m 52s\tremaining: 6.45s\n",
            "749:\tlearn: 55.4564409\ttotal: 1m 52s\tremaining: 6.3s\n",
            "750:\tlearn: 55.4144073\ttotal: 1m 52s\tremaining: 6.15s\n",
            "751:\tlearn: 55.3668087\ttotal: 1m 52s\tremaining: 6s\n",
            "752:\tlearn: 55.3235270\ttotal: 1m 52s\tremaining: 5.85s\n",
            "753:\tlearn: 55.2950601\ttotal: 1m 53s\tremaining: 5.7s\n",
            "754:\tlearn: 55.2882467\ttotal: 1m 53s\tremaining: 5.55s\n",
            "755:\tlearn: 55.2493632\ttotal: 1m 53s\tremaining: 5.4s\n",
            "756:\tlearn: 55.2180204\ttotal: 1m 53s\tremaining: 5.25s\n",
            "757:\tlearn: 55.2059392\ttotal: 1m 53s\tremaining: 5.1s\n",
            "758:\tlearn: 55.1822252\ttotal: 1m 53s\tremaining: 4.95s\n",
            "759:\tlearn: 55.1726068\ttotal: 1m 54s\tremaining: 4.8s\n",
            "760:\tlearn: 55.1419340\ttotal: 1m 54s\tremaining: 4.65s\n",
            "761:\tlearn: 55.1081924\ttotal: 1m 54s\tremaining: 4.5s\n",
            "762:\tlearn: 55.0802693\ttotal: 1m 54s\tremaining: 4.35s\n",
            "763:\tlearn: 55.0344913\ttotal: 1m 54s\tremaining: 4.2s\n",
            "764:\tlearn: 55.0056173\ttotal: 1m 54s\tremaining: 4.05s\n",
            "765:\tlearn: 54.9927905\ttotal: 1m 54s\tremaining: 3.9s\n",
            "766:\tlearn: 54.9334475\ttotal: 1m 55s\tremaining: 3.75s\n",
            "767:\tlearn: 54.9011451\ttotal: 1m 55s\tremaining: 3.6s\n",
            "768:\tlearn: 54.8865945\ttotal: 1m 55s\tremaining: 3.45s\n",
            "769:\tlearn: 54.8541485\ttotal: 1m 55s\tremaining: 3.3s\n",
            "770:\tlearn: 54.8044033\ttotal: 1m 55s\tremaining: 3.15s\n",
            "771:\tlearn: 54.7210571\ttotal: 1m 55s\tremaining: 3s\n",
            "772:\tlearn: 54.6557696\ttotal: 1m 55s\tremaining: 2.85s\n",
            "773:\tlearn: 54.6036393\ttotal: 1m 56s\tremaining: 2.7s\n",
            "774:\tlearn: 54.5398570\ttotal: 1m 56s\tremaining: 2.55s\n",
            "775:\tlearn: 54.4884625\ttotal: 1m 56s\tremaining: 2.4s\n",
            "776:\tlearn: 54.4697884\ttotal: 1m 56s\tremaining: 2.25s\n",
            "777:\tlearn: 54.4297841\ttotal: 1m 56s\tremaining: 2.1s\n",
            "778:\tlearn: 54.3724399\ttotal: 1m 56s\tremaining: 1.95s\n",
            "779:\tlearn: 54.3517578\ttotal: 1m 56s\tremaining: 1.8s\n",
            "780:\tlearn: 54.3268702\ttotal: 1m 57s\tremaining: 1.65s\n",
            "781:\tlearn: 54.2944148\ttotal: 1m 57s\tremaining: 1.5s\n",
            "782:\tlearn: 54.2630789\ttotal: 1m 57s\tremaining: 1.35s\n",
            "783:\tlearn: 54.2591218\ttotal: 1m 57s\tremaining: 1.2s\n",
            "784:\tlearn: 54.1495151\ttotal: 1m 57s\tremaining: 1.05s\n",
            "785:\tlearn: 54.1048685\ttotal: 1m 57s\tremaining: 900ms\n",
            "786:\tlearn: 54.0450739\ttotal: 1m 58s\tremaining: 750ms\n",
            "787:\tlearn: 54.0245056\ttotal: 1m 58s\tremaining: 600ms\n",
            "788:\tlearn: 53.9673107\ttotal: 1m 58s\tremaining: 450ms\n",
            "789:\tlearn: 53.9240161\ttotal: 1m 58s\tremaining: 300ms\n",
            "790:\tlearn: 53.9001067\ttotal: 1m 58s\tremaining: 150ms\n",
            "791:\tlearn: 53.8704109\ttotal: 1m 58s\tremaining: 0us\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 05:35:54,142]\u001b[0m A new study created in memory with name: no-name-9a45990b-9b73-4ff6-9786-80073f391832\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-06 05:38:13,591]\u001b[0m Trial 0 finished with value: 57.20639981749408 and parameters: {'iterations': 921, 'learning_rate': 0.7313877507251003, 'depth': 10, 'min_data_in_leaf': 15, 'reg_lambda': 30.77074089604671, 'subsample': 0.5286843392696118, 'random_strength': 54.372999344451735, 'od_wait': 51, 'leaf_estimation_iterations': 18, 'bagging_temperature': 1.0913808956698392, 'colsample_bylevel': 0.36753066317430727}. Best is trial 0 with value: 57.20639981749408.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:40:26,699]\u001b[0m Trial 1 finished with value: 46.48582358805089 and parameters: {'iterations': 433, 'learning_rate': 0.5899605678803301, 'depth': 9, 'min_data_in_leaf': 15, 'reg_lambda': 62.223289044475145, 'subsample': 0.9759050658077331, 'random_strength': 30.55733714009755, 'od_wait': 125, 'leaf_estimation_iterations': 20, 'bagging_temperature': 10.099229114607697, 'colsample_bylevel': 0.8290967984097537}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:42:35,209]\u001b[0m Trial 2 finished with value: 46.53512271328078 and parameters: {'iterations': 410, 'learning_rate': 0.4506669808617264, 'depth': 10, 'min_data_in_leaf': 3, 'reg_lambda': 88.24185968624417, 'subsample': 0.5767874828540265, 'random_strength': 34.121820356029986, 'od_wait': 17, 'leaf_estimation_iterations': 10, 'bagging_temperature': 3.222866960723397, 'colsample_bylevel': 0.741754742017198}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:57:39,962]\u001b[0m Trial 3 finished with value: 77.91200544407783 and parameters: {'iterations': 512, 'learning_rate': 0.9588177481123491, 'depth': 16, 'min_data_in_leaf': 26, 'reg_lambda': 53.028201021938344, 'subsample': 0.32173788557507876, 'random_strength': 60.97801011201156, 'od_wait': 56, 'leaf_estimation_iterations': 6, 'bagging_temperature': 26.84050895232499, 'colsample_bylevel': 0.9263474970320034}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 05:59:56,575]\u001b[0m Trial 4 finished with value: 55.88042439912455 and parameters: {'iterations': 382, 'learning_rate': 0.6281929865343184, 'depth': 11, 'min_data_in_leaf': 23, 'reg_lambda': 67.48160197118058, 'subsample': 0.46680635854075814, 'random_strength': 38.12078536449377, 'od_wait': 117, 'leaf_estimation_iterations': 12, 'bagging_temperature': 17.060345470203735, 'colsample_bylevel': 0.35649213290734616}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:17:25,326]\u001b[0m Trial 5 finished with value: 64.88535161091701 and parameters: {'iterations': 967, 'learning_rate': 0.5579895684861491, 'depth': 16, 'min_data_in_leaf': 10, 'reg_lambda': 41.81684195820015, 'subsample': 0.8918888324531753, 'random_strength': 26.091438589017304, 'od_wait': 43, 'leaf_estimation_iterations': 8, 'bagging_temperature': 4.953607348912727, 'colsample_bylevel': 0.3855567490790124}. Best is trial 1 with value: 46.48582358805089.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:20:39,902]\u001b[0m Trial 6 finished with value: 46.001003133443334 and parameters: {'iterations': 943, 'learning_rate': 0.21140082703780222, 'depth': 8, 'min_data_in_leaf': 6, 'reg_lambda': 51.66819605723886, 'subsample': 0.825936386500262, 'random_strength': 31.767995963528005, 'od_wait': 77, 'leaf_estimation_iterations': 10, 'bagging_temperature': 34.298229258161875, 'colsample_bylevel': 0.6925484867824061}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:23:23,039]\u001b[0m Trial 7 finished with value: 50.71947372107086 and parameters: {'iterations': 965, 'learning_rate': 0.8912628123865083, 'depth': 8, 'min_data_in_leaf': 16, 'reg_lambda': 93.09697064434633, 'subsample': 0.35478605686033665, 'random_strength': 74.86793792254224, 'od_wait': 73, 'leaf_estimation_iterations': 4, 'bagging_temperature': 2.8833042884114404, 'colsample_bylevel': 0.8290567414715476}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:25:00,558]\u001b[0m Trial 8 finished with value: 67.84803326366799 and parameters: {'iterations': 851, 'learning_rate': 0.7293253542469601, 'depth': 10, 'min_data_in_leaf': 21, 'reg_lambda': 32.807396831469354, 'subsample': 0.8014562684416848, 'random_strength': 47.44658664226404, 'od_wait': 74, 'leaf_estimation_iterations': 5, 'bagging_temperature': 6.756786290556164, 'colsample_bylevel': 0.07590413079547753}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:41:41,840]\u001b[0m Trial 9 finished with value: 56.31175308973086 and parameters: {'iterations': 883, 'learning_rate': 0.5127880818019495, 'depth': 14, 'min_data_in_leaf': 5, 'reg_lambda': 33.214674802274374, 'subsample': 0.7647158181683289, 'random_strength': 38.10746992849887, 'od_wait': 149, 'leaf_estimation_iterations': 6, 'bagging_temperature': 33.967723059727184, 'colsample_bylevel': 0.9837466574356587}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:42:45,157]\u001b[0m Trial 10 finished with value: 80.50860446373237 and parameters: {'iterations': 709, 'learning_rate': 0.12202047326049932, 'depth': 5, 'min_data_in_leaf': 1, 'reg_lambda': 76.24576702057104, 'subsample': 0.7108862525165657, 'random_strength': 97.24491447481724, 'od_wait': 106, 'leaf_estimation_iterations': 1, 'bagging_temperature': 69.95005454658138, 'colsample_bylevel': 0.6040405053578852}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:44:59,863]\u001b[0m Trial 11 finished with value: 54.871069202346874 and parameters: {'iterations': 629, 'learning_rate': 0.21833643647270726, 'depth': 7, 'min_data_in_leaf': 10, 'reg_lambda': 55.54864045530132, 'subsample': 0.9997353402587645, 'random_strength': 13.017338441324387, 'od_wait': 107, 'leaf_estimation_iterations': 20, 'bagging_temperature': 99.66157629574, 'colsample_bylevel': 0.6550388874489359}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:47:11,251]\u001b[0m Trial 12 finished with value: 47.42126402536329 and parameters: {'iterations': 732, 'learning_rate': 0.3353168481850217, 'depth': 7, 'min_data_in_leaf': 9, 'reg_lambda': 55.768778894287756, 'subsample': 0.9669447478647194, 'random_strength': 17.767083773926824, 'od_wait': 148, 'leaf_estimation_iterations': 14, 'bagging_temperature': 12.841276123711907, 'colsample_bylevel': 0.5247297179758806}. Best is trial 6 with value: 46.001003133443334.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 06:54:27,717]\u001b[0m Trial 13 finished with value: 45.38507370774545 and parameters: {'iterations': 533, 'learning_rate': 0.3582220796656632, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 69.46275838928238, 'subsample': 0.8540806581123832, 'random_strength': 28.159360754861588, 'od_wait': 125, 'leaf_estimation_iterations': 15, 'bagging_temperature': 49.644424887945355, 'colsample_bylevel': 0.7998758691220191}. Best is trial 13 with value: 45.38507370774545.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:08:20,730]\u001b[0m Trial 14 finished with value: 48.41706671254681 and parameters: {'iterations': 570, 'learning_rate': 0.3473522362771956, 'depth': 13, 'min_data_in_leaf': 27, 'reg_lambda': 79.01726191500387, 'subsample': 0.8397598233779698, 'random_strength': 67.36611186701461, 'od_wait': 91, 'leaf_estimation_iterations': 15, 'bagging_temperature': 36.02291324873896, 'colsample_bylevel': 0.7183006087448827}. Best is trial 13 with value: 45.38507370774545.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:18:01,062]\u001b[0m Trial 15 finished with value: 43.48887650381214 and parameters: {'iterations': 783, 'learning_rate': 0.2584880573013857, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 45.02231140060744, 'subsample': 0.6753500090427036, 'random_strength': 21.788829786258407, 'od_wait': 90, 'leaf_estimation_iterations': 16, 'bagging_temperature': 58.67357912276298, 'colsample_bylevel': 0.5379457040607726}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:25:34,068]\u001b[0m Trial 16 finished with value: 50.48731219910578 and parameters: {'iterations': 790, 'learning_rate': 0.36827523538520046, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 71.36224159383978, 'subsample': 0.6728869610125017, 'random_strength': 22.248322331820447, 'od_wait': 93, 'leaf_estimation_iterations': 16, 'bagging_temperature': 68.07824674104201, 'colsample_bylevel': 0.1566786862837674}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:34:49,001]\u001b[0m Trial 17 finished with value: 51.794375401241304 and parameters: {'iterations': 309, 'learning_rate': 0.23406832707723646, 'depth': 14, 'min_data_in_leaf': 30, 'reg_lambda': 43.32432635230013, 'subsample': 0.6160947515585349, 'random_strength': 10.274637826660694, 'od_wait': 128, 'leaf_estimation_iterations': 13, 'bagging_temperature': 59.49424678491784, 'colsample_bylevel': 0.4840114290491303}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:39:22,886]\u001b[0m Trial 18 finished with value: 55.79856928137143 and parameters: {'iterations': 527, 'learning_rate': 0.10874463140581817, 'depth': 12, 'min_data_in_leaf': 20, 'reg_lambda': 85.69461753584385, 'subsample': 0.7309339623530247, 'random_strength': 44.07889631754874, 'od_wait': 134, 'leaf_estimation_iterations': 16, 'bagging_temperature': 20.40268545253036, 'colsample_bylevel': 0.2640711272382002}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n",
            "\u001b[32m[I 2023-10-06 07:57:40,488]\u001b[0m Trial 19 finished with value: 58.93536158593662 and parameters: {'iterations': 638, 'learning_rate': 0.4175147995093469, 'depth': 14, 'min_data_in_leaf': 26, 'reg_lambda': 43.908029962396185, 'subsample': 0.8923653279928291, 'random_strength': 81.67179289193317, 'od_wait': 102, 'leaf_estimation_iterations': 18, 'bagging_temperature': 49.18172333090726, 'colsample_bylevel': 0.5389911542559457}. Best is trial 15 with value: 43.48887650381214.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best RMSE for fold 5: 43.48887650381214\n",
            "Best hyperparameters for fold 5: {'iterations': 783, 'learning_rate': 0.2584880573013857, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 45.02231140060744, 'subsample': 0.6753500090427036, 'random_strength': 21.788829786258407, 'od_wait': 90, 'leaf_estimation_iterations': 16, 'bagging_temperature': 58.67357912276298, 'colsample_bylevel': 0.5379457040607726}\n",
            "0:\tlearn: 205.2582218\ttotal: 697ms\tremaining: 9m 5s\n",
            "1:\tlearn: 204.2043019\ttotal: 886ms\tremaining: 5m 45s\n",
            "2:\tlearn: 203.0624090\ttotal: 1.01s\tremaining: 4m 23s\n",
            "3:\tlearn: 202.0806996\ttotal: 1.23s\tremaining: 3m 59s\n",
            "4:\tlearn: 200.1313413\ttotal: 1.92s\tremaining: 4m 59s\n",
            "5:\tlearn: 198.2753633\ttotal: 2.61s\tremaining: 5m 37s\n",
            "6:\tlearn: 197.5619923\ttotal: 3.39s\tremaining: 6m 16s\n",
            "7:\tlearn: 197.5466068\ttotal: 3.45s\tremaining: 5m 34s\n",
            "8:\tlearn: 197.1022517\ttotal: 3.64s\tremaining: 5m 13s\n",
            "9:\tlearn: 196.9068203\ttotal: 3.75s\tremaining: 4m 50s\n",
            "10:\tlearn: 196.8625524\ttotal: 3.9s\tremaining: 4m 33s\n",
            "11:\tlearn: 195.7974895\ttotal: 4.6s\tremaining: 4m 55s\n",
            "12:\tlearn: 195.5563617\ttotal: 5s\tremaining: 4m 56s\n",
            "13:\tlearn: 195.5528132\ttotal: 5.07s\tremaining: 4m 38s\n",
            "14:\tlearn: 195.4150949\ttotal: 5.32s\tremaining: 4m 32s\n",
            "15:\tlearn: 194.7677278\ttotal: 6.05s\tremaining: 4m 50s\n",
            "16:\tlearn: 194.7328622\ttotal: 6.19s\tremaining: 4m 38s\n",
            "17:\tlearn: 194.5833479\ttotal: 6.36s\tremaining: 4m 30s\n",
            "18:\tlearn: 194.5630793\ttotal: 6.44s\tremaining: 4m 18s\n",
            "19:\tlearn: 194.0990671\ttotal: 6.6s\tremaining: 4m 11s\n",
            "20:\tlearn: 194.0966539\ttotal: 6.66s\tremaining: 4m 1s\n",
            "21:\tlearn: 194.0334212\ttotal: 6.78s\tremaining: 3m 54s\n",
            "22:\tlearn: 193.8010498\ttotal: 7.53s\tremaining: 4m 8s\n",
            "23:\tlearn: 193.8005653\ttotal: 7.59s\tremaining: 4m\n",
            "24:\tlearn: 192.2456125\ttotal: 8.28s\tremaining: 4m 11s\n",
            "25:\tlearn: 191.4975149\ttotal: 8.98s\tremaining: 4m 21s\n",
            "26:\tlearn: 191.1153200\ttotal: 9.66s\tremaining: 4m 30s\n",
            "27:\tlearn: 191.0134430\ttotal: 10.4s\tremaining: 4m 40s\n",
            "28:\tlearn: 190.3881858\ttotal: 11.1s\tremaining: 4m 48s\n",
            "29:\tlearn: 190.3664000\ttotal: 11.2s\tremaining: 4m 41s\n",
            "30:\tlearn: 189.7721080\ttotal: 11.4s\tremaining: 4m 35s\n",
            "31:\tlearn: 189.3626907\ttotal: 11.5s\tremaining: 4m 28s\n",
            "32:\tlearn: 189.3574213\ttotal: 11.5s\tremaining: 4m 22s\n",
            "33:\tlearn: 189.3170066\ttotal: 11.7s\tremaining: 4m 17s\n",
            "34:\tlearn: 188.5953449\ttotal: 11.9s\tremaining: 4m 14s\n",
            "35:\tlearn: 188.5953445\ttotal: 12s\tremaining: 4m 8s\n",
            "36:\tlearn: 188.5952932\ttotal: 12s\tremaining: 4m 2s\n",
            "37:\tlearn: 188.5924858\ttotal: 12.1s\tremaining: 3m 57s\n",
            "38:\tlearn: 188.5851530\ttotal: 12.2s\tremaining: 3m 53s\n",
            "39:\tlearn: 188.5845305\ttotal: 12.3s\tremaining: 3m 48s\n",
            "40:\tlearn: 188.4670431\ttotal: 12.5s\tremaining: 3m 46s\n",
            "41:\tlearn: 188.0799683\ttotal: 13.2s\tremaining: 3m 52s\n",
            "42:\tlearn: 187.3781470\ttotal: 13.8s\tremaining: 3m 58s\n",
            "43:\tlearn: 187.0137253\ttotal: 14.5s\tremaining: 4m 4s\n",
            "44:\tlearn: 186.9213084\ttotal: 14.6s\tremaining: 4m\n",
            "45:\tlearn: 186.5733896\ttotal: 15.4s\tremaining: 4m 6s\n",
            "46:\tlearn: 186.5654044\ttotal: 15.5s\tremaining: 4m 2s\n",
            "47:\tlearn: 186.0539578\ttotal: 16.2s\tremaining: 4m 7s\n",
            "48:\tlearn: 185.9152955\ttotal: 16.9s\tremaining: 4m 13s\n",
            "49:\tlearn: 185.7445747\ttotal: 17.6s\tremaining: 4m 17s\n",
            "50:\tlearn: 185.0050928\ttotal: 17.7s\tremaining: 4m 14s\n",
            "51:\tlearn: 184.4358044\ttotal: 18.4s\tremaining: 4m 18s\n",
            "52:\tlearn: 184.3113793\ttotal: 18.5s\tremaining: 4m 15s\n",
            "53:\tlearn: 179.8821559\ttotal: 19.2s\tremaining: 4m 18s\n",
            "54:\tlearn: 179.4633260\ttotal: 19.9s\tremaining: 4m 23s\n",
            "55:\tlearn: 174.3650744\ttotal: 20.5s\tremaining: 4m 26s\n",
            "56:\tlearn: 174.3339013\ttotal: 20.6s\tremaining: 4m 22s\n",
            "57:\tlearn: 173.4821138\ttotal: 21.4s\tremaining: 4m 26s\n",
            "58:\tlearn: 170.6873931\ttotal: 22s\tremaining: 4m 29s\n",
            "59:\tlearn: 169.5589200\ttotal: 22.7s\tremaining: 4m 33s\n",
            "60:\tlearn: 168.7746577\ttotal: 23.3s\tremaining: 4m 35s\n",
            "61:\tlearn: 167.6970339\ttotal: 23.9s\tremaining: 4m 37s\n",
            "62:\tlearn: 166.7728626\ttotal: 24.5s\tremaining: 4m 39s\n",
            "63:\tlearn: 163.1831319\ttotal: 25.1s\tremaining: 4m 42s\n",
            "64:\tlearn: 159.9892709\ttotal: 25.7s\tremaining: 4m 43s\n",
            "65:\tlearn: 157.3274977\ttotal: 26.3s\tremaining: 4m 45s\n",
            "66:\tlearn: 155.5405567\ttotal: 26.9s\tremaining: 4m 47s\n",
            "67:\tlearn: 154.0512526\ttotal: 27.4s\tremaining: 4m 48s\n",
            "68:\tlearn: 152.5627027\ttotal: 28s\tremaining: 4m 49s\n",
            "69:\tlearn: 151.8313768\ttotal: 28.6s\tremaining: 4m 51s\n",
            "70:\tlearn: 150.6744616\ttotal: 29.2s\tremaining: 4m 53s\n",
            "71:\tlearn: 148.5478697\ttotal: 29.8s\tremaining: 4m 54s\n",
            "72:\tlearn: 146.1305968\ttotal: 30.3s\tremaining: 4m 54s\n",
            "73:\tlearn: 143.3174088\ttotal: 30.9s\tremaining: 4m 55s\n",
            "74:\tlearn: 142.3827071\ttotal: 31.6s\tremaining: 4m 58s\n",
            "75:\tlearn: 141.4858803\ttotal: 32.2s\tremaining: 4m 59s\n",
            "76:\tlearn: 140.0506573\ttotal: 32.8s\tremaining: 5m\n",
            "77:\tlearn: 139.1223777\ttotal: 33.4s\tremaining: 5m 1s\n",
            "78:\tlearn: 138.3133799\ttotal: 33.9s\tremaining: 5m 2s\n",
            "79:\tlearn: 137.4140883\ttotal: 34.6s\tremaining: 5m 4s\n",
            "80:\tlearn: 135.5996477\ttotal: 35.1s\tremaining: 5m 4s\n",
            "81:\tlearn: 133.3842636\ttotal: 35.6s\tremaining: 5m 4s\n",
            "82:\tlearn: 132.6716511\ttotal: 36.2s\tremaining: 5m 5s\n",
            "83:\tlearn: 131.9649255\ttotal: 36.9s\tremaining: 5m 7s\n",
            "84:\tlearn: 131.5524589\ttotal: 37.1s\tremaining: 5m 4s\n",
            "85:\tlearn: 130.3050949\ttotal: 37.6s\tremaining: 5m 5s\n",
            "86:\tlearn: 129.3506829\ttotal: 38.2s\tremaining: 5m 5s\n",
            "87:\tlearn: 128.8139323\ttotal: 38.9s\tremaining: 5m 7s\n",
            "88:\tlearn: 127.6534784\ttotal: 39.5s\tremaining: 5m 7s\n",
            "89:\tlearn: 127.1878359\ttotal: 40.3s\tremaining: 5m 9s\n",
            "90:\tlearn: 125.3087942\ttotal: 40.8s\tremaining: 5m 10s\n",
            "91:\tlearn: 124.6500328\ttotal: 41.5s\tremaining: 5m 11s\n",
            "92:\tlearn: 123.4858986\ttotal: 42.1s\tremaining: 5m 12s\n",
            "93:\tlearn: 122.6510694\ttotal: 42.7s\tremaining: 5m 12s\n",
            "94:\tlearn: 121.6690360\ttotal: 43.2s\tremaining: 5m 13s\n",
            "95:\tlearn: 120.7220411\ttotal: 43.8s\tremaining: 5m 13s\n",
            "96:\tlearn: 120.1693591\ttotal: 44.4s\tremaining: 5m 14s\n",
            "97:\tlearn: 119.6674074\ttotal: 45.1s\tremaining: 5m 15s\n",
            "98:\tlearn: 118.5808625\ttotal: 45.7s\tremaining: 5m 15s\n",
            "99:\tlearn: 117.8960115\ttotal: 46.3s\tremaining: 5m 16s\n",
            "100:\tlearn: 117.8365532\ttotal: 47s\tremaining: 5m 17s\n",
            "101:\tlearn: 117.4055944\ttotal: 47.7s\tremaining: 5m 18s\n",
            "102:\tlearn: 116.3774963\ttotal: 48.3s\tremaining: 5m 18s\n",
            "103:\tlearn: 115.7535861\ttotal: 48.9s\tremaining: 5m 19s\n",
            "104:\tlearn: 115.1051666\ttotal: 49.5s\tremaining: 5m 19s\n",
            "105:\tlearn: 114.1374441\ttotal: 50s\tremaining: 5m 19s\n",
            "106:\tlearn: 113.4097496\ttotal: 50.6s\tremaining: 5m 19s\n",
            "107:\tlearn: 113.1207241\ttotal: 51.2s\tremaining: 5m 19s\n",
            "108:\tlearn: 112.7457337\ttotal: 51.8s\tremaining: 5m 20s\n",
            "109:\tlearn: 112.2990859\ttotal: 52.4s\tremaining: 5m 20s\n",
            "110:\tlearn: 111.8520897\ttotal: 53s\tremaining: 5m 20s\n",
            "111:\tlearn: 111.1440661\ttotal: 53.6s\tremaining: 5m 20s\n",
            "112:\tlearn: 110.8162098\ttotal: 54.1s\tremaining: 5m 20s\n",
            "113:\tlearn: 110.3511621\ttotal: 54.7s\tremaining: 5m 21s\n",
            "114:\tlearn: 109.6671280\ttotal: 55.3s\tremaining: 5m 21s\n",
            "115:\tlearn: 108.2785391\ttotal: 55.9s\tremaining: 5m 21s\n",
            "116:\tlearn: 107.8243424\ttotal: 56.5s\tremaining: 5m 21s\n",
            "117:\tlearn: 107.5440460\ttotal: 57.1s\tremaining: 5m 21s\n",
            "118:\tlearn: 107.0519033\ttotal: 57.7s\tremaining: 5m 22s\n",
            "119:\tlearn: 106.2106836\ttotal: 58.3s\tremaining: 5m 22s\n",
            "120:\tlearn: 105.2724782\ttotal: 58.8s\tremaining: 5m 21s\n",
            "121:\tlearn: 104.7771442\ttotal: 59.4s\tremaining: 5m 21s\n",
            "122:\tlearn: 104.4603940\ttotal: 1m\tremaining: 5m 22s\n",
            "123:\tlearn: 103.6761125\ttotal: 1m\tremaining: 5m 21s\n",
            "124:\tlearn: 103.1935313\ttotal: 1m 1s\tremaining: 5m 22s\n",
            "125:\tlearn: 102.3790608\ttotal: 1m 1s\tremaining: 5m 22s\n",
            "126:\tlearn: 102.0425058\ttotal: 1m 2s\tremaining: 5m 22s\n",
            "127:\tlearn: 101.4040974\ttotal: 1m 3s\tremaining: 5m 22s\n",
            "128:\tlearn: 100.3391653\ttotal: 1m 3s\tremaining: 5m 22s\n",
            "129:\tlearn: 99.7830425\ttotal: 1m 4s\tremaining: 5m 22s\n",
            "130:\tlearn: 99.2970352\ttotal: 1m 4s\tremaining: 5m 22s\n",
            "131:\tlearn: 99.0233446\ttotal: 1m 5s\tremaining: 5m 22s\n",
            "132:\tlearn: 98.7678563\ttotal: 1m 6s\tremaining: 5m 22s\n",
            "133:\tlearn: 98.4083157\ttotal: 1m 6s\tremaining: 5m 22s\n",
            "134:\tlearn: 98.1371940\ttotal: 1m 7s\tremaining: 5m 22s\n",
            "135:\tlearn: 97.6258881\ttotal: 1m 7s\tremaining: 5m 22s\n",
            "136:\tlearn: 96.7808313\ttotal: 1m 8s\tremaining: 5m 22s\n",
            "137:\tlearn: 96.1054475\ttotal: 1m 9s\tremaining: 5m 22s\n",
            "138:\tlearn: 95.6788122\ttotal: 1m 9s\tremaining: 5m 22s\n",
            "139:\tlearn: 95.3078639\ttotal: 1m 10s\tremaining: 5m 22s\n",
            "140:\tlearn: 94.9477548\ttotal: 1m 10s\tremaining: 5m 22s\n",
            "141:\tlearn: 94.4250681\ttotal: 1m 11s\tremaining: 5m 22s\n",
            "142:\tlearn: 94.2216089\ttotal: 1m 12s\tremaining: 5m 22s\n",
            "143:\tlearn: 93.7533412\ttotal: 1m 12s\tremaining: 5m 22s\n",
            "144:\tlearn: 93.5447589\ttotal: 1m 13s\tremaining: 5m 21s\n",
            "145:\tlearn: 92.9855469\ttotal: 1m 13s\tremaining: 5m 21s\n",
            "146:\tlearn: 92.6639535\ttotal: 1m 14s\tremaining: 5m 21s\n",
            "147:\tlearn: 91.8475118\ttotal: 1m 14s\tremaining: 5m 21s\n",
            "148:\tlearn: 91.6232433\ttotal: 1m 15s\tremaining: 5m 21s\n",
            "149:\tlearn: 91.2798941\ttotal: 1m 16s\tremaining: 5m 21s\n",
            "150:\tlearn: 90.6670996\ttotal: 1m 16s\tremaining: 5m 20s\n",
            "151:\tlearn: 90.4724180\ttotal: 1m 17s\tremaining: 5m 21s\n",
            "152:\tlearn: 90.0291767\ttotal: 1m 17s\tremaining: 5m 20s\n",
            "153:\tlearn: 89.4812569\ttotal: 1m 18s\tremaining: 5m 20s\n",
            "154:\tlearn: 89.1915029\ttotal: 1m 19s\tremaining: 5m 20s\n",
            "155:\tlearn: 88.7095959\ttotal: 1m 19s\tremaining: 5m 19s\n",
            "156:\tlearn: 88.3278851\ttotal: 1m 20s\tremaining: 5m 19s\n",
            "157:\tlearn: 87.8235846\ttotal: 1m 20s\tremaining: 5m 19s\n",
            "158:\tlearn: 87.5532449\ttotal: 1m 21s\tremaining: 5m 19s\n",
            "159:\tlearn: 87.2677775\ttotal: 1m 21s\tremaining: 5m 19s\n",
            "160:\tlearn: 87.0726203\ttotal: 1m 22s\tremaining: 5m 19s\n",
            "161:\tlearn: 86.6094932\ttotal: 1m 23s\tremaining: 5m 19s\n",
            "162:\tlearn: 86.4273053\ttotal: 1m 23s\tremaining: 5m 19s\n",
            "163:\tlearn: 85.7673184\ttotal: 1m 24s\tremaining: 5m 19s\n",
            "164:\tlearn: 85.4040354\ttotal: 1m 25s\tremaining: 5m 18s\n",
            "165:\tlearn: 85.1999491\ttotal: 1m 25s\tremaining: 5m 18s\n",
            "166:\tlearn: 84.9621772\ttotal: 1m 26s\tremaining: 5m 18s\n",
            "167:\tlearn: 84.6062781\ttotal: 1m 27s\tremaining: 5m 18s\n",
            "168:\tlearn: 84.1794285\ttotal: 1m 27s\tremaining: 5m 18s\n",
            "169:\tlearn: 83.7615996\ttotal: 1m 28s\tremaining: 5m 18s\n",
            "170:\tlearn: 82.9398595\ttotal: 1m 28s\tremaining: 5m 17s\n",
            "171:\tlearn: 82.6194478\ttotal: 1m 29s\tremaining: 5m 17s\n",
            "172:\tlearn: 81.7443117\ttotal: 1m 29s\tremaining: 5m 17s\n",
            "173:\tlearn: 81.4662223\ttotal: 1m 30s\tremaining: 5m 17s\n",
            "174:\tlearn: 81.3467401\ttotal: 1m 31s\tremaining: 5m 16s\n",
            "175:\tlearn: 80.8408738\ttotal: 1m 31s\tremaining: 5m 16s\n",
            "176:\tlearn: 80.6436904\ttotal: 1m 32s\tremaining: 5m 16s\n",
            "177:\tlearn: 80.2419585\ttotal: 1m 32s\tremaining: 5m 15s\n",
            "178:\tlearn: 80.0795610\ttotal: 1m 33s\tremaining: 5m 15s\n",
            "179:\tlearn: 79.7376477\ttotal: 1m 34s\tremaining: 5m 15s\n",
            "180:\tlearn: 79.5050825\ttotal: 1m 34s\tremaining: 5m 15s\n",
            "181:\tlearn: 79.2301648\ttotal: 1m 35s\tremaining: 5m 14s\n",
            "182:\tlearn: 79.0876139\ttotal: 1m 35s\tremaining: 5m 14s\n",
            "183:\tlearn: 78.9345669\ttotal: 1m 36s\tremaining: 5m 14s\n",
            "184:\tlearn: 78.7648952\ttotal: 1m 37s\tremaining: 5m 13s\n",
            "185:\tlearn: 78.6266448\ttotal: 1m 37s\tremaining: 5m 13s\n",
            "186:\tlearn: 78.4909269\ttotal: 1m 38s\tremaining: 5m 13s\n",
            "187:\tlearn: 78.2841773\ttotal: 1m 39s\tremaining: 5m 13s\n",
            "188:\tlearn: 77.9203038\ttotal: 1m 39s\tremaining: 5m 13s\n",
            "189:\tlearn: 77.6024264\ttotal: 1m 40s\tremaining: 5m 12s\n",
            "190:\tlearn: 77.4380813\ttotal: 1m 40s\tremaining: 5m 12s\n",
            "191:\tlearn: 77.2022733\ttotal: 1m 41s\tremaining: 5m 12s\n",
            "192:\tlearn: 77.0057350\ttotal: 1m 42s\tremaining: 5m 12s\n",
            "193:\tlearn: 76.6233452\ttotal: 1m 42s\tremaining: 5m 11s\n",
            "194:\tlearn: 76.3701704\ttotal: 1m 43s\tremaining: 5m 11s\n",
            "195:\tlearn: 76.2505206\ttotal: 1m 43s\tremaining: 5m 11s\n",
            "196:\tlearn: 76.1133439\ttotal: 1m 44s\tremaining: 5m 10s\n",
            "197:\tlearn: 75.9119226\ttotal: 1m 45s\tremaining: 5m 10s\n",
            "198:\tlearn: 75.5994094\ttotal: 1m 45s\tremaining: 5m 10s\n",
            "199:\tlearn: 75.4896616\ttotal: 1m 46s\tremaining: 5m 10s\n",
            "200:\tlearn: 75.1414828\ttotal: 1m 46s\tremaining: 5m 9s\n",
            "201:\tlearn: 75.0120142\ttotal: 1m 47s\tremaining: 5m 8s\n",
            "202:\tlearn: 74.5245649\ttotal: 1m 47s\tremaining: 5m 8s\n",
            "203:\tlearn: 74.1408265\ttotal: 1m 48s\tremaining: 5m 8s\n",
            "204:\tlearn: 73.8293844\ttotal: 1m 49s\tremaining: 5m 7s\n",
            "205:\tlearn: 73.5035190\ttotal: 1m 49s\tremaining: 5m 7s\n",
            "206:\tlearn: 73.4377219\ttotal: 1m 50s\tremaining: 5m 7s\n",
            "207:\tlearn: 73.2475093\ttotal: 1m 51s\tremaining: 5m 7s\n",
            "208:\tlearn: 72.9557016\ttotal: 1m 51s\tremaining: 5m 7s\n",
            "209:\tlearn: 72.7434648\ttotal: 1m 52s\tremaining: 5m 6s\n",
            "210:\tlearn: 72.3213175\ttotal: 1m 52s\tremaining: 5m 6s\n",
            "211:\tlearn: 72.1219031\ttotal: 1m 53s\tremaining: 5m 5s\n",
            "212:\tlearn: 72.1200984\ttotal: 1m 54s\tremaining: 5m 5s\n",
            "213:\tlearn: 71.8601575\ttotal: 1m 54s\tremaining: 5m 4s\n",
            "214:\tlearn: 71.6209994\ttotal: 1m 55s\tremaining: 5m 4s\n",
            "215:\tlearn: 71.4438724\ttotal: 1m 55s\tremaining: 5m 4s\n",
            "216:\tlearn: 71.1588179\ttotal: 1m 56s\tremaining: 5m 4s\n",
            "217:\tlearn: 71.0286674\ttotal: 1m 57s\tremaining: 5m 3s\n",
            "218:\tlearn: 70.7043551\ttotal: 1m 57s\tremaining: 5m 3s\n",
            "219:\tlearn: 70.2919778\ttotal: 1m 58s\tremaining: 5m 2s\n",
            "220:\tlearn: 69.9596030\ttotal: 1m 58s\tremaining: 5m 2s\n",
            "221:\tlearn: 69.7639830\ttotal: 1m 59s\tremaining: 5m 2s\n",
            "222:\tlearn: 69.6062866\ttotal: 2m\tremaining: 5m 2s\n",
            "223:\tlearn: 69.4224223\ttotal: 2m\tremaining: 5m 1s\n",
            "224:\tlearn: 69.2980152\ttotal: 2m 1s\tremaining: 5m 1s\n",
            "225:\tlearn: 69.1732114\ttotal: 2m 2s\tremaining: 5m\n",
            "226:\tlearn: 69.0263715\ttotal: 2m 2s\tremaining: 5m\n",
            "227:\tlearn: 68.8954782\ttotal: 2m 3s\tremaining: 4m 59s\n",
            "228:\tlearn: 68.6263961\ttotal: 2m 3s\tremaining: 4m 59s\n",
            "229:\tlearn: 68.2997282\ttotal: 2m 4s\tremaining: 4m 59s\n",
            "230:\tlearn: 67.8958630\ttotal: 2m 5s\tremaining: 4m 58s\n",
            "231:\tlearn: 67.6973274\ttotal: 2m 5s\tremaining: 4m 58s\n",
            "232:\tlearn: 67.5026703\ttotal: 2m 6s\tremaining: 4m 57s\n",
            "233:\tlearn: 67.1734353\ttotal: 2m 6s\tremaining: 4m 57s\n",
            "234:\tlearn: 66.8735067\ttotal: 2m 7s\tremaining: 4m 57s\n",
            "235:\tlearn: 66.7390613\ttotal: 2m 8s\tremaining: 4m 56s\n",
            "236:\tlearn: 66.5672454\ttotal: 2m 8s\tremaining: 4m 56s\n",
            "237:\tlearn: 66.4180904\ttotal: 2m 9s\tremaining: 4m 55s\n",
            "238:\tlearn: 66.2449955\ttotal: 2m 9s\tremaining: 4m 55s\n",
            "239:\tlearn: 65.9296421\ttotal: 2m 10s\tremaining: 4m 55s\n",
            "240:\tlearn: 65.4417331\ttotal: 2m 11s\tremaining: 4m 54s\n",
            "241:\tlearn: 65.1857699\ttotal: 2m 11s\tremaining: 4m 54s\n",
            "242:\tlearn: 65.0784167\ttotal: 2m 12s\tremaining: 4m 53s\n",
            "243:\tlearn: 64.8013356\ttotal: 2m 12s\tremaining: 4m 53s\n",
            "244:\tlearn: 64.6948768\ttotal: 2m 13s\tremaining: 4m 52s\n",
            "245:\tlearn: 64.5520728\ttotal: 2m 14s\tremaining: 4m 52s\n",
            "246:\tlearn: 64.4337424\ttotal: 2m 14s\tremaining: 4m 52s\n",
            "247:\tlearn: 64.1116615\ttotal: 2m 15s\tremaining: 4m 51s\n",
            "248:\tlearn: 63.9751352\ttotal: 2m 15s\tremaining: 4m 51s\n",
            "249:\tlearn: 63.8415193\ttotal: 2m 16s\tremaining: 4m 50s\n",
            "250:\tlearn: 63.7471994\ttotal: 2m 17s\tremaining: 4m 50s\n",
            "251:\tlearn: 63.5493159\ttotal: 2m 17s\tremaining: 4m 50s\n",
            "252:\tlearn: 63.4133729\ttotal: 2m 18s\tremaining: 4m 49s\n",
            "253:\tlearn: 63.1958374\ttotal: 2m 18s\tremaining: 4m 49s\n",
            "254:\tlearn: 62.9385122\ttotal: 2m 19s\tremaining: 4m 48s\n",
            "255:\tlearn: 62.6812261\ttotal: 2m 20s\tremaining: 4m 48s\n",
            "256:\tlearn: 62.5480673\ttotal: 2m 20s\tremaining: 4m 48s\n",
            "257:\tlearn: 62.4433851\ttotal: 2m 21s\tremaining: 4m 47s\n",
            "258:\tlearn: 62.3552898\ttotal: 2m 21s\tremaining: 4m 47s\n",
            "259:\tlearn: 62.3184480\ttotal: 2m 22s\tremaining: 4m 47s\n",
            "260:\tlearn: 62.1212563\ttotal: 2m 23s\tremaining: 4m 46s\n",
            "261:\tlearn: 62.0131807\ttotal: 2m 23s\tremaining: 4m 46s\n",
            "262:\tlearn: 61.8327061\ttotal: 2m 24s\tremaining: 4m 45s\n",
            "263:\tlearn: 61.8326044\ttotal: 2m 25s\tremaining: 4m 45s\n",
            "264:\tlearn: 61.7009173\ttotal: 2m 25s\tremaining: 4m 44s\n",
            "265:\tlearn: 61.5030348\ttotal: 2m 26s\tremaining: 4m 44s\n",
            "266:\tlearn: 61.3975080\ttotal: 2m 26s\tremaining: 4m 43s\n",
            "267:\tlearn: 61.2833033\ttotal: 2m 27s\tremaining: 4m 43s\n",
            "268:\tlearn: 61.0442427\ttotal: 2m 28s\tremaining: 4m 43s\n",
            "269:\tlearn: 60.9920533\ttotal: 2m 28s\tremaining: 4m 42s\n",
            "270:\tlearn: 60.8325594\ttotal: 2m 29s\tremaining: 4m 42s\n",
            "271:\tlearn: 60.6411420\ttotal: 2m 29s\tremaining: 4m 41s\n",
            "272:\tlearn: 60.3792489\ttotal: 2m 30s\tremaining: 4m 41s\n",
            "273:\tlearn: 60.0730274\ttotal: 2m 31s\tremaining: 4m 40s\n",
            "274:\tlearn: 59.8936159\ttotal: 2m 31s\tremaining: 4m 40s\n",
            "275:\tlearn: 59.7501090\ttotal: 2m 32s\tremaining: 4m 40s\n",
            "276:\tlearn: 59.5935088\ttotal: 2m 33s\tremaining: 4m 39s\n",
            "277:\tlearn: 59.4706105\ttotal: 2m 33s\tremaining: 4m 39s\n",
            "278:\tlearn: 59.3533539\ttotal: 2m 34s\tremaining: 4m 38s\n",
            "279:\tlearn: 59.1665585\ttotal: 2m 34s\tremaining: 4m 37s\n",
            "280:\tlearn: 58.9983335\ttotal: 2m 35s\tremaining: 4m 37s\n",
            "281:\tlearn: 58.9175665\ttotal: 2m 36s\tremaining: 4m 37s\n",
            "282:\tlearn: 58.8109693\ttotal: 2m 36s\tremaining: 4m 36s\n",
            "283:\tlearn: 58.6416658\ttotal: 2m 37s\tremaining: 4m 36s\n",
            "284:\tlearn: 58.4258467\ttotal: 2m 37s\tremaining: 4m 35s\n",
            "285:\tlearn: 58.3008729\ttotal: 2m 38s\tremaining: 4m 35s\n",
            "286:\tlearn: 58.1850105\ttotal: 2m 39s\tremaining: 4m 34s\n",
            "287:\tlearn: 57.9738421\ttotal: 2m 39s\tremaining: 4m 34s\n",
            "288:\tlearn: 57.8822201\ttotal: 2m 40s\tremaining: 4m 34s\n",
            "289:\tlearn: 57.7672981\ttotal: 2m 41s\tremaining: 4m 33s\n",
            "290:\tlearn: 57.6376092\ttotal: 2m 41s\tremaining: 4m 33s\n",
            "291:\tlearn: 57.4511121\ttotal: 2m 42s\tremaining: 4m 32s\n",
            "292:\tlearn: 57.2835052\ttotal: 2m 42s\tremaining: 4m 32s\n",
            "293:\tlearn: 57.2334263\ttotal: 2m 43s\tremaining: 4m 32s\n",
            "294:\tlearn: 57.0448236\ttotal: 2m 44s\tremaining: 4m 31s\n",
            "295:\tlearn: 56.9284192\ttotal: 2m 44s\tremaining: 4m 31s\n",
            "296:\tlearn: 56.8148455\ttotal: 2m 45s\tremaining: 4m 31s\n",
            "297:\tlearn: 56.7227764\ttotal: 2m 46s\tremaining: 4m 30s\n",
            "298:\tlearn: 56.6075377\ttotal: 2m 47s\tremaining: 4m 30s\n",
            "299:\tlearn: 56.4951889\ttotal: 2m 47s\tremaining: 4m 30s\n",
            "300:\tlearn: 56.3217334\ttotal: 2m 48s\tremaining: 4m 29s\n",
            "301:\tlearn: 56.1786437\ttotal: 2m 48s\tremaining: 4m 29s\n",
            "302:\tlearn: 56.0257693\ttotal: 2m 49s\tremaining: 4m 28s\n",
            "303:\tlearn: 55.8901595\ttotal: 2m 50s\tremaining: 4m 28s\n",
            "304:\tlearn: 55.6466852\ttotal: 2m 50s\tremaining: 4m 27s\n",
            "305:\tlearn: 55.4640753\ttotal: 2m 51s\tremaining: 4m 27s\n",
            "306:\tlearn: 55.3420473\ttotal: 2m 52s\tremaining: 4m 26s\n",
            "307:\tlearn: 55.1481608\ttotal: 2m 52s\tremaining: 4m 26s\n",
            "308:\tlearn: 54.8838766\ttotal: 2m 53s\tremaining: 4m 25s\n",
            "309:\tlearn: 54.7434110\ttotal: 2m 54s\tremaining: 4m 25s\n",
            "310:\tlearn: 54.6887406\ttotal: 2m 54s\tremaining: 4m 25s\n",
            "311:\tlearn: 54.5403488\ttotal: 2m 55s\tremaining: 4m 24s\n",
            "312:\tlearn: 54.4358357\ttotal: 2m 55s\tremaining: 4m 24s\n",
            "313:\tlearn: 54.3328303\ttotal: 2m 56s\tremaining: 4m 23s\n",
            "314:\tlearn: 54.2060019\ttotal: 2m 57s\tremaining: 4m 23s\n",
            "315:\tlearn: 54.0997470\ttotal: 2m 57s\tremaining: 4m 22s\n",
            "316:\tlearn: 54.0825751\ttotal: 2m 58s\tremaining: 4m 22s\n",
            "317:\tlearn: 53.9828187\ttotal: 2m 59s\tremaining: 4m 22s\n",
            "318:\tlearn: 53.8109674\ttotal: 2m 59s\tremaining: 4m 21s\n",
            "319:\tlearn: 53.7114162\ttotal: 3m\tremaining: 4m 21s\n",
            "320:\tlearn: 53.5806337\ttotal: 3m 1s\tremaining: 4m 20s\n",
            "321:\tlearn: 53.4658978\ttotal: 3m 1s\tremaining: 4m 20s\n",
            "322:\tlearn: 53.2791965\ttotal: 3m 2s\tremaining: 4m 19s\n",
            "323:\tlearn: 53.1304162\ttotal: 3m 2s\tremaining: 4m 19s\n",
            "324:\tlearn: 53.0362012\ttotal: 3m 3s\tremaining: 4m 18s\n",
            "325:\tlearn: 52.8812414\ttotal: 3m 4s\tremaining: 4m 18s\n",
            "326:\tlearn: 52.7585154\ttotal: 3m 4s\tremaining: 4m 17s\n",
            "327:\tlearn: 52.6381064\ttotal: 3m 5s\tremaining: 4m 17s\n",
            "328:\tlearn: 52.5150130\ttotal: 3m 6s\tremaining: 4m 16s\n",
            "329:\tlearn: 52.4588342\ttotal: 3m 6s\tremaining: 4m 16s\n",
            "330:\tlearn: 52.3107730\ttotal: 3m 7s\tremaining: 4m 15s\n",
            "331:\tlearn: 52.2243787\ttotal: 3m 7s\tremaining: 4m 15s\n",
            "332:\tlearn: 52.1681769\ttotal: 3m 8s\tremaining: 4m 15s\n",
            "333:\tlearn: 51.9784938\ttotal: 3m 9s\tremaining: 4m 14s\n",
            "334:\tlearn: 51.7418145\ttotal: 3m 9s\tremaining: 4m 13s\n",
            "335:\tlearn: 51.6132037\ttotal: 3m 10s\tremaining: 4m 13s\n",
            "336:\tlearn: 51.5193613\ttotal: 3m 11s\tremaining: 4m 12s\n",
            "337:\tlearn: 51.3325317\ttotal: 3m 11s\tremaining: 4m 12s\n",
            "338:\tlearn: 51.2193444\ttotal: 3m 12s\tremaining: 4m 11s\n",
            "339:\tlearn: 51.1075617\ttotal: 3m 12s\tremaining: 4m 11s\n",
            "340:\tlearn: 50.9660630\ttotal: 3m 13s\tremaining: 4m 10s\n",
            "341:\tlearn: 50.8184161\ttotal: 3m 14s\tremaining: 4m 10s\n",
            "342:\tlearn: 50.7136941\ttotal: 3m 14s\tremaining: 4m 9s\n",
            "343:\tlearn: 50.6577014\ttotal: 3m 15s\tremaining: 4m 9s\n",
            "344:\tlearn: 50.6167361\ttotal: 3m 16s\tremaining: 4m 8s\n",
            "345:\tlearn: 50.3996103\ttotal: 3m 16s\tremaining: 4m 8s\n",
            "346:\tlearn: 50.2533757\ttotal: 3m 17s\tremaining: 4m 7s\n",
            "347:\tlearn: 50.1108041\ttotal: 3m 18s\tremaining: 4m 7s\n",
            "348:\tlearn: 49.9908996\ttotal: 3m 18s\tremaining: 4m 7s\n",
            "349:\tlearn: 49.8751518\ttotal: 3m 19s\tremaining: 4m 6s\n",
            "350:\tlearn: 49.7967723\ttotal: 3m 20s\tremaining: 4m 6s\n",
            "351:\tlearn: 49.7478935\ttotal: 3m 20s\tremaining: 4m 5s\n",
            "352:\tlearn: 49.5562822\ttotal: 3m 21s\tremaining: 4m 5s\n",
            "353:\tlearn: 49.4503309\ttotal: 3m 22s\tremaining: 4m 4s\n",
            "354:\tlearn: 49.3435443\ttotal: 3m 22s\tremaining: 4m 4s\n",
            "355:\tlearn: 49.2295291\ttotal: 3m 23s\tremaining: 4m 3s\n",
            "356:\tlearn: 49.1161940\ttotal: 3m 23s\tremaining: 4m 3s\n",
            "357:\tlearn: 49.0571713\ttotal: 3m 24s\tremaining: 4m 2s\n",
            "358:\tlearn: 48.8931824\ttotal: 3m 25s\tremaining: 4m 2s\n",
            "359:\tlearn: 48.8040361\ttotal: 3m 25s\tremaining: 4m 1s\n",
            "360:\tlearn: 48.7509036\ttotal: 3m 26s\tremaining: 4m 1s\n",
            "361:\tlearn: 48.6092374\ttotal: 3m 26s\tremaining: 4m\n",
            "362:\tlearn: 48.4460293\ttotal: 3m 27s\tremaining: 4m\n",
            "363:\tlearn: 48.2962638\ttotal: 3m 28s\tremaining: 3m 59s\n",
            "364:\tlearn: 48.2545890\ttotal: 3m 28s\tremaining: 3m 59s\n",
            "365:\tlearn: 48.1996235\ttotal: 3m 29s\tremaining: 3m 58s\n",
            "366:\tlearn: 48.1341680\ttotal: 3m 30s\tremaining: 3m 58s\n",
            "367:\tlearn: 48.0348406\ttotal: 3m 30s\tremaining: 3m 57s\n",
            "368:\tlearn: 47.9828649\ttotal: 3m 31s\tremaining: 3m 57s\n",
            "369:\tlearn: 47.8883112\ttotal: 3m 32s\tremaining: 3m 56s\n",
            "370:\tlearn: 47.7837308\ttotal: 3m 32s\tremaining: 3m 56s\n",
            "371:\tlearn: 47.7547532\ttotal: 3m 33s\tremaining: 3m 55s\n",
            "372:\tlearn: 47.5806700\ttotal: 3m 34s\tremaining: 3m 55s\n",
            "373:\tlearn: 47.5558835\ttotal: 3m 34s\tremaining: 3m 54s\n",
            "374:\tlearn: 47.4062656\ttotal: 3m 35s\tremaining: 3m 54s\n",
            "375:\tlearn: 47.2780979\ttotal: 3m 36s\tremaining: 3m 53s\n",
            "376:\tlearn: 47.1824122\ttotal: 3m 36s\tremaining: 3m 53s\n",
            "377:\tlearn: 47.0440734\ttotal: 3m 37s\tremaining: 3m 52s\n",
            "378:\tlearn: 46.8550782\ttotal: 3m 38s\tremaining: 3m 52s\n",
            "379:\tlearn: 46.7728933\ttotal: 3m 38s\tremaining: 3m 51s\n",
            "380:\tlearn: 46.6164256\ttotal: 3m 39s\tremaining: 3m 51s\n",
            "381:\tlearn: 46.5564667\ttotal: 3m 39s\tremaining: 3m 50s\n",
            "382:\tlearn: 46.4640704\ttotal: 3m 40s\tremaining: 3m 50s\n",
            "383:\tlearn: 46.4044757\ttotal: 3m 41s\tremaining: 3m 49s\n",
            "384:\tlearn: 46.3648464\ttotal: 3m 41s\tremaining: 3m 49s\n",
            "385:\tlearn: 46.2763598\ttotal: 3m 42s\tremaining: 3m 48s\n",
            "386:\tlearn: 46.2088210\ttotal: 3m 43s\tremaining: 3m 48s\n",
            "387:\tlearn: 46.0588748\ttotal: 3m 43s\tremaining: 3m 47s\n",
            "388:\tlearn: 46.0083857\ttotal: 3m 44s\tremaining: 3m 47s\n",
            "389:\tlearn: 45.9343653\ttotal: 3m 45s\tremaining: 3m 46s\n",
            "390:\tlearn: 45.8487098\ttotal: 3m 45s\tremaining: 3m 46s\n",
            "391:\tlearn: 45.7250049\ttotal: 3m 46s\tremaining: 3m 45s\n",
            "392:\tlearn: 45.6572975\ttotal: 3m 46s\tremaining: 3m 45s\n",
            "393:\tlearn: 45.5282804\ttotal: 3m 47s\tremaining: 3m 44s\n",
            "394:\tlearn: 45.3669626\ttotal: 3m 48s\tremaining: 3m 44s\n",
            "395:\tlearn: 45.2690452\ttotal: 3m 48s\tremaining: 3m 43s\n",
            "396:\tlearn: 45.2105410\ttotal: 3m 49s\tremaining: 3m 43s\n",
            "397:\tlearn: 45.0429064\ttotal: 3m 50s\tremaining: 3m 42s\n",
            "398:\tlearn: 44.9213483\ttotal: 3m 50s\tremaining: 3m 42s\n",
            "399:\tlearn: 44.8035188\ttotal: 3m 51s\tremaining: 3m 41s\n",
            "400:\tlearn: 44.7307887\ttotal: 3m 52s\tremaining: 3m 41s\n",
            "401:\tlearn: 44.6810505\ttotal: 3m 52s\tremaining: 3m 40s\n",
            "402:\tlearn: 44.5430207\ttotal: 3m 53s\tremaining: 3m 40s\n",
            "403:\tlearn: 44.4599318\ttotal: 3m 54s\tremaining: 3m 39s\n",
            "404:\tlearn: 44.3213145\ttotal: 3m 54s\tremaining: 3m 39s\n",
            "405:\tlearn: 44.2535845\ttotal: 3m 55s\tremaining: 3m 38s\n",
            "406:\tlearn: 44.1545816\ttotal: 3m 56s\tremaining: 3m 38s\n",
            "407:\tlearn: 44.0807887\ttotal: 3m 56s\tremaining: 3m 37s\n",
            "408:\tlearn: 44.0806478\ttotal: 3m 57s\tremaining: 3m 37s\n",
            "409:\tlearn: 43.9417251\ttotal: 3m 58s\tremaining: 3m 36s\n",
            "410:\tlearn: 43.8672349\ttotal: 3m 58s\tremaining: 3m 36s\n",
            "411:\tlearn: 43.8032880\ttotal: 3m 59s\tremaining: 3m 35s\n",
            "412:\tlearn: 43.6621182\ttotal: 4m\tremaining: 3m 35s\n",
            "413:\tlearn: 43.5781662\ttotal: 4m\tremaining: 3m 34s\n",
            "414:\tlearn: 43.4735106\ttotal: 4m 1s\tremaining: 3m 33s\n",
            "415:\tlearn: 43.4279091\ttotal: 4m 1s\tremaining: 3m 33s\n",
            "416:\tlearn: 43.2831981\ttotal: 4m 2s\tremaining: 3m 32s\n",
            "417:\tlearn: 43.1698824\ttotal: 4m 3s\tremaining: 3m 32s\n",
            "418:\tlearn: 43.0427181\ttotal: 4m 3s\tremaining: 3m 31s\n",
            "419:\tlearn: 42.9692529\ttotal: 4m 4s\tremaining: 3m 31s\n",
            "420:\tlearn: 42.9076782\ttotal: 4m 5s\tremaining: 3m 30s\n",
            "421:\tlearn: 42.8338125\ttotal: 4m 5s\tremaining: 3m 30s\n",
            "422:\tlearn: 42.7113549\ttotal: 4m 6s\tremaining: 3m 29s\n",
            "423:\tlearn: 42.5986198\ttotal: 4m 6s\tremaining: 3m 29s\n",
            "424:\tlearn: 42.4949178\ttotal: 4m 7s\tremaining: 3m 28s\n",
            "425:\tlearn: 42.4240885\ttotal: 4m 8s\tremaining: 3m 28s\n",
            "426:\tlearn: 42.3600451\ttotal: 4m 8s\tremaining: 3m 27s\n",
            "427:\tlearn: 42.2683079\ttotal: 4m 9s\tremaining: 3m 26s\n",
            "428:\tlearn: 42.2203929\ttotal: 4m 10s\tremaining: 3m 26s\n",
            "429:\tlearn: 42.2164277\ttotal: 4m 10s\tremaining: 3m 26s\n",
            "430:\tlearn: 42.1493218\ttotal: 4m 11s\tremaining: 3m 25s\n",
            "431:\tlearn: 42.0615803\ttotal: 4m 12s\tremaining: 3m 25s\n",
            "432:\tlearn: 41.9716436\ttotal: 4m 12s\tremaining: 3m 24s\n",
            "433:\tlearn: 41.9012954\ttotal: 4m 13s\tremaining: 3m 23s\n",
            "434:\tlearn: 41.7518960\ttotal: 4m 14s\tremaining: 3m 23s\n",
            "435:\tlearn: 41.6578782\ttotal: 4m 14s\tremaining: 3m 22s\n",
            "436:\tlearn: 41.6022848\ttotal: 4m 15s\tremaining: 3m 22s\n",
            "437:\tlearn: 41.5130104\ttotal: 4m 16s\tremaining: 3m 21s\n",
            "438:\tlearn: 41.4398925\ttotal: 4m 16s\tremaining: 3m 21s\n",
            "439:\tlearn: 41.3700486\ttotal: 4m 17s\tremaining: 3m 20s\n",
            "440:\tlearn: 41.3047942\ttotal: 4m 18s\tremaining: 3m 20s\n",
            "441:\tlearn: 41.1716238\ttotal: 4m 18s\tremaining: 3m 19s\n",
            "442:\tlearn: 41.0419166\ttotal: 4m 19s\tremaining: 3m 19s\n",
            "443:\tlearn: 40.8775039\ttotal: 4m 19s\tremaining: 3m 18s\n",
            "444:\tlearn: 40.8219762\ttotal: 4m 20s\tremaining: 3m 18s\n",
            "445:\tlearn: 40.7192393\ttotal: 4m 21s\tremaining: 3m 17s\n",
            "446:\tlearn: 40.6470104\ttotal: 4m 22s\tremaining: 3m 17s\n",
            "447:\tlearn: 40.5789041\ttotal: 4m 22s\tremaining: 3m 16s\n",
            "448:\tlearn: 40.5262170\ttotal: 4m 23s\tremaining: 3m 15s\n",
            "449:\tlearn: 40.4490861\ttotal: 4m 24s\tremaining: 3m 15s\n",
            "450:\tlearn: 40.3303341\ttotal: 4m 24s\tremaining: 3m 14s\n",
            "451:\tlearn: 40.2283902\ttotal: 4m 25s\tremaining: 3m 14s\n",
            "452:\tlearn: 40.1928886\ttotal: 4m 26s\tremaining: 3m 13s\n",
            "453:\tlearn: 40.1314801\ttotal: 4m 26s\tremaining: 3m 13s\n",
            "454:\tlearn: 40.0598410\ttotal: 4m 27s\tremaining: 3m 12s\n",
            "455:\tlearn: 39.9895150\ttotal: 4m 27s\tremaining: 3m 12s\n",
            "456:\tlearn: 39.9231073\ttotal: 4m 28s\tremaining: 3m 11s\n",
            "457:\tlearn: 39.8794641\ttotal: 4m 29s\tremaining: 3m 11s\n",
            "458:\tlearn: 39.7499947\ttotal: 4m 29s\tremaining: 3m 10s\n",
            "459:\tlearn: 39.6524938\ttotal: 4m 30s\tremaining: 3m 9s\n",
            "460:\tlearn: 39.6267369\ttotal: 4m 31s\tremaining: 3m 9s\n",
            "461:\tlearn: 39.5550352\ttotal: 4m 31s\tremaining: 3m 8s\n",
            "462:\tlearn: 39.4948455\ttotal: 4m 32s\tremaining: 3m 8s\n",
            "463:\tlearn: 39.3900024\ttotal: 4m 33s\tremaining: 3m 7s\n",
            "464:\tlearn: 39.3131860\ttotal: 4m 33s\tremaining: 3m 7s\n",
            "465:\tlearn: 39.1974082\ttotal: 4m 34s\tremaining: 3m 6s\n",
            "466:\tlearn: 39.1845694\ttotal: 4m 35s\tremaining: 3m 6s\n",
            "467:\tlearn: 39.0647470\ttotal: 4m 35s\tremaining: 3m 5s\n",
            "468:\tlearn: 38.9687071\ttotal: 4m 36s\tremaining: 3m 5s\n",
            "469:\tlearn: 38.8618470\ttotal: 4m 37s\tremaining: 3m 4s\n",
            "470:\tlearn: 38.8236767\ttotal: 4m 37s\tremaining: 3m 4s\n",
            "471:\tlearn: 38.7595097\ttotal: 4m 38s\tremaining: 3m 3s\n",
            "472:\tlearn: 38.7112066\ttotal: 4m 39s\tremaining: 3m 2s\n",
            "473:\tlearn: 38.6366892\ttotal: 4m 39s\tremaining: 3m 2s\n",
            "474:\tlearn: 38.5550293\ttotal: 4m 40s\tremaining: 3m 1s\n",
            "475:\tlearn: 38.5236201\ttotal: 4m 41s\tremaining: 3m 1s\n",
            "476:\tlearn: 38.4369085\ttotal: 4m 41s\tremaining: 3m\n",
            "477:\tlearn: 38.3127263\ttotal: 4m 42s\tremaining: 3m\n",
            "478:\tlearn: 38.2602802\ttotal: 4m 43s\tremaining: 2m 59s\n",
            "479:\tlearn: 38.2109004\ttotal: 4m 43s\tremaining: 2m 59s\n",
            "480:\tlearn: 38.1836401\ttotal: 4m 44s\tremaining: 2m 58s\n",
            "481:\tlearn: 38.0733091\ttotal: 4m 45s\tremaining: 2m 58s\n",
            "482:\tlearn: 38.0084110\ttotal: 4m 46s\tremaining: 2m 57s\n",
            "483:\tlearn: 37.9694656\ttotal: 4m 46s\tremaining: 2m 57s\n",
            "484:\tlearn: 37.8434287\ttotal: 4m 47s\tremaining: 2m 56s\n",
            "485:\tlearn: 37.7545754\ttotal: 4m 47s\tremaining: 2m 55s\n",
            "486:\tlearn: 37.6702475\ttotal: 4m 48s\tremaining: 2m 55s\n",
            "487:\tlearn: 37.6446614\ttotal: 4m 49s\tremaining: 2m 54s\n",
            "488:\tlearn: 37.6031458\ttotal: 4m 50s\tremaining: 2m 54s\n",
            "489:\tlearn: 37.5568270\ttotal: 4m 50s\tremaining: 2m 53s\n",
            "490:\tlearn: 37.5026852\ttotal: 4m 51s\tremaining: 2m 53s\n",
            "491:\tlearn: 37.4944485\ttotal: 4m 52s\tremaining: 2m 52s\n",
            "492:\tlearn: 37.4100858\ttotal: 4m 53s\tremaining: 2m 52s\n",
            "493:\tlearn: 37.3456359\ttotal: 4m 53s\tremaining: 2m 51s\n",
            "494:\tlearn: 37.2823499\ttotal: 4m 54s\tremaining: 2m 51s\n",
            "495:\tlearn: 37.2544518\ttotal: 4m 54s\tremaining: 2m 50s\n",
            "496:\tlearn: 37.1596153\ttotal: 4m 55s\tremaining: 2m 50s\n",
            "497:\tlearn: 37.0483474\ttotal: 4m 56s\tremaining: 2m 49s\n",
            "498:\tlearn: 36.9454990\ttotal: 4m 57s\tremaining: 2m 49s\n",
            "499:\tlearn: 36.8792925\ttotal: 4m 57s\tremaining: 2m 48s\n",
            "500:\tlearn: 36.7496398\ttotal: 4m 58s\tremaining: 2m 47s\n",
            "501:\tlearn: 36.6640588\ttotal: 4m 59s\tremaining: 2m 47s\n",
            "502:\tlearn: 36.5939750\ttotal: 4m 59s\tremaining: 2m 46s\n",
            "503:\tlearn: 36.4915012\ttotal: 5m\tremaining: 2m 46s\n",
            "504:\tlearn: 36.4291494\ttotal: 5m 1s\tremaining: 2m 45s\n",
            "505:\tlearn: 36.3067385\ttotal: 5m 1s\tremaining: 2m 45s\n",
            "506:\tlearn: 36.2374870\ttotal: 5m 2s\tremaining: 2m 44s\n",
            "507:\tlearn: 36.1577782\ttotal: 5m 3s\tremaining: 2m 44s\n",
            "508:\tlearn: 36.1237946\ttotal: 5m 3s\tremaining: 2m 43s\n",
            "509:\tlearn: 36.1215783\ttotal: 5m 4s\tremaining: 2m 42s\n",
            "510:\tlearn: 36.0317195\ttotal: 5m 5s\tremaining: 2m 42s\n",
            "511:\tlearn: 35.9637740\ttotal: 5m 5s\tremaining: 2m 41s\n",
            "512:\tlearn: 35.8729784\ttotal: 5m 6s\tremaining: 2m 41s\n",
            "513:\tlearn: 35.8497473\ttotal: 5m 7s\tremaining: 2m 40s\n",
            "514:\tlearn: 35.7832044\ttotal: 5m 7s\tremaining: 2m 40s\n",
            "515:\tlearn: 35.7252686\ttotal: 5m 8s\tremaining: 2m 39s\n",
            "516:\tlearn: 35.6529445\ttotal: 5m 9s\tremaining: 2m 39s\n",
            "517:\tlearn: 35.5813248\ttotal: 5m 10s\tremaining: 2m 38s\n",
            "518:\tlearn: 35.5370199\ttotal: 5m 10s\tremaining: 2m 38s\n",
            "519:\tlearn: 35.4439827\ttotal: 5m 11s\tremaining: 2m 37s\n",
            "520:\tlearn: 35.3732398\ttotal: 5m 11s\tremaining: 2m 36s\n",
            "521:\tlearn: 35.3162877\ttotal: 5m 12s\tremaining: 2m 36s\n",
            "522:\tlearn: 35.2236736\ttotal: 5m 13s\tremaining: 2m 35s\n",
            "523:\tlearn: 35.1435291\ttotal: 5m 14s\tremaining: 2m 35s\n",
            "524:\tlearn: 35.1069832\ttotal: 5m 14s\tremaining: 2m 34s\n",
            "525:\tlearn: 35.0317151\ttotal: 5m 15s\tremaining: 2m 34s\n",
            "526:\tlearn: 34.9540830\ttotal: 5m 16s\tremaining: 2m 33s\n",
            "527:\tlearn: 34.8760399\ttotal: 5m 16s\tremaining: 2m 33s\n",
            "528:\tlearn: 34.7981303\ttotal: 5m 17s\tremaining: 2m 32s\n",
            "529:\tlearn: 34.7052307\ttotal: 5m 18s\tremaining: 2m 32s\n",
            "530:\tlearn: 34.6235952\ttotal: 5m 19s\tremaining: 2m 31s\n",
            "531:\tlearn: 34.5534420\ttotal: 5m 19s\tremaining: 2m 30s\n",
            "532:\tlearn: 34.5311090\ttotal: 5m 20s\tremaining: 2m 30s\n",
            "533:\tlearn: 34.4993615\ttotal: 5m 21s\tremaining: 2m 29s\n",
            "534:\tlearn: 34.4703900\ttotal: 5m 22s\tremaining: 2m 29s\n",
            "535:\tlearn: 34.3733517\ttotal: 5m 22s\tremaining: 2m 28s\n",
            "536:\tlearn: 34.2675512\ttotal: 5m 23s\tremaining: 2m 28s\n",
            "537:\tlearn: 34.2343214\ttotal: 5m 24s\tremaining: 2m 27s\n",
            "538:\tlearn: 34.1516129\ttotal: 5m 24s\tremaining: 2m 27s\n",
            "539:\tlearn: 34.1326695\ttotal: 5m 25s\tremaining: 2m 26s\n",
            "540:\tlearn: 34.1273696\ttotal: 5m 26s\tremaining: 2m 25s\n",
            "541:\tlearn: 34.1265038\ttotal: 5m 26s\tremaining: 2m 25s\n",
            "542:\tlearn: 34.0510366\ttotal: 5m 27s\tremaining: 2m 24s\n",
            "543:\tlearn: 33.9912175\ttotal: 5m 28s\tremaining: 2m 24s\n",
            "544:\tlearn: 33.9300318\ttotal: 5m 28s\tremaining: 2m 23s\n",
            "545:\tlearn: 33.8959197\ttotal: 5m 29s\tremaining: 2m 23s\n",
            "546:\tlearn: 33.7578031\ttotal: 5m 30s\tremaining: 2m 22s\n",
            "547:\tlearn: 33.7106387\ttotal: 5m 31s\tremaining: 2m 21s\n",
            "548:\tlearn: 33.6255912\ttotal: 5m 31s\tremaining: 2m 21s\n",
            "549:\tlearn: 33.5070920\ttotal: 5m 32s\tremaining: 2m 20s\n",
            "550:\tlearn: 33.4664563\ttotal: 5m 33s\tremaining: 2m 20s\n",
            "551:\tlearn: 33.4057851\ttotal: 5m 34s\tremaining: 2m 19s\n",
            "552:\tlearn: 33.3723317\ttotal: 5m 34s\tremaining: 2m 19s\n",
            "553:\tlearn: 33.3331699\ttotal: 5m 35s\tremaining: 2m 18s\n",
            "554:\tlearn: 33.2894393\ttotal: 5m 36s\tremaining: 2m 18s\n",
            "555:\tlearn: 33.1923993\ttotal: 5m 36s\tremaining: 2m 17s\n",
            "556:\tlearn: 33.1679980\ttotal: 5m 37s\tremaining: 2m 16s\n",
            "557:\tlearn: 33.1409175\ttotal: 5m 38s\tremaining: 2m 16s\n",
            "558:\tlearn: 33.0551936\ttotal: 5m 38s\tremaining: 2m 15s\n",
            "559:\tlearn: 33.0081328\ttotal: 5m 39s\tremaining: 2m 15s\n",
            "560:\tlearn: 33.0067679\ttotal: 5m 40s\tremaining: 2m 14s\n",
            "561:\tlearn: 32.9363316\ttotal: 5m 40s\tremaining: 2m 14s\n",
            "562:\tlearn: 32.8860785\ttotal: 5m 41s\tremaining: 2m 13s\n",
            "563:\tlearn: 32.8211626\ttotal: 5m 42s\tremaining: 2m 12s\n",
            "564:\tlearn: 32.7831504\ttotal: 5m 42s\tremaining: 2m 12s\n",
            "565:\tlearn: 32.6981085\ttotal: 5m 43s\tremaining: 2m 11s\n",
            "566:\tlearn: 32.6314434\ttotal: 5m 44s\tremaining: 2m 11s\n",
            "567:\tlearn: 32.5954473\ttotal: 5m 44s\tremaining: 2m 10s\n",
            "568:\tlearn: 32.5701924\ttotal: 5m 45s\tremaining: 2m 10s\n",
            "569:\tlearn: 32.5125557\ttotal: 5m 46s\tremaining: 2m 9s\n",
            "570:\tlearn: 32.4390040\ttotal: 5m 47s\tremaining: 2m 8s\n",
            "571:\tlearn: 32.3436592\ttotal: 5m 47s\tremaining: 2m 8s\n",
            "572:\tlearn: 32.2414275\ttotal: 5m 48s\tremaining: 2m 7s\n",
            "573:\tlearn: 32.2087389\ttotal: 5m 49s\tremaining: 2m 7s\n",
            "574:\tlearn: 32.1806600\ttotal: 5m 50s\tremaining: 2m 6s\n",
            "575:\tlearn: 32.1434660\ttotal: 5m 50s\tremaining: 2m 6s\n",
            "576:\tlearn: 32.0881138\ttotal: 5m 51s\tremaining: 2m 5s\n",
            "577:\tlearn: 32.0872759\ttotal: 5m 52s\tremaining: 2m 4s\n",
            "578:\tlearn: 32.0564369\ttotal: 5m 52s\tremaining: 2m 4s\n",
            "579:\tlearn: 32.0557588\ttotal: 5m 53s\tremaining: 2m 3s\n",
            "580:\tlearn: 32.0166934\ttotal: 5m 54s\tremaining: 2m 3s\n",
            "581:\tlearn: 31.9395649\ttotal: 5m 54s\tremaining: 2m 2s\n",
            "582:\tlearn: 31.8566844\ttotal: 5m 55s\tremaining: 2m 1s\n",
            "583:\tlearn: 31.8242276\ttotal: 5m 56s\tremaining: 2m 1s\n",
            "584:\tlearn: 31.7619381\ttotal: 5m 56s\tremaining: 2m\n",
            "585:\tlearn: 31.6755552\ttotal: 5m 57s\tremaining: 2m\n",
            "586:\tlearn: 31.6488802\ttotal: 5m 58s\tremaining: 1m 59s\n",
            "587:\tlearn: 31.5936902\ttotal: 5m 58s\tremaining: 1m 59s\n",
            "588:\tlearn: 31.5066106\ttotal: 5m 59s\tremaining: 1m 58s\n",
            "589:\tlearn: 31.4405114\ttotal: 6m\tremaining: 1m 57s\n",
            "590:\tlearn: 31.3993873\ttotal: 6m 1s\tremaining: 1m 57s\n",
            "591:\tlearn: 31.3227306\ttotal: 6m 1s\tremaining: 1m 56s\n",
            "592:\tlearn: 31.2526867\ttotal: 6m 2s\tremaining: 1m 56s\n",
            "593:\tlearn: 31.1618921\ttotal: 6m 3s\tremaining: 1m 55s\n",
            "594:\tlearn: 31.1188380\ttotal: 6m 4s\tremaining: 1m 55s\n",
            "595:\tlearn: 31.0505617\ttotal: 6m 4s\tremaining: 1m 54s\n",
            "596:\tlearn: 30.9824466\ttotal: 6m 5s\tremaining: 1m 53s\n",
            "597:\tlearn: 30.9449986\ttotal: 6m 6s\tremaining: 1m 53s\n",
            "598:\tlearn: 30.9034801\ttotal: 6m 6s\tremaining: 1m 52s\n",
            "599:\tlearn: 30.8813242\ttotal: 6m 7s\tremaining: 1m 52s\n",
            "600:\tlearn: 30.8431951\ttotal: 6m 8s\tremaining: 1m 51s\n",
            "601:\tlearn: 30.8059793\ttotal: 6m 8s\tremaining: 1m 50s\n",
            "602:\tlearn: 30.7064641\ttotal: 6m 9s\tremaining: 1m 50s\n",
            "603:\tlearn: 30.6266438\ttotal: 6m 10s\tremaining: 1m 49s\n",
            "604:\tlearn: 30.5667196\ttotal: 6m 10s\tremaining: 1m 49s\n",
            "605:\tlearn: 30.5297197\ttotal: 6m 11s\tremaining: 1m 48s\n",
            "606:\tlearn: 30.4183786\ttotal: 6m 12s\tremaining: 1m 47s\n",
            "607:\tlearn: 30.3981335\ttotal: 6m 12s\tremaining: 1m 47s\n",
            "608:\tlearn: 30.3428560\ttotal: 6m 13s\tremaining: 1m 46s\n",
            "609:\tlearn: 30.3302835\ttotal: 6m 14s\tremaining: 1m 46s\n",
            "610:\tlearn: 30.2912855\ttotal: 6m 14s\tremaining: 1m 45s\n",
            "611:\tlearn: 30.2306515\ttotal: 6m 15s\tremaining: 1m 44s\n",
            "612:\tlearn: 30.1659054\ttotal: 6m 16s\tremaining: 1m 44s\n",
            "613:\tlearn: 30.0994801\ttotal: 6m 17s\tremaining: 1m 43s\n",
            "614:\tlearn: 30.0783188\ttotal: 6m 17s\tremaining: 1m 43s\n",
            "615:\tlearn: 30.0749163\ttotal: 6m 18s\tremaining: 1m 42s\n",
            "616:\tlearn: 29.9942433\ttotal: 6m 19s\tremaining: 1m 42s\n",
            "617:\tlearn: 29.9483661\ttotal: 6m 20s\tremaining: 1m 41s\n",
            "618:\tlearn: 29.8950600\ttotal: 6m 20s\tremaining: 1m 40s\n",
            "619:\tlearn: 29.8707193\ttotal: 6m 21s\tremaining: 1m 40s\n",
            "620:\tlearn: 29.8183650\ttotal: 6m 22s\tremaining: 1m 39s\n",
            "621:\tlearn: 29.7422780\ttotal: 6m 22s\tremaining: 1m 39s\n",
            "622:\tlearn: 29.7130917\ttotal: 6m 23s\tremaining: 1m 38s\n",
            "623:\tlearn: 29.7129389\ttotal: 6m 24s\tremaining: 1m 37s\n",
            "624:\tlearn: 29.6631473\ttotal: 6m 24s\tremaining: 1m 37s\n",
            "625:\tlearn: 29.6336268\ttotal: 6m 25s\tremaining: 1m 36s\n",
            "626:\tlearn: 29.5883886\ttotal: 6m 26s\tremaining: 1m 36s\n",
            "627:\tlearn: 29.5146182\ttotal: 6m 27s\tremaining: 1m 35s\n",
            "628:\tlearn: 29.4710862\ttotal: 6m 28s\tremaining: 1m 35s\n",
            "629:\tlearn: 29.4186777\ttotal: 6m 28s\tremaining: 1m 34s\n",
            "630:\tlearn: 29.3774013\ttotal: 6m 29s\tremaining: 1m 33s\n",
            "631:\tlearn: 29.3121887\ttotal: 6m 30s\tremaining: 1m 33s\n",
            "632:\tlearn: 29.2568033\ttotal: 6m 31s\tremaining: 1m 32s\n",
            "633:\tlearn: 29.1673577\ttotal: 6m 31s\tremaining: 1m 32s\n",
            "634:\tlearn: 29.1347900\ttotal: 6m 32s\tremaining: 1m 31s\n",
            "635:\tlearn: 29.0934389\ttotal: 6m 33s\tremaining: 1m 30s\n",
            "636:\tlearn: 29.0266866\ttotal: 6m 34s\tremaining: 1m 30s\n",
            "637:\tlearn: 28.9840722\ttotal: 6m 34s\tremaining: 1m 29s\n",
            "638:\tlearn: 28.9551520\ttotal: 6m 35s\tremaining: 1m 29s\n",
            "639:\tlearn: 28.9363878\ttotal: 6m 36s\tremaining: 1m 28s\n",
            "640:\tlearn: 28.8731570\ttotal: 6m 36s\tremaining: 1m 27s\n",
            "641:\tlearn: 28.8238813\ttotal: 6m 37s\tremaining: 1m 27s\n",
            "642:\tlearn: 28.7910394\ttotal: 6m 38s\tremaining: 1m 26s\n",
            "643:\tlearn: 28.7758996\ttotal: 6m 38s\tremaining: 1m 26s\n",
            "644:\tlearn: 28.7725471\ttotal: 6m 39s\tremaining: 1m 25s\n",
            "645:\tlearn: 28.7091465\ttotal: 6m 40s\tremaining: 1m 24s\n",
            "646:\tlearn: 28.6770884\ttotal: 6m 41s\tremaining: 1m 24s\n",
            "647:\tlearn: 28.6032716\ttotal: 6m 41s\tremaining: 1m 23s\n",
            "648:\tlearn: 28.5536090\ttotal: 6m 42s\tremaining: 1m 23s\n",
            "649:\tlearn: 28.5160310\ttotal: 6m 43s\tremaining: 1m 22s\n",
            "650:\tlearn: 28.4596313\ttotal: 6m 43s\tremaining: 1m 21s\n",
            "651:\tlearn: 28.4076130\ttotal: 6m 44s\tremaining: 1m 21s\n",
            "652:\tlearn: 28.3237447\ttotal: 6m 45s\tremaining: 1m 20s\n",
            "653:\tlearn: 28.3091690\ttotal: 6m 46s\tremaining: 1m 20s\n",
            "654:\tlearn: 28.2539208\ttotal: 6m 46s\tremaining: 1m 19s\n",
            "655:\tlearn: 28.2270267\ttotal: 6m 47s\tremaining: 1m 18s\n",
            "656:\tlearn: 28.1733331\ttotal: 6m 48s\tremaining: 1m 18s\n",
            "657:\tlearn: 28.1112263\ttotal: 6m 49s\tremaining: 1m 17s\n",
            "658:\tlearn: 28.0901404\ttotal: 6m 49s\tremaining: 1m 17s\n",
            "659:\tlearn: 28.0306324\ttotal: 6m 50s\tremaining: 1m 16s\n",
            "660:\tlearn: 27.9853132\ttotal: 6m 51s\tremaining: 1m 15s\n",
            "661:\tlearn: 27.9563585\ttotal: 6m 52s\tremaining: 1m 15s\n",
            "662:\tlearn: 27.9095797\ttotal: 6m 52s\tremaining: 1m 14s\n",
            "663:\tlearn: 27.8986416\ttotal: 6m 53s\tremaining: 1m 14s\n",
            "664:\tlearn: 27.8123518\ttotal: 6m 54s\tremaining: 1m 13s\n",
            "665:\tlearn: 27.7958978\ttotal: 6m 55s\tremaining: 1m 12s\n",
            "666:\tlearn: 27.7690997\ttotal: 6m 55s\tremaining: 1m 12s\n",
            "667:\tlearn: 27.7071850\ttotal: 6m 56s\tremaining: 1m 11s\n",
            "668:\tlearn: 27.6673389\ttotal: 6m 57s\tremaining: 1m 11s\n",
            "669:\tlearn: 27.6141862\ttotal: 6m 57s\tremaining: 1m 10s\n",
            "670:\tlearn: 27.5635057\ttotal: 6m 58s\tremaining: 1m 9s\n",
            "671:\tlearn: 27.5194413\ttotal: 6m 59s\tremaining: 1m 9s\n",
            "672:\tlearn: 27.4810077\ttotal: 7m\tremaining: 1m 8s\n",
            "673:\tlearn: 27.4382050\ttotal: 7m\tremaining: 1m 8s\n",
            "674:\tlearn: 27.4133766\ttotal: 7m 1s\tremaining: 1m 7s\n",
            "675:\tlearn: 27.3575540\ttotal: 7m 2s\tremaining: 1m 6s\n",
            "676:\tlearn: 27.3065450\ttotal: 7m 2s\tremaining: 1m 6s\n",
            "677:\tlearn: 27.2648770\ttotal: 7m 3s\tremaining: 1m 5s\n",
            "678:\tlearn: 27.2245544\ttotal: 7m 4s\tremaining: 1m 5s\n",
            "679:\tlearn: 27.1775429\ttotal: 7m 5s\tremaining: 1m 4s\n",
            "680:\tlearn: 27.1223770\ttotal: 7m 5s\tremaining: 1m 3s\n",
            "681:\tlearn: 27.0968058\ttotal: 7m 6s\tremaining: 1m 3s\n",
            "682:\tlearn: 27.0596076\ttotal: 7m 7s\tremaining: 1m 2s\n",
            "683:\tlearn: 26.9882321\ttotal: 7m 8s\tremaining: 1m 1s\n",
            "684:\tlearn: 26.9343560\ttotal: 7m 8s\tremaining: 1m 1s\n",
            "685:\tlearn: 26.9064374\ttotal: 7m 9s\tremaining: 1m\n",
            "686:\tlearn: 26.8430050\ttotal: 7m 10s\tremaining: 1m\n",
            "687:\tlearn: 26.7723257\ttotal: 7m 11s\tremaining: 59.6s\n",
            "688:\tlearn: 26.7257104\ttotal: 7m 12s\tremaining: 59s\n",
            "689:\tlearn: 26.6948634\ttotal: 7m 13s\tremaining: 58.4s\n",
            "690:\tlearn: 26.6515101\ttotal: 7m 13s\tremaining: 57.7s\n",
            "691:\tlearn: 26.5950916\ttotal: 7m 14s\tremaining: 57.1s\n",
            "692:\tlearn: 26.5655182\ttotal: 7m 15s\tremaining: 56.5s\n",
            "693:\tlearn: 26.5368457\ttotal: 7m 15s\tremaining: 55.9s\n",
            "694:\tlearn: 26.4915474\ttotal: 7m 16s\tremaining: 55.3s\n",
            "695:\tlearn: 26.4787552\ttotal: 7m 17s\tremaining: 54.7s\n",
            "696:\tlearn: 26.4612887\ttotal: 7m 18s\tremaining: 54s\n",
            "697:\tlearn: 26.4119684\ttotal: 7m 18s\tremaining: 53.4s\n",
            "698:\tlearn: 26.3825775\ttotal: 7m 19s\tremaining: 52.8s\n",
            "699:\tlearn: 26.3383493\ttotal: 7m 20s\tremaining: 52.2s\n",
            "700:\tlearn: 26.3071177\ttotal: 7m 21s\tremaining: 51.6s\n",
            "701:\tlearn: 26.2508920\ttotal: 7m 21s\tremaining: 51s\n",
            "702:\tlearn: 26.1811858\ttotal: 7m 22s\tremaining: 50.4s\n",
            "703:\tlearn: 26.1398049\ttotal: 7m 23s\tremaining: 49.8s\n",
            "704:\tlearn: 26.1153039\ttotal: 7m 24s\tremaining: 49.1s\n",
            "705:\tlearn: 26.0784376\ttotal: 7m 24s\tremaining: 48.5s\n",
            "706:\tlearn: 26.0242395\ttotal: 7m 25s\tremaining: 47.9s\n",
            "707:\tlearn: 25.9799821\ttotal: 7m 26s\tremaining: 47.3s\n",
            "708:\tlearn: 25.9269684\ttotal: 7m 27s\tremaining: 46.7s\n",
            "709:\tlearn: 25.8699171\ttotal: 7m 28s\tremaining: 46.1s\n",
            "710:\tlearn: 25.8091634\ttotal: 7m 28s\tremaining: 45.5s\n",
            "711:\tlearn: 25.7592866\ttotal: 7m 29s\tremaining: 44.9s\n",
            "712:\tlearn: 25.7419036\ttotal: 7m 30s\tremaining: 44.2s\n",
            "713:\tlearn: 25.7078390\ttotal: 7m 31s\tremaining: 43.6s\n",
            "714:\tlearn: 25.6993550\ttotal: 7m 31s\tremaining: 43s\n",
            "715:\tlearn: 25.6660889\ttotal: 7m 32s\tremaining: 42.4s\n",
            "716:\tlearn: 25.6512568\ttotal: 7m 33s\tremaining: 41.7s\n",
            "717:\tlearn: 25.6068182\ttotal: 7m 34s\tremaining: 41.1s\n",
            "718:\tlearn: 25.5694045\ttotal: 7m 34s\tremaining: 40.5s\n",
            "719:\tlearn: 25.5302316\ttotal: 7m 35s\tremaining: 39.9s\n",
            "720:\tlearn: 25.4864266\ttotal: 7m 36s\tremaining: 39.2s\n",
            "721:\tlearn: 25.4151809\ttotal: 7m 37s\tremaining: 38.6s\n",
            "722:\tlearn: 25.3638286\ttotal: 7m 38s\tremaining: 38s\n",
            "723:\tlearn: 25.3452554\ttotal: 7m 38s\tremaining: 37.4s\n",
            "724:\tlearn: 25.2988918\ttotal: 7m 39s\tremaining: 36.8s\n",
            "725:\tlearn: 25.2566146\ttotal: 7m 40s\tremaining: 36.1s\n",
            "726:\tlearn: 25.1933471\ttotal: 7m 40s\tremaining: 35.5s\n",
            "727:\tlearn: 25.1586979\ttotal: 7m 41s\tremaining: 34.9s\n",
            "728:\tlearn: 25.1456920\ttotal: 7m 42s\tremaining: 34.3s\n",
            "729:\tlearn: 25.1334536\ttotal: 7m 43s\tremaining: 33.6s\n",
            "730:\tlearn: 25.0977638\ttotal: 7m 43s\tremaining: 33s\n",
            "731:\tlearn: 25.0696547\ttotal: 7m 44s\tremaining: 32.4s\n",
            "732:\tlearn: 25.0692003\ttotal: 7m 45s\tremaining: 31.8s\n",
            "733:\tlearn: 25.0320777\ttotal: 7m 46s\tremaining: 31.1s\n",
            "734:\tlearn: 25.0003507\ttotal: 7m 46s\tremaining: 30.5s\n",
            "735:\tlearn: 24.9381652\ttotal: 7m 47s\tremaining: 29.9s\n",
            "736:\tlearn: 24.8578044\ttotal: 7m 48s\tremaining: 29.2s\n",
            "737:\tlearn: 24.8146324\ttotal: 7m 49s\tremaining: 28.6s\n",
            "738:\tlearn: 24.7791451\ttotal: 7m 49s\tremaining: 28s\n",
            "739:\tlearn: 24.7607990\ttotal: 7m 50s\tremaining: 27.3s\n",
            "740:\tlearn: 24.7044878\ttotal: 7m 51s\tremaining: 26.7s\n",
            "741:\tlearn: 24.6798919\ttotal: 7m 52s\tremaining: 26.1s\n",
            "742:\tlearn: 24.6312263\ttotal: 7m 52s\tremaining: 25.5s\n",
            "743:\tlearn: 24.5940151\ttotal: 7m 53s\tremaining: 24.8s\n",
            "744:\tlearn: 24.5641002\ttotal: 7m 54s\tremaining: 24.2s\n",
            "745:\tlearn: 24.5390858\ttotal: 7m 55s\tremaining: 23.6s\n",
            "746:\tlearn: 24.4938179\ttotal: 7m 55s\tremaining: 22.9s\n",
            "747:\tlearn: 24.4372857\ttotal: 7m 56s\tremaining: 22.3s\n",
            "748:\tlearn: 24.4181623\ttotal: 7m 57s\tremaining: 21.7s\n",
            "749:\tlearn: 24.3574138\ttotal: 7m 58s\tremaining: 21s\n",
            "750:\tlearn: 24.3179462\ttotal: 7m 58s\tremaining: 20.4s\n",
            "751:\tlearn: 24.2822957\ttotal: 7m 59s\tremaining: 19.8s\n",
            "752:\tlearn: 24.2536273\ttotal: 8m\tremaining: 19.1s\n",
            "753:\tlearn: 24.2342335\ttotal: 8m 1s\tremaining: 18.5s\n",
            "754:\tlearn: 24.1757022\ttotal: 8m 2s\tremaining: 17.9s\n",
            "755:\tlearn: 24.1479433\ttotal: 8m 2s\tremaining: 17.2s\n",
            "756:\tlearn: 24.1043855\ttotal: 8m 3s\tremaining: 16.6s\n",
            "757:\tlearn: 24.0744841\ttotal: 8m 4s\tremaining: 16s\n",
            "758:\tlearn: 24.0205550\ttotal: 8m 5s\tremaining: 15.3s\n",
            "759:\tlearn: 23.9641633\ttotal: 8m 5s\tremaining: 14.7s\n",
            "760:\tlearn: 23.9171355\ttotal: 8m 6s\tremaining: 14.1s\n",
            "761:\tlearn: 23.8597951\ttotal: 8m 7s\tremaining: 13.4s\n",
            "762:\tlearn: 23.8061276\ttotal: 8m 8s\tremaining: 12.8s\n",
            "763:\tlearn: 23.7488596\ttotal: 8m 8s\tremaining: 12.2s\n",
            "764:\tlearn: 23.7075800\ttotal: 8m 9s\tremaining: 11.5s\n",
            "765:\tlearn: 23.6849651\ttotal: 8m 10s\tremaining: 10.9s\n",
            "766:\tlearn: 23.6318088\ttotal: 8m 11s\tremaining: 10.2s\n",
            "767:\tlearn: 23.6210006\ttotal: 8m 11s\tremaining: 9.61s\n",
            "768:\tlearn: 23.6038271\ttotal: 8m 12s\tremaining: 8.97s\n",
            "769:\tlearn: 23.5872972\ttotal: 8m 13s\tremaining: 8.33s\n",
            "770:\tlearn: 23.5330749\ttotal: 8m 14s\tremaining: 7.69s\n",
            "771:\tlearn: 23.4985593\ttotal: 8m 14s\tremaining: 7.05s\n",
            "772:\tlearn: 23.4654111\ttotal: 8m 15s\tremaining: 6.41s\n",
            "773:\tlearn: 23.4236482\ttotal: 8m 16s\tremaining: 5.77s\n",
            "774:\tlearn: 23.3738717\ttotal: 8m 17s\tremaining: 5.13s\n",
            "775:\tlearn: 23.3267175\ttotal: 8m 17s\tremaining: 4.49s\n",
            "776:\tlearn: 23.3018553\ttotal: 8m 18s\tremaining: 3.85s\n",
            "777:\tlearn: 23.2836821\ttotal: 8m 19s\tremaining: 3.21s\n",
            "778:\tlearn: 23.2656513\ttotal: 8m 20s\tremaining: 2.57s\n",
            "779:\tlearn: 23.2141769\ttotal: 8m 20s\tremaining: 1.93s\n",
            "780:\tlearn: 23.1692540\ttotal: 8m 21s\tremaining: 1.28s\n",
            "781:\tlearn: 23.1342051\ttotal: 8m 22s\tremaining: 643ms\n",
            "782:\tlearn: 23.0996775\ttotal: 8m 23s\tremaining: 0us\n",
            "Fold 1, Best MAE: 42.25324087037929, Best hyperparameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "Fold 2, Best MAE: 44.239773522743256, Best hyperparameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "Fold 3, Best MAE: 42.615525740274286, Best hyperparameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "Fold 4, Best MAE: 42.99479569369386, Best hyperparameters: {'iterations': 792, 'learning_rate': 0.4094644098911695, 'depth': 8, 'min_data_in_leaf': 13, 'reg_lambda': 43.85829584203627, 'subsample': 0.6745303236399682, 'random_strength': 85.6116646925902, 'od_wait': 62, 'leaf_estimation_iterations': 16, 'bagging_temperature': 51.306767359876446, 'colsample_bylevel': 0.4699924477380927}\n",
            "Fold 5, Best MAE: 43.48887650381214, Best hyperparameters: {'iterations': 783, 'learning_rate': 0.2584880573013857, 'depth': 12, 'min_data_in_leaf': 30, 'reg_lambda': 45.02231140060744, 'subsample': 0.6753500090427036, 'random_strength': 21.788829786258407, 'od_wait': 90, 'leaf_estimation_iterations': 16, 'bagging_temperature': 58.67357912276298, 'colsample_bylevel': 0.5379457040607726}\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=724)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "# 각 Fold에 대해서 Optuna로 하이퍼파라미터 튜닝\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index, :], train_y.iloc[val_index, :]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=724))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val.values), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best MAE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = CatBoostRegressor(**best_params)\n",
        "    model.fit(X_train, y_train, cat_features=cat_features)\n",
        "    test_pred = model.predict(test)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "# 평균 앙상블\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "# 결과 출력\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")\n",
        "\n",
        "# 최종 예측 저장\n",
        "final_prediction_df = pd.DataFrame(final_prediction, columns=['CI_HOUR'])\n",
        "submission['CI_HOUR'] = final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SAMPLE_ID</th>\n",
              "      <th>CI_HOUR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_000000</td>\n",
              "      <td>85.724445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_000001</td>\n",
              "      <td>290.797561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_000002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_000003</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_000004</td>\n",
              "      <td>57.593767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244984</th>\n",
              "      <td>TEST_244984</td>\n",
              "      <td>69.481320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244985</th>\n",
              "      <td>TEST_244985</td>\n",
              "      <td>418.025254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244986</th>\n",
              "      <td>TEST_244986</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244987</th>\n",
              "      <td>TEST_244987</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244988</th>\n",
              "      <td>TEST_244988</td>\n",
              "      <td>897.068029</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244989 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          SAMPLE_ID     CI_HOUR\n",
              "0       TEST_000000   85.724445\n",
              "1       TEST_000001  290.797561\n",
              "2       TEST_000002    0.000000\n",
              "3       TEST_000003    0.000000\n",
              "4       TEST_000004   57.593767\n",
              "...             ...         ...\n",
              "244984  TEST_244984   69.481320\n",
              "244985  TEST_244985  418.025254\n",
              "244986  TEST_244986    0.000000\n",
              "244987  TEST_244987    0.000000\n",
              "244988  TEST_244988  897.068029\n",
              "\n",
              "[244989 rows x 2 columns]"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all = pd.concat([test,submission],axis=1)\n",
        "all['CI_HOUR'][all['DIST'] == 0] = 0\n",
        "submission['CI_HOUR'] = all['CI_HOUR']\n",
        "submission['CI_HOUR'][submission['CI_HOUR'] < 0] = 0\n",
        "submission.to_csv('CAT_v2_724.csv',index=False)\n",
        "submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bHexh3orVzUq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AlogP</th>\n",
              "      <th>Molecular_Weight</th>\n",
              "      <th>Num_H_Acceptors</th>\n",
              "      <th>Num_H_Donors</th>\n",
              "      <th>Num_RotatableBonds</th>\n",
              "      <th>LogD</th>\n",
              "      <th>Molecular_PolarSurfaceArea</th>\n",
              "      <th>nAcid</th>\n",
              "      <th>nBase</th>\n",
              "      <th>SpAbs_A</th>\n",
              "      <th>...</th>\n",
              "      <th>MACCS_key_158</th>\n",
              "      <th>MACCS_key_159</th>\n",
              "      <th>MACCS_key_160</th>\n",
              "      <th>MACCS_key_161</th>\n",
              "      <th>MACCS_key_162</th>\n",
              "      <th>MACCS_key_163</th>\n",
              "      <th>MACCS_key_164</th>\n",
              "      <th>MACCS_key_165</th>\n",
              "      <th>MLM</th>\n",
              "      <th>HLM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.259</td>\n",
              "      <td>400.495</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3.259</td>\n",
              "      <td>117.37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.689316</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26.010</td>\n",
              "      <td>50.680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.169</td>\n",
              "      <td>301.407</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.172</td>\n",
              "      <td>73.47</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.575899</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>29.270</td>\n",
              "      <td>50.590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.593</td>\n",
              "      <td>297.358</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.585</td>\n",
              "      <td>62.45</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>29.802128</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.586</td>\n",
              "      <td>80.892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.771</td>\n",
              "      <td>494.652</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3.475</td>\n",
              "      <td>92.60</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>45.884166</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.710</td>\n",
              "      <td>2.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.335</td>\n",
              "      <td>268.310</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.337</td>\n",
              "      <td>42.43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.308663</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>93.270</td>\n",
              "      <td>99.990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>3.409</td>\n",
              "      <td>396.195</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3.409</td>\n",
              "      <td>64.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.902711</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.556</td>\n",
              "      <td>3.079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>1.912</td>\n",
              "      <td>359.381</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.844</td>\n",
              "      <td>77.37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.887372</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.560</td>\n",
              "      <td>47.630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3468</th>\n",
              "      <td>1.941</td>\n",
              "      <td>261.320</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2.124</td>\n",
              "      <td>70.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.546531</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>56.150</td>\n",
              "      <td>1.790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3469</th>\n",
              "      <td>0.989</td>\n",
              "      <td>284.696</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.989</td>\n",
              "      <td>91.51</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.936088</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030</td>\n",
              "      <td>2.770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3470</th>\n",
              "      <td>4.321</td>\n",
              "      <td>295.399</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4.321</td>\n",
              "      <td>50.36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>27.713581</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.450</td>\n",
              "      <td>2.650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3471 rows × 5313 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n",
              "0     3.259           400.495                5             2   \n",
              "1     2.169           301.407                2             1   \n",
              "2     1.593           297.358                5             0   \n",
              "3     4.771           494.652                6             0   \n",
              "4     2.335           268.310                3             0   \n",
              "...     ...               ...              ...           ...   \n",
              "3466  3.409           396.195                3             1   \n",
              "3467  1.912           359.381                4             1   \n",
              "3468  1.941           261.320                3             1   \n",
              "3469  0.989           284.696                5             1   \n",
              "3470  4.321           295.399                2             0   \n",
              "\n",
              "      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  nAcid  nBase  \\\n",
              "0                      8  3.259                      117.37      0      0   \n",
              "1                      2  2.172                       73.47      0      0   \n",
              "2                      3  1.585                       62.45      2      1   \n",
              "3                      5  3.475                       92.60      0      1   \n",
              "4                      1  2.337                       42.43      0      0   \n",
              "...                  ...    ...                         ...    ...    ...   \n",
              "3466                   5  3.409                       64.74      0      0   \n",
              "3467                   3  1.844                       77.37      0      0   \n",
              "3468                   6  2.124                       70.14      0      0   \n",
              "3469                   5  0.989                       91.51      0      0   \n",
              "3470                   4  4.321                       50.36      0      0   \n",
              "\n",
              "        SpAbs_A  ...  MACCS_key_158  MACCS_key_159  MACCS_key_160  \\\n",
              "0     35.689316  ...              1              1              1   \n",
              "1     26.575899  ...              1              0              1   \n",
              "2     29.802128  ...              1              0              1   \n",
              "3     45.884166  ...              1              1              1   \n",
              "4     26.308663  ...              1              1              1   \n",
              "...         ...  ...            ...            ...            ...   \n",
              "3466  30.902711  ...              1              0              1   \n",
              "3467  35.887372  ...              1              1              1   \n",
              "3468  23.546531  ...              1              1              1   \n",
              "3469  23.936088  ...              1              1              0   \n",
              "3470  27.713581  ...              0              0              1   \n",
              "\n",
              "      MACCS_key_161  MACCS_key_162  MACCS_key_163  MACCS_key_164  \\\n",
              "0                 1              1              1              1   \n",
              "1                 1              1              1              1   \n",
              "2                 1              1              1              0   \n",
              "3                 1              1              1              1   \n",
              "4                 1              1              1              1   \n",
              "...             ...            ...            ...            ...   \n",
              "3466              1              1              0              1   \n",
              "3467              1              1              1              1   \n",
              "3468              1              1              1              1   \n",
              "3469              1              1              1              1   \n",
              "3470              1              1              1              1   \n",
              "\n",
              "      MACCS_key_165     MLM     HLM  \n",
              "0                 1  26.010  50.680  \n",
              "1                 1  29.270  50.590  \n",
              "2                 1   5.586  80.892  \n",
              "3                 1   5.710   2.000  \n",
              "4                 1  93.270  99.990  \n",
              "...             ...     ...     ...  \n",
              "3466              1   1.556   3.079  \n",
              "3467              1  35.560  47.630  \n",
              "3468              1  56.150   1.790  \n",
              "3469              1   0.030   2.770  \n",
              "3470              1   0.450   2.650  \n",
              "\n",
              "[3471 rows x 5313 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Objective 함수 정의\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    n_d = trial.suggest_int(\"n_d\", 4, 64)\n",
        "    n_a = trial.suggest_int(\"n_a\", 4, 64)\n",
        "    n_steps = trial.suggest_int(\"n_steps\", 1, 10)\n",
        "    gamma = trial.suggest_float(\"gamma\", 1.0, 2.0)\n",
        "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-5, 1e-3, log=True)\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "    optimizer_params = {\"lr\": lr}\n",
        "    \n",
        "    model = TabNetRegressor(\n",
        "        n_d=n_d,\n",
        "        n_a=n_a,\n",
        "        n_steps=n_steps,\n",
        "        gamma=gamma,\n",
        "        lambda_sparse=lambda_sparse,\n",
        "        optimizer_params=optimizer_params\n",
        "    )\n",
        "    \n",
        "    y_train_2d = y_train.values.reshape(-1, 1)\n",
        "    y_val_2d = y_val.values.reshape(-1, 1)\n",
        "    \n",
        "    model.fit(\n",
        "        X_train=X_train.values, \n",
        "        y_train=y_train_2d,\n",
        "        eval_set=[(X_val.values, y_val_2d)],\n",
        "        eval_metric=['mae'], \n",
        "        max_epochs=50, \n",
        "        patience=10\n",
        "    )\n",
        "    \n",
        "    preds_val = model.predict(X_val.values)\n",
        "    mae = mean_absolute_error(y_val.values.ravel(), preds_val)\n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모든 카테고리형 변수를 문자열로 변환\n",
        "categorical_cols = train_x.select_dtypes(include=['object', 'category']).columns\n",
        "train_x[categorical_cols] = train_x[categorical_cols].astype(str)\n",
        "test[categorical_cols] = test[categorical_cols].astype(str)\n",
        "# object 타입 피처만 레이블 인코딩\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoders = {}\n",
        "test_unknown = test.copy()\n",
        "\n",
        "for col in train_x.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    train_x[col] = le.fit_transform(train_x[col])\n",
        "    \n",
        "    known_mask = test[col].isin(le.classes_)\n",
        "    test_unknown = test_unknown[~known_mask]  # 새로운 레이블을 포함하는 데이터 포인트를 별도로 저장\n",
        "    test = test[known_mask]\n",
        "    \n",
        "    test[col] = le.transform(test[col])\n",
        "    label_encoders[col] = le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 01:57:59,512]\u001b[0m A new study created in memory with name: no-name-a4896c90-e6f1-42a6-8c8b-5140d4f7cb5f\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hyperparameters for fold 1...\n",
            "epoch 0  | loss: 54667.76901| val_0_mae: 180.92095|  0:00:12s\n",
            "epoch 1  | loss: 54281.59527| val_0_mae: 96.3394 |  0:00:23s\n",
            "epoch 2  | loss: 53814.79642| val_0_mae: 95.458  |  0:00:35s\n",
            "epoch 3  | loss: 53561.90072| val_0_mae: 94.85518|  0:00:47s\n",
            "epoch 4  | loss: 53108.98061| val_0_mae: 93.88412|  0:00:59s\n",
            "epoch 5  | loss: 52801.55621| val_0_mae: 93.0834 |  0:01:10s\n",
            "epoch 6  | loss: 52390.89816| val_0_mae: 92.26418|  0:01:21s\n",
            "epoch 7  | loss: 52056.07431| val_0_mae: 91.56768|  0:01:32s\n",
            "epoch 8  | loss: 51702.23693| val_0_mae: 91.00958|  0:01:43s\n",
            "epoch 9  | loss: 51476.37852| val_0_mae: 90.44323|  0:01:54s\n",
            "epoch 10 | loss: 51061.01298| val_0_mae: 89.9735 |  0:02:06s\n",
            "epoch 11 | loss: 50774.96551| val_0_mae: 89.50916|  0:02:18s\n",
            "epoch 12 | loss: 50536.75444| val_0_mae: 89.09913|  0:02:29s\n",
            "epoch 13 | loss: 50189.4527| val_0_mae: 88.73736|  0:02:41s\n",
            "epoch 14 | loss: 49885.91626| val_0_mae: 88.31281|  0:02:52s\n",
            "epoch 15 | loss: 49535.19113| val_0_mae: 88.13942|  0:03:03s\n",
            "epoch 16 | loss: 49218.82092| val_0_mae: 87.85673|  0:03:14s\n",
            "epoch 17 | loss: 48891.80871| val_0_mae: 87.62802|  0:03:26s\n",
            "epoch 18 | loss: 48622.86031| val_0_mae: 87.55156|  0:03:37s\n",
            "epoch 19 | loss: 48249.8295| val_0_mae: 87.49303|  0:03:48s\n",
            "epoch 20 | loss: 47972.63406| val_0_mae: 87.45159|  0:03:59s\n",
            "epoch 21 | loss: 47524.58687| val_0_mae: 87.47925|  0:04:11s\n",
            "epoch 22 | loss: 47300.80867| val_0_mae: 87.52749|  0:04:22s\n",
            "epoch 23 | loss: 47002.73457| val_0_mae: 87.71097|  0:04:33s\n",
            "epoch 24 | loss: 46627.28957| val_0_mae: 88.02458|  0:04:44s\n",
            "epoch 25 | loss: 46483.77078| val_0_mae: 88.32544|  0:04:55s\n",
            "epoch 26 | loss: 46106.75601| val_0_mae: 88.46872|  0:05:06s\n",
            "epoch 27 | loss: 45925.19347| val_0_mae: 88.79751|  0:05:18s\n",
            "epoch 28 | loss: 45613.11767| val_0_mae: 89.24789|  0:05:29s\n",
            "epoch 29 | loss: 45403.36398| val_0_mae: 89.69284|  0:05:40s\n",
            "epoch 30 | loss: 45193.25525| val_0_mae: 90.25419|  0:05:51s\n",
            "\n",
            "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_mae = 87.45159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:04:09,494]\u001b[0m Trial 0 finished with value: 87.4515860027796 and parameters: {'n_d': 58, 'n_a': 46, 'n_steps': 5, 'gamma': 1.471768994831443, 'lambda_sparse': 1.0520131499846483e-05, 'lr': 9.551591485457036e-05}. Best is trial 0 with value: 87.4515860027796.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 55200.8212| val_0_mae: 140.91934|  0:00:18s\n",
            "epoch 1  | loss: 54819.6298| val_0_mae: 104.29145|  0:00:35s\n",
            "epoch 2  | loss: 54864.47222| val_0_mae: 99.48548|  0:00:54s\n",
            "epoch 3  | loss: 54627.99508| val_0_mae: 98.81957|  0:01:13s\n",
            "epoch 4  | loss: 54556.0473| val_0_mae: 98.27959|  0:01:32s\n",
            "epoch 5  | loss: 54470.91333| val_0_mae: 97.81953|  0:01:51s\n",
            "epoch 6  | loss: 54251.61036| val_0_mae: 97.32818|  0:02:10s\n",
            "epoch 7  | loss: 54020.38877| val_0_mae: 96.90192|  0:02:29s\n",
            "epoch 8  | loss: 53893.73273| val_0_mae: 96.4771 |  0:02:48s\n",
            "epoch 9  | loss: 53832.84439| val_0_mae: 96.04991|  0:03:07s\n",
            "epoch 10 | loss: 53692.07525| val_0_mae: 95.65555|  0:03:27s\n",
            "epoch 11 | loss: 53605.63461| val_0_mae: 95.25395|  0:03:47s\n",
            "epoch 12 | loss: 53461.50434| val_0_mae: 94.9103 |  0:04:06s\n",
            "epoch 13 | loss: 53462.24061| val_0_mae: 94.5333 |  0:04:24s\n",
            "epoch 14 | loss: 53168.84983| val_0_mae: 94.22555|  0:04:46s\n",
            "epoch 15 | loss: 53158.89853| val_0_mae: 93.87284|  0:05:05s\n",
            "epoch 16 | loss: 52942.24935| val_0_mae: 93.5218 |  0:05:25s\n",
            "epoch 17 | loss: 52825.89031| val_0_mae: 93.17162|  0:05:45s\n",
            "epoch 18 | loss: 52754.69376| val_0_mae: 92.89039|  0:06:06s\n",
            "epoch 19 | loss: 52631.00737| val_0_mae: 92.63238|  0:06:25s\n",
            "epoch 20 | loss: 52469.38619| val_0_mae: 92.36485|  0:06:45s\n",
            "epoch 21 | loss: 52294.18978| val_0_mae: 92.12832|  0:07:05s\n",
            "epoch 22 | loss: 52255.04525| val_0_mae: 91.82937|  0:07:25s\n",
            "epoch 23 | loss: 52053.08052| val_0_mae: 91.5739 |  0:07:46s\n",
            "epoch 24 | loss: 51953.61611| val_0_mae: 91.29751|  0:08:06s\n",
            "epoch 25 | loss: 51918.84507| val_0_mae: 91.00982|  0:08:26s\n",
            "epoch 26 | loss: 51725.628| val_0_mae: 90.77064|  0:08:46s\n",
            "epoch 27 | loss: 51625.24408| val_0_mae: 90.53137|  0:09:07s\n",
            "epoch 28 | loss: 51454.64762| val_0_mae: 90.3105 |  0:09:27s\n",
            "epoch 29 | loss: 51339.49012| val_0_mae: 90.08561|  0:09:47s\n",
            "epoch 30 | loss: 51197.78717| val_0_mae: 89.87873|  0:10:07s\n",
            "epoch 31 | loss: 51030.4484| val_0_mae: 89.6764 |  0:10:26s\n",
            "epoch 32 | loss: 50904.01982| val_0_mae: 89.50132|  0:10:46s\n",
            "epoch 33 | loss: 50863.99302| val_0_mae: 89.25047|  0:11:05s\n",
            "epoch 34 | loss: 50743.54548| val_0_mae: 89.10319|  0:11:25s\n",
            "epoch 35 | loss: 50527.48773| val_0_mae: 88.89607|  0:11:45s\n",
            "epoch 36 | loss: 50354.59492| val_0_mae: 88.7992 |  0:12:06s\n",
            "epoch 37 | loss: 50247.45655| val_0_mae: 88.61919|  0:12:26s\n",
            "epoch 38 | loss: 50181.84406| val_0_mae: 88.43468|  0:12:47s\n",
            "epoch 39 | loss: 50016.84902| val_0_mae: 88.31349|  0:13:07s\n",
            "epoch 40 | loss: 49898.67976| val_0_mae: 88.20293|  0:13:27s\n",
            "epoch 41 | loss: 49669.91382| val_0_mae: 88.10884|  0:13:47s\n",
            "epoch 42 | loss: 49613.27013| val_0_mae: 87.95766|  0:14:08s\n",
            "epoch 43 | loss: 49392.73704| val_0_mae: 87.84672|  0:14:28s\n",
            "epoch 44 | loss: 49385.67642| val_0_mae: 87.81537|  0:14:48s\n",
            "epoch 45 | loss: 49225.80657| val_0_mae: 87.77375|  0:15:09s\n",
            "epoch 46 | loss: 49032.92301| val_0_mae: 87.62799|  0:15:28s\n",
            "epoch 47 | loss: 48970.87919| val_0_mae: 87.63349|  0:15:47s\n",
            "epoch 48 | loss: 48722.12248| val_0_mae: 87.59522|  0:16:05s\n",
            "epoch 49 | loss: 48686.18037| val_0_mae: 87.50547|  0:16:24s\n",
            "Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_mae = 87.50547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:21:06,939]\u001b[0m Trial 1 finished with value: 87.50547043557528 and parameters: {'n_d': 34, 'n_a': 21, 'n_steps': 9, 'gamma': 1.0189881736525601, 'lambda_sparse': 5.433270488189108e-05, 'lr': 3.736575233292654e-05}. Best is trial 0 with value: 87.4515860027796.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 56165.53162| val_0_mae: 121.68962|  0:00:12s\n",
            "epoch 1  | loss: 56051.26562| val_0_mae: 109.10764|  0:00:25s\n",
            "epoch 2  | loss: 55978.33927| val_0_mae: 103.57621|  0:00:38s\n",
            "epoch 3  | loss: 55695.06884| val_0_mae: 102.77369|  0:00:50s\n",
            "epoch 4  | loss: 55616.79267| val_0_mae: 102.1243|  0:01:03s\n",
            "epoch 5  | loss: 55510.32342| val_0_mae: 101.53767|  0:01:15s\n",
            "epoch 6  | loss: 55288.81478| val_0_mae: 100.96353|  0:01:28s\n",
            "epoch 7  | loss: 55249.3629| val_0_mae: 100.46883|  0:01:40s\n",
            "epoch 8  | loss: 55079.78255| val_0_mae: 99.99958|  0:01:53s\n",
            "epoch 9  | loss: 54784.32521| val_0_mae: 99.51779|  0:02:06s\n",
            "epoch 10 | loss: 54825.70543| val_0_mae: 99.04981|  0:02:19s\n",
            "epoch 11 | loss: 54633.61721| val_0_mae: 98.63612|  0:02:31s\n",
            "epoch 12 | loss: 54618.31074| val_0_mae: 98.21637|  0:02:44s\n",
            "epoch 13 | loss: 54454.86712| val_0_mae: 97.90979|  0:02:57s\n",
            "epoch 14 | loss: 54431.58068| val_0_mae: 97.47347|  0:03:09s\n",
            "epoch 15 | loss: 54247.88097| val_0_mae: 97.1203 |  0:03:22s\n",
            "epoch 16 | loss: 54147.97119| val_0_mae: 96.76958|  0:03:34s\n",
            "epoch 17 | loss: 54003.98159| val_0_mae: 96.44835|  0:03:47s\n",
            "epoch 18 | loss: 54002.59247| val_0_mae: 96.11228|  0:03:59s\n",
            "epoch 19 | loss: 53747.74515| val_0_mae: 95.7125 |  0:04:11s\n",
            "epoch 20 | loss: 53769.22652| val_0_mae: 95.46807|  0:04:23s\n",
            "epoch 21 | loss: 53522.46586| val_0_mae: 95.15146|  0:04:35s\n",
            "epoch 22 | loss: 53522.64373| val_0_mae: 94.77945|  0:04:48s\n",
            "epoch 23 | loss: 53417.77508| val_0_mae: 94.50758|  0:05:01s\n",
            "epoch 24 | loss: 53313.73561| val_0_mae: 94.16229|  0:05:13s\n",
            "epoch 25 | loss: 53193.45372| val_0_mae: 93.90529|  0:05:26s\n",
            "epoch 26 | loss: 53017.81351| val_0_mae: 93.57826|  0:05:39s\n",
            "epoch 27 | loss: 52988.32237| val_0_mae: 93.2881 |  0:05:52s\n",
            "epoch 28 | loss: 52825.71117| val_0_mae: 92.98421|  0:06:05s\n",
            "epoch 29 | loss: 52751.09608| val_0_mae: 92.73828|  0:06:18s\n",
            "epoch 30 | loss: 52602.01453| val_0_mae: 92.39195|  0:06:31s\n",
            "epoch 31 | loss: 52537.04341| val_0_mae: 92.15194|  0:06:44s\n",
            "epoch 32 | loss: 52237.66404| val_0_mae: 91.90387|  0:06:57s\n",
            "epoch 33 | loss: 52244.4889| val_0_mae: 91.64851|  0:07:11s\n",
            "epoch 34 | loss: 52043.33313| val_0_mae: 91.38639|  0:07:24s\n",
            "epoch 35 | loss: 52000.18867| val_0_mae: 91.14008|  0:07:37s\n",
            "epoch 36 | loss: 51912.39029| val_0_mae: 90.91026|  0:07:50s\n",
            "epoch 37 | loss: 51584.5855| val_0_mae: 90.68667|  0:08:03s\n",
            "epoch 38 | loss: 51697.83444| val_0_mae: 90.41624|  0:08:16s\n",
            "epoch 39 | loss: 51515.92176| val_0_mae: 90.19822|  0:08:29s\n",
            "epoch 40 | loss: 51307.26882| val_0_mae: 89.9548 |  0:08:42s\n",
            "epoch 41 | loss: 51252.54224| val_0_mae: 89.71145|  0:08:55s\n",
            "epoch 42 | loss: 51077.99508| val_0_mae: 89.51197|  0:09:08s\n",
            "epoch 43 | loss: 50931.33946| val_0_mae: 89.3787 |  0:09:20s\n",
            "epoch 44 | loss: 50747.92701| val_0_mae: 89.11786|  0:09:32s\n",
            "epoch 45 | loss: 50704.44979| val_0_mae: 88.98439|  0:09:45s\n",
            "epoch 46 | loss: 50438.94086| val_0_mae: 88.77437|  0:09:57s\n",
            "epoch 47 | loss: 50418.75014| val_0_mae: 88.61294|  0:10:09s\n",
            "epoch 48 | loss: 50266.76641| val_0_mae: 88.5067 |  0:10:22s\n",
            "epoch 49 | loss: 49980.89624| val_0_mae: 88.38432|  0:10:36s\n",
            "Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_mae = 88.38432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:32:03,453]\u001b[0m Trial 2 finished with value: 88.38432475220883 and parameters: {'n_d': 37, 'n_a': 27, 'n_steps': 5, 'gamma': 1.4603327006353592, 'lambda_sparse': 0.0008534092206517997, 'lr': 4.844451042562636e-05}. Best is trial 0 with value: 87.4515860027796.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 55328.52405| val_0_mae: 102.12292|  0:00:15s\n",
            "epoch 1  | loss: 54551.50612| val_0_mae: 97.92952|  0:00:30s\n",
            "epoch 2  | loss: 53936.78102| val_0_mae: 94.77041|  0:00:45s\n",
            "epoch 3  | loss: 53417.31689| val_0_mae: 93.91873|  0:01:01s\n",
            "epoch 4  | loss: 53012.97798| val_0_mae: 92.61567|  0:01:16s\n",
            "epoch 5  | loss: 52525.03646| val_0_mae: 91.47999|  0:01:31s\n",
            "epoch 6  | loss: 51853.47927| val_0_mae: 90.42937|  0:01:46s\n",
            "epoch 7  | loss: 51589.36794| val_0_mae: 89.52483|  0:02:02s\n",
            "epoch 8  | loss: 50911.37336| val_0_mae: 88.7996 |  0:02:18s\n",
            "epoch 9  | loss: 50522.28703| val_0_mae: 88.26465|  0:02:33s\n",
            "epoch 10 | loss: 50039.44559| val_0_mae: 87.80993|  0:02:48s\n",
            "epoch 11 | loss: 49595.04656| val_0_mae: 87.50941|  0:03:03s\n",
            "epoch 12 | loss: 49176.06787| val_0_mae: 87.24937|  0:03:19s\n",
            "epoch 13 | loss: 48691.30337| val_0_mae: 87.22915|  0:03:34s\n",
            "epoch 14 | loss: 48293.57882| val_0_mae: 87.31586|  0:03:50s\n",
            "epoch 15 | loss: 47840.62217| val_0_mae: 87.55827|  0:04:05s\n",
            "epoch 16 | loss: 47497.36271| val_0_mae: 87.90539|  0:04:19s\n",
            "epoch 17 | loss: 47156.06365| val_0_mae: 88.33343|  0:04:33s\n",
            "epoch 18 | loss: 46791.07335| val_0_mae: 88.95642|  0:04:48s\n",
            "epoch 19 | loss: 46446.78789| val_0_mae: 89.57177|  0:05:03s\n",
            "epoch 20 | loss: 46030.12559| val_0_mae: 90.24093|  0:05:17s\n",
            "epoch 21 | loss: 45729.87791| val_0_mae: 91.17123|  0:05:31s\n",
            "epoch 22 | loss: 45423.31725| val_0_mae: 91.9555 |  0:05:45s\n",
            "epoch 23 | loss: 45337.8606| val_0_mae: 93.00282|  0:06:00s\n",
            "\n",
            "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_mae = 87.22915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:38:26,739]\u001b[0m Trial 3 finished with value: 87.22914795206387 and parameters: {'n_d': 53, 'n_a': 62, 'n_steps': 6, 'gamma': 1.8290967984097537, 'lambda_sparse': 2.0638315057558745e-05, 'lr': 0.00014753325905081117}. Best is trial 3 with value: 87.22914795206387.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 55057.72515| val_0_mae: 103.21543|  0:00:19s\n",
            "epoch 1  | loss: 54999.30688| val_0_mae: 97.88925|  0:00:39s\n",
            "epoch 2  | loss: 54923.15769| val_0_mae: 99.3274 |  0:00:59s\n",
            "epoch 3  | loss: 54882.61184| val_0_mae: 99.66273|  0:01:18s\n",
            "epoch 4  | loss: 54781.98376| val_0_mae: 99.4892 |  0:01:38s\n",
            "epoch 5  | loss: 54834.16983| val_0_mae: 99.36418|  0:01:57s\n",
            "epoch 6  | loss: 54784.86199| val_0_mae: 99.13258|  0:02:17s\n",
            "epoch 7  | loss: 54676.25836| val_0_mae: 98.90912|  0:02:36s\n",
            "epoch 8  | loss: 54621.33027| val_0_mae: 98.75119|  0:02:56s\n",
            "epoch 9  | loss: 54569.33982| val_0_mae: 98.64019|  0:03:16s\n",
            "epoch 10 | loss: 54499.69737| val_0_mae: 98.42384|  0:03:36s\n",
            "epoch 11 | loss: 54499.32717| val_0_mae: 98.35121|  0:03:56s\n",
            "\n",
            "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_mae = 97.88925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-10-08 02:42:55,642]\u001b[0m Trial 4 finished with value: 97.88925116629748 and parameters: {'n_d': 33, 'n_a': 8, 'n_steps': 9, 'gamma': 1.3954106897914667, 'lambda_sparse': 3.435899500927241e-05, 'lr': 1.4530706847817623e-05}. Best is trial 3 with value: 87.22914795206387.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 46095.39365| val_0_mae: 417.88197|  0:00:18s\n",
            "epoch 1  | loss: 43015.46217| val_0_mae: 136.88145|  0:00:36s\n",
            "epoch 2  | loss: 42736.49585| val_0_mae: 103.93545|  0:00:55s\n",
            "epoch 3  | loss: 42568.07205| val_0_mae: 100.54177|  0:01:13s\n",
            "epoch 4  | loss: 42609.04317| val_0_mae: 98.73855|  0:01:30s\n",
            "epoch 5  | loss: 42485.57384| val_0_mae: 106.69002|  0:01:48s\n",
            "epoch 6  | loss: 41993.5897| val_0_mae: 101.21619|  0:02:07s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\USER\\Desktop\\새 폴더 (8)\\XGB_v2.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m724\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val), n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m _optimize(\n\u001b[0;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    410\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "\u001b[1;32mc:\\Users\\USER\\Desktop\\새 폴더 (8)\\XGB_v2.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizing hyperparameters for fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m724\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, X_train, y_train, X_val, y_val), n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_score \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
            "\u001b[1;32mc:\\Users\\USER\\Desktop\\새 폴더 (8)\\XGB_v2.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m y_train_2d \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m y_val_2d \u001b[39m=\u001b[39m y_val\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     X_train\u001b[39m=\u001b[39;49mX_train\u001b[39m.\u001b[39;49mvalues, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     y_train\u001b[39m=\u001b[39;49my_train_2d,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[(X_val\u001b[39m.\u001b[39;49mvalues, y_val_2d)],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     eval_metric\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmae\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m preds_val \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_val\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/Desktop/%EC%83%88%20%ED%8F%B4%EB%8D%94%20%288%29/XGB_v2.ipynb#X50sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m mae \u001b[39m=\u001b[39m mean_absolute_error(y_val\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel(), preds_val)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:245\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n\u001b[1;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_epoch(eval_name, valid_dataloader)\n\u001b[0;32m    247\u001b[0m \u001b[39m# Call method on_epoch_end for all callbacks\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_end(\n\u001b[0;32m    249\u001b[0m     epoch_idx, logs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mepoch_metrics\n\u001b[0;32m    250\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:530\u001b[0m, in \u001b[0;36mTabModel._predict_epoch\u001b[1;34m(self, name, loader)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39m# Main loop\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[1;32m--> 530\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_batch(X)\n\u001b[0;32m    531\u001b[0m     list_y_true\u001b[39m.\u001b[39mappend(y)\n\u001b[0;32m    532\u001b[0m     list_y_score\u001b[39m.\u001b[39mappend(scores)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:558\u001b[0m, in \u001b[0;36mTabModel._predict_batch\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    555\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m    557\u001b[0m \u001b[39m# compute model output\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m scores, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scores, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    561\u001b[0m     scores \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scores]\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:586\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    585\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[1;32m--> 586\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:471\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    470\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 471\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    472\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    474\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[0;32m    475\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:168\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[1;34m(self, x, prior)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39m# output\u001b[39;00m\n\u001b[0;32m    167\u001b[0m masked_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(M, x)\n\u001b[1;32m--> 168\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeat_transformers[step](masked_x)\n\u001b[0;32m    169\u001b[0m d \u001b[39m=\u001b[39m ReLU()(out[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_d])\n\u001b[0;32m    170\u001b[0m steps_output\u001b[39m.\u001b[39mappend(d)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:707\u001b[0m, in \u001b[0;36mFeatTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    706\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared(x)\n\u001b[1;32m--> 707\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspecifics(x)\n\u001b[0;32m    708\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:749\u001b[0m, in \u001b[0;36mGLU_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    746\u001b[0m     layers_left \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_glu)\n\u001b[0;32m    748\u001b[0m \u001b[39mfor\u001b[39;00m glu_id \u001b[39min\u001b[39;00m layers_left:\n\u001b[1;32m--> 749\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39madd(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglu_layers[glu_id](x))\n\u001b[0;32m    750\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m scale\n\u001b[0;32m    751\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:773\u001b[0m, in \u001b[0;36mGLU_Layer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    772\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m--> 773\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x)\n\u001b[0;32m    774\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(x[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim], torch\u001b[39m.\u001b[39msigmoid(x[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim :]))\n\u001b[0;32m    775\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:36\u001b[0m, in \u001b[0;36mGBN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     chunks \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mchunk(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_batch_size)), \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x_) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m chunks]\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(res, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytorch_tabnet\\tab_network.py:36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     chunks \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mchunk(\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvirtual_batch_size)), \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     res \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x_) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m chunks]\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(res, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2452\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=724)\n",
        "fold_results = []\n",
        "test_predictions = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_x)):\n",
        "    X_train, X_val = train_x.iloc[train_index, :], train_x.iloc[val_index, :]\n",
        "    y_train, y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
        "\n",
        "    print(f\"Optimizing hyperparameters for fold {fold+1}...\")\n",
        "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=724))\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=20)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    \n",
        "    print(f\"Best MAE for fold {fold+1}: {best_score}\")\n",
        "    print(f\"Best hyperparameters for fold {fold+1}: {best_params}\")\n",
        "    \n",
        "    fold_results.append({\n",
        "        'fold': fold+1,\n",
        "        'best_score': best_score,\n",
        "        'best_params': best_params\n",
        "    })\n",
        "    \n",
        "    # 최적의 하이퍼파라미터로 테스트 데이터 예측\n",
        "    model = TabNetRegressor(**best_params)\n",
        "    model.fit(X_train.values, y_train.values.ravel(), max_epochs=100)\n",
        "    test_pred = model.predict(test.values)\n",
        "    test_predictions.append(test_pred)\n",
        "\n",
        "final_prediction = np.mean(test_predictions, axis=0)\n",
        "\n",
        "for result in fold_results:\n",
        "    print(f\"Fold {result['fold']}, Best MAE: {result['best_score']}, Best hyperparameters: {result['best_params']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
